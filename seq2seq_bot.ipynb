{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhSCyy2BpoBj",
    "outputId": "27eedf05-cee7-455b-8b2a-a15145c63cc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 22 15:15:12 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.68.02    Driver Version: 512.77       CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   47C    P0    23W /  N/A |      0MiB /  6144MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pI5nKY8JpoBn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkSgEe-Jku-r"
   },
   "source": [
    "# Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QNNAb0a9kuEE"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3v2Ww3lkf9x"
   },
   "source": [
    "В этом проекте мы будем учиться делать перевод с французского на английский. Примерно так:\n",
    "\n",
    "    [KEY: > input, = target, < output]\n",
    "\n",
    "    > il est en train de peindre un tableau .\n",
    "    = he is painting a picture .\n",
    "    < he is painting a picture .\n",
    "\n",
    "    > pourquoi ne pas essayer ce vin delicieux ?\n",
    "    = why not try that delicious wine ?\n",
    "    < why not try that delicious wine ?\n",
    "\n",
    "    > elle n est pas poete mais romanciere .\n",
    "    = she is not a poet but a novelist .\n",
    "    < she not not a poet but a novelist .\n",
    "\n",
    "    > vous etes trop maigre .\n",
    "    = you re too skinny .\n",
    "    < you re all alone .\n",
    "\n",
    "Для этого мы будем исплользовать мощную идею «sequence-to-sequence» сетей (https://arxiv.org/abs/1409.3215), в которых две рекуррентные сети обучаются вместе для преоразования одной последовательности в другую.\n",
    "\n",
    "* Encoder-сеть сжимает входную последовательность в вектор.\n",
    "* Decoder-сеть разжимает этот вектор в новую последовательность.\n",
    "\n",
    "Всё как с автоэнкодерами, только encoder и decoder из разных доменов.\n",
    "\n",
    "Чтобы вся эта схема обучалась стабильнее, мы будем использовать механизм attention (https://arxiv.org/abs/1409.0473), позволяющий декодеру «фокусироваться» на специфичных токенах входной последовательности.\n",
    "\n",
    "**Рекомендуемое чтение:**\n",
    "\n",
    "-  Learning Phrase Representations using RNN Encoder-Decoder for\n",
    "   Statistical Machine Translation (https://arxiv.org/abs/1406.1078)\n",
    "-  Sequence to Sequence Learning with Neural\n",
    "   Networks (https://arxiv.org/abs/1409.3215)\n",
    "-  Neural Machine Translation by Jointly Learning to Align and\n",
    "   Translate 9https://arxiv.org/abs/1409.0473)\n",
    "-  A Neural Conversational Model (https://arxiv.org/abs/1506.05869>)\n",
    "\n",
    "Если кто-то пропустил предыдущие занатия, то лучше сначала сделать их: основные концепции такие же, как в языковых моделях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zt1ZcvSZkf9y"
   },
   "outputs": [],
   "source": [
    "# осторожно: тетрадка старая\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A9f1xpMkf90"
   },
   "source": [
    "## Данные\n",
    "\n",
    "В этом проекте мы будем работать с кучей пар предложений на английском и французском.\n",
    "\n",
    "Скачайте данные отсюда (https://download.pytorch.org/tutorial/data.zip) и возьмите оттуда файлик eng-fra.txt. В нём должно быть много строчек примерно такого формата:\n",
    "\n",
    "    I am cold.    J'ai froid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pAoNlcCkf92"
   },
   "source": [
    "Делать предобработку будем по аналогии с char-level RNN-ками из предыдущих туториалов, только на этот раз нам важно отдельно запариться с EOS (end-of-sequence) — специальным токеном, который сеть будет генерировать при окончании генерации предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zYgxYCIZkf93"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def __repr__(self):\n",
    "        most_popular_words = sorted(\n",
    "            self.word2count.keys(), key=lambda word: self.word2count[word], reverse=True\n",
    "        )[:10]\n",
    "        most_popular_words = \", \".join(most_popular_words)\n",
    "        return f\"Language: {self.name} | Num words: {self.n_words} | Most popular: {most_popular_words}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G--d0a55kf95"
   },
   "source": [
    "Все файлы в юникоде. Чтобы  облегчить нам работу, мы переведем все в ASCII, сделаем lowercase и выкинем большинство пунктуации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ygu9l499kf96"
   },
   "outputs": [],
   "source": [
    "# \"hello!\" -> hello, ! \n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# def normalizeString(s):\n",
    "#     s = unicodeToAscii(s.lower().strip())\n",
    "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "#     return s\n",
    "\n",
    "def normalizeString(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\sa-zA-Z0-9@\\[\\]]',' ',text) # Удаляет пунктцацию\n",
    "    text = re.sub(r'\\w*\\d+\\w*', '', text) # Удаляет цифры\n",
    "    text = re.sub('\\s{2,}', \" \", text) # Удаляет ненужные пробелы\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nExIwuorkf98"
   },
   "source": [
    "При чтении данных разделим файл на строки, а строки на пары.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kR6MaACekf99"
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('happiness_provokers', encoding='utf-8')\n",
    "\n",
    "    # Get pairs and normalize\n",
    "    pairs = list(zip([normalizeString(s) for s in list(df['query'].values)], [normalizeString(s) for s in list(df.reply.values)]))\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAvHyswFrMj7",
    "outputId": "28b73ea3-9bbf-496a-b4cb-db94bf59ae5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/utsx/.local/lib/python3.8/site-packages (4.19.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: filelock in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/utsx/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/utsx/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NMwAc5_8rPuY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "5bf945ae3c7540f6ba472f20e2fdacb1",
      "7129a21f55af4ccb8bb7544ee34b6831",
      "ffe6cf8b83c649ec8aa08d270316b730",
      "c3382ae68dd84a4ba848cf70807658e0",
      "0db735ce391a424c87a3f287340ab7fa",
      "69de0ae6cbbe4eb7812e5884318d20b6",
      "6e6527f3e6c24c69ab4d669c28207a84",
      "5f0035855dfe4cd597ee7030acf16d72",
      "489ea138854d48008e1904c9e46dd6c1",
      "f2f168b45b1745a3ac7fb3c21699faa9",
      "37b45c0d2e3f421f8f8dba60722b1762",
      "1a279709b86e47e6b44117900e352dde",
      "f2392865fc8646509d45a5bfcdbdaad5",
      "9d1ff43c43ee4dfd9d3d6db33f3fef7f",
      "5bf2e4b9333d4909b661e69dc5e5bb5c",
      "061670ab7df04ec59ac89a79141d5773",
      "6a228332a0f44ba29f6e211f43c884a7",
      "9002278fd22143c69a6ab1dd360bcbcf",
      "0ad9f6135f4b4405b5facba733d28865",
      "1ecd56e5cd334e90ac1963e6255723b6",
      "fa1455eaad504e11b44f2a9ca79a6697",
      "63a6cde631a947a6bb85d1cbb89d9f21",
      "f60dee956d9d444f98a69ea9a6c8f17d",
      "2ea006d2693a46b7a6f73ad98c3ad71f",
      "5e76746f82b5427ab86238f4af8f0990",
      "efc568a8b8644795b1ac2e3e6a533a3c",
      "7fe87edbe4a945e3b48b4640aefc9aa0",
      "0f35c10168a74023b503c589e0a3e48e",
      "ed65250433ee4013acda7747b672123b",
      "a688e92efa5740e3ae9a4dc973a06df8",
      "fecd7f53f93644bfa8c70dc8946a7919",
      "df887e109e674daeb07ea2a6a9291202",
      "7740d64e27724da6ba665101cbe4fd74"
     ]
    },
    "id": "9D0dSudIra29",
    "outputId": "ea4ff828-33a6-417c-df78-aa39e7cf5854"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dxwJGabVrl9Y"
   },
   "outputs": [],
   "source": [
    "seq = \"\"\"Hi bruh wtf r u doing ahh?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ld2s6T5IrpRY",
    "outputId": "b4df701d-72c0-4cc0-e314-d415a34f27ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      " bru\n",
      "h\n",
      " w\n",
      "tf\n",
      " r\n",
      " u\n",
      " doing\n",
      " a\n",
      "hh\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer.encode(seq):\n",
    "  print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSo1cu0rkf-C"
   },
   "source": [
    "Полный процесс такой:\n",
    "\n",
    "- Считать текстовый файл, просплитить по линиям, а затем по парам.\n",
    "- Нормализовать текст, профильтровать по длине.\n",
    "- Сделать готовые списки слов из сырых предложений в каждом языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "1G_BKtnukf-D",
    "outputId": "f3e4fd1e-3fdd-46c7-9ded-85397918896d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 32885 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_user 18346\n",
      "eng_emotion_provoker 18295\n",
      "(' that s funny there is a store in alabama that sells things like clothes that were in unclaimed baggage at the airports ', ' i hope they wash them first a man wore pieces of clothing at an airport to avoid baggage fees')\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    pairs = pairs[::]\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng_user', 'eng_emotion_provoker')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OM1rYFl6lRi3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language: eng_user | Num words: 18346 | Most popular: , i, the, that, a, to, it, of, is, in"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GmEuofeTlPKw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language: eng_emotion_provoker | Num words: 18295 | Most popular: , i, the, that, a, to, it, of, is, you"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4dMte9MFpoBy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32885"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bBIv4CpwpoBy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' i don t know but i am at the end of knowledge on this subject what do you think about the facts of comics i would have thought us invented them but it s japan or uk ',\n",
       " ' japan has a great printing history i am not surprised about this do you read comics books i have read a few but not about super heroes ')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmijlK-akf-G"
   },
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
    "black cat\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzGBS-alkf-H"
   },
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtVkq0VQBhdx"
   },
   "source": [
    "(https://blog.floydhub.com/content/images/2019/07/image17-1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qpcrvvXakf-I"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # num_embedding = vocab_size_fra\n",
    "        self.embedder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # (batch_size, num_words) -> (batch_size, num_words, dim_1)\n",
    "        embeddings = self.embedder(input).view(1, 1, -1)\n",
    "        # (batch_size, num_words, dim_2)\n",
    "        output, hidden = self.gru(embeddings, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-WEZjgj0poBz"
   },
   "outputs": [],
   "source": [
    "tokens = torch.randint(0, 1000, size=(128, 40))\n",
    "embedder = nn.Embedding(1000, 128)  # здесь лежит матрица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6-Ick8dBpoBz"
   },
   "outputs": [],
   "source": [
    "onehot = torch.nn.functional.one_hot(tokens, num_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "w3kNebFKpoBz"
   },
   "outputs": [],
   "source": [
    "embeddingds_first_way = embedder(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-LYlliqEtLay"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 40, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddingds_first_way.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNZcay6Hkf-K"
   },
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWJW6eKikf-L"
   },
   "source": [
    "Simple Decoder\n",
    "_________________\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string ``<SOS>`` or `<BOS>`\n",
    "token, and the first hidden state is the context vector (the encoder's\n",
    "last hidden state).\n",
    "The last token is `<EOS>` whats mean end of string\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WtM1ZGRskf-M"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedder = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # (batch_size, num_words, dim)\n",
    "        # (1, 1, num_words * dim)\n",
    "        output = self.embedder(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # (batch_size, num_words, dim) -> (batch_size, num_words, num_classes)\n",
    "        # (batch_size, num_words, vocab_size_eng)\n",
    "        output = self.out(output[0])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfTzzPptkf-O"
   },
   "source": [
    "I encourage you to train and observe the results of this model, but to\n",
    "save space we'll be going straight for the gold and introducing the\n",
    "Attention Mechanism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNllTJbIkf-P"
   },
   "source": [
    "Attention Decoder\n",
    "____________\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "h1sZQGIJpoB0"
   },
   "outputs": [],
   "source": [
    "# v^TWm\n",
    "# U^T tanh (W_1 v + W_2 m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "LZwO_Gg2kf-P"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(\n",
    "                torch.cat((embedded[0], hidden[0]), 1)\n",
    "            ), \n",
    "            dim=1\n",
    "        )\n",
    "        attn_applied = torch.bmm(\n",
    "            attn_weights.unsqueeze(0),\n",
    "            encoder_outputs.unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLtuThIQkf-R"
   },
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
    "  limitation by using a relative position approach. Read about \"local\n",
    "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
    "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Nmb8OwO7kf-S"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3OzldY0kf-U"
   },
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "na-c3f0nkf-V"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(\n",
    "    input_tensor, \n",
    "    target_tensor,\n",
    "    encoder, \n",
    "    decoder, \n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer, \n",
    "    criterion,\n",
    "    max_length=MAX_LENGTH\n",
    "):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        # y_true: [sos, i, love, pizza, eos]\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # [0.9, 0.1, 0.0]\n",
    "            # [1, 0, 0]\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # beam_search is betters\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                # y_true: [sos, i, eos]\n",
    "                # [sos, i, eos, love]\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfmlhEzDkf-X"
   },
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "WNbWUTxfkf-Y"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc8pY8ZCkf-a"
   },
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "skZhUHw1kf-a"
   },
   "outputs": [],
   "source": [
    "PATH_ENC = \"blablaenc.pt\"\n",
    "PATH_DECO = \"blabladeco.pt\"\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % 100 == 0:\n",
    "          torch.save(encoder.state_dict(), PATH_ENC)\n",
    "          torch.save(decoder.state_dict(), PATH_DECO)\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "            output_words_check, _ = evaluate(encoder, decoder, 'what are you doing my dear friend')\n",
    "            print(output_words_check)\n",
    "            print()\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT4JmR_fkf-d"
   },
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Zf3Fhj-5kf-e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgHOJ5Ugkf-j"
   },
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "XvYgjVG_kf-j"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-Xly1n3kf-l"
   },
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "58m1z4kJkf-m"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrSEC280kf-o"
   },
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n",
    ".. Note::\n",
    "   If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBS0Q3gokf-p",
    "outputId": "8851f1df-8bc3-4052-b25f-4aa9d8e07a17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 11s (- 283m 27s) (50 0%) 6.0652\n",
      "['', 'i', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of']\n",
      "\n",
      "0m 15s (- 191m 40s) (100 0%) 5.3391\n",
      "['', 'that', 'that', 'is', '<EOS>']\n",
      "\n",
      "0m 18s (- 151m 3s) (150 0%) 5.1372\n",
      "['', 'i', 'it', 'it', 'it', 'it', 'it', 'it', '<EOS>']\n",
      "\n",
      "0m 20s (- 128m 46s) (200 0%) 4.9012\n",
      "['', 'i', 'the', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 24s (- 120m 20s) (250 0%) 5.6689\n",
      "['', 'i', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', '', '<EOS>']\n",
      "\n",
      "0m 26s (- 111m 32s) (300 0%) 5.0866\n",
      "['', 'i', 'know', 'that', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "\n",
      "0m 30s (- 106m 58s) (350 0%) 5.2601\n",
      "['', 'that', 'is', 'is', 'that', 'is', 'is', 'a', '<EOS>']\n",
      "\n",
      "0m 32s (- 102m 32s) (400 0%) 5.3609\n",
      "['', 'i', 'would', 'a', 'it', 'would', 'a', 'a', 'it', 'a', 'to', '<EOS>']\n",
      "\n",
      "0m 35s (- 97m 23s) (450 0%) 4.7893\n",
      "['', 'i', 'i', 'can', 'i', 'can', 't', 'i', 'can', '', '<EOS>']\n",
      "\n",
      "0m 37s (- 93m 43s) (500 0%) 4.5102\n",
      "['', 'i', 'i', 'have', 'i', 'a', 'i', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "\n",
      "0m 40s (- 92m 27s) (550 0%) 5.4463\n",
      "['', 'i', 'i', 'i', 'like', 'i', 'like', 'i', 'and', 'i', 'of', 'i', 'and', 'i', 'of', 'and', 'i', 'and', 'i', 'of', 'and', 'i', 'and', 'i', 'of', 'and', 'i', 'and', 'i', 'of', '', '<EOS>']\n",
      "\n",
      "0m 44s (- 91m 10s) (600 0%) 5.1839\n",
      "['', 'that', 'is', 'a', 'the', 'is', 'a', 'the', 'is', 'a', 'the', 'is', 'a', '', '<EOS>']\n",
      "\n",
      "0m 46s (- 88m 58s) (650 0%) 4.7981\n",
      "['', 'i', 'i', 's', 'a', 'i', 's', 'a', '', '<EOS>']\n",
      "\n",
      "0m 49s (- 87m 20s) (700 0%) 4.8004\n",
      "['', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you', 'the', 'you', 'the', 'you', 'the', 'the', 'you']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 52s (- 87m 4s) (750 1%) 4.8088\n",
      "['', 'yes', 'is', 'i', 'is', 'a', 'you', 'know', 'the', 'you', 'know', '', '<EOS>']\n",
      "\n",
      "0m 55s (- 86m 9s) (800 1%) 5.3390\n",
      "['', 'it', 's', 'the', 'to', 'the', 'to', 'the', 'to', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "0m 58s (- 84m 43s) (850 1%) 4.9572\n",
      "['', 'i', 'i', 'is', 'i', 'the', 'to', 'the', 'to', 'the', 'to', 'the', 'to', 'the', 'to', 'the', 'to', '<EOS>']\n",
      "\n",
      "1m 1s (- 83m 43s) (900 1%) 5.0952\n",
      "['', 'i', 'i', 'know', 'that', 'a', 'that', 'a', 'i', 'a', 'that', 'a', 'a', 'that', 'a', '<EOS>']\n",
      "\n",
      "1m 4s (- 83m 9s) (950 1%) 5.0695\n",
      "['', 'yeah', 'that', 'is', 'a', 'of', 'the', 'of', 'the', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "1m 7s (- 82m 43s) (1000 1%) 4.9406\n",
      "['', 'i', 'would', 'i', 'would', 'to', 'a', 'to', 'a', 'to', 'to', 'a', 'to', 'for', 'a', 'to', '', '<EOS>']\n",
      "\n",
      "1m 9s (- 82m 0s) (1050 1%) 4.7839\n",
      "['', 'i', 'guess', 'i', 'guess', 'i', 'guess', 'i', 'guess', 'i', 'guess', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'would', 'be', 'a', 'lot', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'would', 'be', 'a', 'lot', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i', 'think', 'i']\n",
      "\n",
      "1m 13s (- 82m 14s) (1100 1%) 5.0957\n",
      "['', 'i', 'i', 't', 'think', 'the', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "1m 15s (- 81m 11s) (1150 1%) 4.8193\n",
      "['', 'i', 'is', 'it', 'is', 'a', 'it', 'is', 'a', '', '<EOS>']\n",
      "\n",
      "1m 18s (- 80m 30s) (1200 1%) 4.7754\n",
      "['', 'that', 'is', 'the', 'that', 'of', 'the', 'that', 'is', 'the', 'the', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "1m 21s (- 79m 55s) (1250 1%) 5.0265\n",
      "['', 'i', 'is', 'the', 'to', 'is', 'the', 'to', 'the', 'to', 'is', 'the', 'to', 'the', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 23s (- 79m 18s) (1300 1%) 4.9583\n",
      "['', 'that', 'is', 'i', 'is', 'that', 'to', 'is', '<EOS>']\n",
      "\n",
      "1m 26s (- 78m 41s) (1350 1%) 4.8373\n",
      "['', 'i', 'it', 's', 'the', 'a', 'i', 'the', 'the', 'a', 'that', '', '<EOS>']\n",
      "\n",
      "1m 29s (- 78m 25s) (1400 1%) 4.9969\n",
      "['', 'i', 'think', 'it', 's', 'in', 'the', 'in', 'the', 'in', 'the', 'in', 'the', '', '<EOS>']\n",
      "\n",
      "1m 32s (- 78m 1s) (1450 1%) 4.8003\n",
      "['', 'i', 'think', 'it', 'was', 'that', 'i', 'think', 'it', 'was', 'that', '', '<EOS>']\n",
      "\n",
      "1m 35s (- 77m 58s) (1500 2%) 4.9110\n",
      "['', 'i', 'know', 'that', 'i', 'think', 'that', 'is', 'i', 'think', 'the', 'that', '', '<EOS>']\n",
      "\n",
      "1m 37s (- 77m 11s) (1550 2%) 4.6236\n",
      "['', 'i', 'do', 'it', 'is', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "1m 40s (- 77m 5s) (1600 2%) 5.2729\n",
      "['', 'it', 'is', 'a', 'that', 'i', 'think', 'it', 'is', 'a', 'of', 'to', 'have', 'a', '<EOS>']\n",
      "\n",
      "1m 43s (- 77m 2s) (1650 2%) 5.3426\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i']\n",
      "\n",
      "1m 47s (- 77m 11s) (1700 2%) 4.8547\n",
      "['', 'that', 'is', 't', 'that', 'i', 'do', 'you', 'know', 'that', 'that', '', '<EOS>']\n",
      "\n",
      "1m 50s (- 76m 48s) (1750 2%) 4.5347\n",
      "['', 'that', 'is', 'i', 'm', 'i', 'm', 'not', 'have', 'to', 'be', 'have', 'a', 'lot', 'of', '', '<EOS>']\n",
      "\n",
      "1m 52s (- 76m 28s) (1800 2%) 4.5829\n",
      "['', 'i', 'i', 'do', 'you', 'know', 'that', 'the', 'the', 'that', 'the', 'the', '<EOS>']\n",
      "\n",
      "1m 55s (- 76m 11s) (1850 2%) 5.0566\n",
      "['', 'yeah', 'i', 'have', 'a', 'that', 'i', 'to', 'be', 'a', 'that', 'i', '<EOS>']\n",
      "\n",
      "1m 58s (- 75m 50s) (1900 2%) 4.5148\n",
      "['', 'i', 'would', 'i', 'that', 'the', 'i', '', '<EOS>']\n",
      "\n",
      "2m 0s (- 75m 31s) (1950 2%) 4.7370\n",
      "['', 'that', 'is', 'i', 'am', 'a', 'the', '<EOS>']\n",
      "\n",
      "2m 3s (- 75m 15s) (2000 2%) 5.0068\n",
      "['', 'i', 'am', 'the', 'the', 'of', 'the', 'of', 'the', 'of', 'the', 'of', 'the', 'of', 'the', '<EOS>']\n",
      "\n",
      "2m 6s (- 75m 4s) (2050 2%) 4.8490\n",
      "['', 'i', 'would', 'have', 'a', 'the', 'of', 'the', '<EOS>']\n",
      "\n",
      "2m 9s (- 74m 52s) (2100 2%) 4.8172\n",
      "['', 'i', 'would', 'know', 'i', 'know', 'a', 't', 'know', 'that', 's', 'a', 'a', 'the', 'a', '', '<EOS>']\n",
      "\n",
      "2m 12s (- 74m 33s) (2150 2%) 4.7044\n",
      "['', 'i', 'is', 'a', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "2m 14s (- 74m 12s) (2200 2%) 4.2760\n",
      "['', 'i', 'have', 'a', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "2m 17s (- 73m 51s) (2250 3%) 4.7702\n",
      "['', 'that', 's', 'that', 's', 'i', 's', 'to', 'the', 'the', 'you', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "2m 19s (- 73m 41s) (2300 3%) 4.9402\n",
      "['', 'i', 'i', 't', 'know', 'that', 'that', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "2m 22s (- 73m 19s) (2350 3%) 4.7805\n",
      "['', 'i', 'did', 'not', 'know', 'that', 'that', 'the', 'that', 'the', '<EOS>']\n",
      "\n",
      "2m 25s (- 73m 7s) (2400 3%) 4.8109\n",
      "['', 'i', 'can', 't', 'know', 'that', 'i', 'can', 't', 'know', 'that', 'i', 'can', 't', 'know', 'that', 'was', 'the', 'in', 'the', '', '<EOS>']\n",
      "\n",
      "2m 27s (- 72m 52s) (2450 3%) 4.7022\n",
      "['', 'that', 'is', 'so', 'i', 'think', 'the', 'you', '<EOS>']\n",
      "\n",
      "2m 30s (- 72m 31s) (2500 3%) 4.0173\n",
      "['', 'i', 'i', 'i', 'a', 'i', 'i', 'a', 'you', '<EOS>']\n",
      "\n",
      "2m 32s (- 72m 20s) (2550 3%) 4.5393\n",
      "['', 'i', 'i', 'the', 'it', 'the', 'it', 'the', 'it', 'the', 'it', 'in', 'the', 'the', '<EOS>']\n",
      "\n",
      "2m 35s (- 72m 6s) (2600 3%) 4.7295\n",
      "['', 'the', 'the', 'that', 'of', 'the', 'the', 'of', 'the', 'the', 'of', 'the', 'the', 'of', 'the', 'the', 'of', 'the', 'the', 'of', 'the', 'the', 'of', 'the', 'the', 'of', 'the', 'the', 'of', 'the', 'the', 'of', 'the', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m 38s (- 71m 58s) (2650 3%) 5.0524\n",
      "['', 'i', 'know', 'the', 'be', 'the', 'in', 'the', 'the', 'in', 'the', 'the', 'in', 'the', 'the', 'in', 'the', 'the', '<EOS>']\n",
      "\n",
      "2m 41s (- 71m 52s) (2700 3%) 5.1536\n",
      "['', 'i', 'i', 'that', 'the', 'i', 'that', 'of', 'the', 'i', '<EOS>']\n",
      "\n",
      "2m 43s (- 71m 45s) (2750 3%) 4.3484\n",
      "['', 'that', 's', 'a', 'i', 'i', 'to', 'that', 'to', 'that', 'to', 'that', 'to', 'the', 'you', '', '<EOS>']\n",
      "\n",
      "2m 46s (- 71m 33s) (2800 3%) 4.6111\n",
      "['', 'i', 'is', 'is', 'a', 'i', 'i', 'do', 'that', 'is', 'a', 'of', 'the', 'i', 'i', '', 'that', 'is', 'of', 'the', 'the', 'i', '<EOS>']\n",
      "\n",
      "2m 49s (- 71m 25s) (2850 3%) 4.7779\n",
      "['', 'i', 'i', 'a', 'i', 'i', 'a', 'i', 'i', 'a', 'i', 'i', 'a', 'i', 'i', 'to', 'a', '<EOS>']\n",
      "\n",
      "2m 52s (- 71m 24s) (2900 3%) 4.9701\n",
      "['', 'i', 'know', 'that', 'a', 'you', 'know', 'that', 'the', 'the', 'the', 'the', '', 'that', 'is', 'a', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "2m 54s (- 71m 8s) (2950 3%) 4.7563\n",
      "['', 'i', 'guess', 'i', 't', 'have', 'a', 'of', 'the', 'the', 'to', '', '<EOS>']\n",
      "\n",
      "2m 57s (- 71m 3s) (3000 4%) 4.6974\n",
      "['', 'i', 'agree', 'that', 's', 'a', 'lot', 'of', 'the', 'that', 's', 'of', 'the', '<EOS>']\n",
      "\n",
      "3m 0s (- 70m 53s) (3050 4%) 4.6368\n",
      "['', 'yes', 'i', 'think', 'that', 's', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'it', '', '<EOS>']\n",
      "\n",
      "3m 3s (- 70m 54s) (3100 4%) 4.9378\n",
      "['', 'yeah', 'i', 'is', 'the', 'that', 'of', 'the', '<EOS>']\n",
      "\n",
      "3m 6s (- 70m 43s) (3150 4%) 4.7692\n",
      "['', 'i', 'agree', 'i', 'have', 'you', 'know', 'i', 'of', 'the', '', '', '<EOS>']\n",
      "\n",
      "3m 8s (- 70m 39s) (3200 4%) 4.8553\n",
      "['', 'i', 'know', 'if', 'they', 'are', 'the', 'first', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', '<EOS>']\n",
      "\n",
      "3m 11s (- 70m 35s) (3250 4%) 4.7812\n",
      "['', 'that', 'is', 'a', 'good', 'that', 'the', 'i', 'of', 'that', 'the', 'the', 'that', 'of', '', '', '<EOS>']\n",
      "\n",
      "3m 14s (- 70m 31s) (3300 4%) 4.7142\n",
      "['', 'i', 'guess', 'they', 'are', 'you', 'a', 'lot', 'of', 'the', '<EOS>']\n",
      "\n",
      "3m 17s (- 70m 22s) (3350 4%) 4.3865\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'i', 'of', 'the', 'to', 'of', 'the', 'i', '', 'i', '<EOS>']\n",
      "\n",
      "3m 20s (- 70m 12s) (3400 4%) 4.4054\n",
      "['', 'i', 'i', 'the', 'you', 'the', 'you', '', 'the', 'you', '', '<EOS>']\n",
      "\n",
      "3m 22s (- 70m 6s) (3450 4%) 4.6056\n",
      "['', 'i', 'wonder', 'if', 'the', 'was', 'in', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "3m 25s (- 70m 4s) (3500 4%) 4.8331\n",
      "['', 'i', 'is', 'is', 'a', 'that', 'i', 'is', 'a', 'the', 'to', 'that', 'of', 'the', '<EOS>']\n",
      "\n",
      "3m 28s (- 70m 1s) (3550 4%) 5.0806\n",
      "['', 'i', 'is', 's', 'it', 'of', 'the', 'it', 's', 'like', 'the', 'for', 'the', '', '<EOS>']\n",
      "\n",
      "3m 31s (- 69m 56s) (3600 4%) 4.4655\n",
      "['', 'i', 'is', 'that', 'a', 'a', 'lot', 'of', '', '<EOS>']\n",
      "\n",
      "3m 34s (- 69m 49s) (3650 4%) 4.4065\n",
      "['', 'that', 'is', 'is', 'cool', 'i', 'think', 'it', 'is', 'a', 'the', 'you', '', '<EOS>']\n",
      "\n",
      "3m 36s (- 69m 39s) (3700 4%) 4.2456\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "3m 39s (- 69m 29s) (3750 5%) 4.5480\n",
      "['', 'that', 's', 'is', 'true', 'i', 'think', 'it', 'is', 'so', 'that', 's', '<EOS>']\n",
      "\n",
      "3m 42s (- 69m 36s) (3800 5%) 4.4913\n",
      "['', 'i', 'is', 't', 'know', 'that', 'to', 'i', 'the', 'to', 'the', 'a', '<EOS>']\n",
      "\n",
      "3m 46s (- 69m 40s) (3850 5%) 4.3284\n",
      "['', 'that', 'is', 'is', 'the', 'to', 'the', 'the', 'the', 'in', 'that', 'the', '', 'in', 'the', 'the', '<EOS>']\n",
      "\n",
      "3m 49s (- 69m 39s) (3900 5%) 4.3513\n",
      "['', 'i', 'think', 'that', 'is', 'that', 'that', 'the', 'to', 'the', 'the', 'i', '', 'that', 'the', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "3m 52s (- 69m 40s) (3950 5%) 4.5404\n",
      "['', 'i', 'is', 'a', 'he', 'was', 'a', 'he', 'has', 'a', 'a', 'the', '', 'that', 'he', 'was', 'a', 'the', '', '', '<EOS>']\n",
      "\n",
      "3m 55s (- 69m 43s) (4000 5%) 4.2764\n",
      "['', 'i', 's', 's', 'a', 'i', 'i', 'he', 'was', 'a', 'the', '<EOS>']\n",
      "\n",
      "3m 59s (- 69m 47s) (4050 5%) 4.2841\n",
      "['', 'i', 'i', 'like', 'the', 'the', '<EOS>']\n",
      "\n",
      "4m 2s (- 69m 56s) (4100 5%) 4.6965\n",
      "['', 'i', 'is', 'not', 'the', 'i', 'the', 'the', '<EOS>']\n",
      "\n",
      "4m 5s (- 69m 59s) (4150 5%) 4.7233\n",
      "['', 'i', 'think', 'it', 's', 'really', 'interesting', 'i', 'think', 'it', 's', 'really', 'interesting', 'i', 'think', 'it', 's', 'really', 'a', 'lot', 'of', 'the', '<EOS>']\n",
      "\n",
      "4m 9s (- 70m 2s) (4200 5%) 4.5468\n",
      "['', 'that', 'is', 'a', 'good', 'i', 'i', 'i', 'with', 'a', 'the', 'a', 'good', 'and', 'the', 'a', 'good', '', '<EOS>']\n",
      "\n",
      "4m 12s (- 70m 3s) (4250 5%) 4.3532\n",
      "['', 'yes', 'it', 'was', 'a', 'lot', 'of', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "4m 15s (- 70m 5s) (4300 5%) 4.4445\n",
      "['', 'yes', 'i', 'know', 'that', 'i', 'the', 'it', 'of', 'the', 'it', '<EOS>']\n",
      "\n",
      "4m 19s (- 70m 8s) (4350 5%) 4.6353\n",
      "['', 'that', 'is', 'a', 'you', 'in', 'the', 'the', '<EOS>']\n",
      "\n",
      "4m 22s (- 70m 11s) (4400 5%) 4.4531\n",
      "['', 'i', 'is', 'i', 'you', 'know', 'that', 'a', 'you', '', '', '<EOS>']\n",
      "\n",
      "4m 25s (- 70m 13s) (4450 5%) 4.5812\n",
      "['', 'i', 'is', 'that', 'a', 'you', 'that', 'of', 'the', '<EOS>']\n",
      "\n",
      "4m 29s (- 70m 14s) (4500 6%) 4.2077\n",
      "['', 'i', 'didn', 't', 'know', 'that', 'i', 't', 'know', 'that', 'i', 'of', 'a', '<EOS>']\n",
      "\n",
      "4m 32s (- 70m 15s) (4550 6%) 4.5591\n",
      "['', 'i', 'i', 't', 'a', 'i', 'that', 'a', 'you', 'i', '', 'i', '', '<EOS>']\n",
      "\n",
      "4m 35s (- 70m 18s) (4600 6%) 4.5176\n",
      "['', 'i', 'don', 't', 'the', 'i', 'the', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "4m 39s (- 70m 22s) (4650 6%) 4.5724\n",
      "['', 'i', 'that', 's', 'that', 'the', 'you', 'that', 'the', '<EOS>']\n",
      "\n",
      "4m 42s (- 70m 19s) (4700 6%) 4.0797\n",
      "['', 'i', 'm', 'sure', 'i', 'm', 'sure', 'he', 'was', 'an', 'great', '<EOS>']\n",
      "\n",
      "4m 45s (- 70m 16s) (4750 6%) 4.0754\n",
      "['', 'i', 'i', 't', 'know', 'that', 'the', 'i', 'of', 'the', '', '', '', '<EOS>']\n",
      "\n",
      "4m 48s (- 70m 15s) (4800 6%) 4.0680\n",
      "['', 'yeah', 'that', 's', 'the', 'you', 'the', 'that', 'the', 'you', 'the', '<EOS>']\n",
      "\n",
      "4m 51s (- 70m 17s) (4850 6%) 4.0630\n",
      "['', 'i', 'is', 'a', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "4m 55s (- 70m 22s) (4900 6%) 4.8131\n",
      "['', 'i', 'do', 'you', 'know', 'that', 'to', 'it', '', '<EOS>']\n",
      "\n",
      "4m 57s (- 70m 14s) (4950 6%) 4.2956\n",
      "['', 'yes', 'i', 'is', 'i', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "5m 0s (- 70m 13s) (5000 6%) 4.6389\n",
      "['', 'i', 'i', 't', 'that', 'a', 'the', 'i', 'a', 'the', 'i', 'a', 'the', 'i', 'a', 'the', 'i', 'a', 'the', 'i', 'a', 'the', 'i', 'a', 'a', 'the', 'i', 'a', 'the', 'i', '<EOS>']\n",
      "\n",
      "5m 4s (- 70m 13s) (5050 6%) 4.6835\n",
      "['', 'that', 's', 'i', 'the', 'i', 'of', 'the', 'to', 'a', 'the', '<EOS>']\n",
      "\n",
      "5m 7s (- 70m 17s) (5100 6%) 4.2180\n",
      "['', 'yeah', 'i', 'is', 'a', 'i', 'in', '', '<EOS>']\n",
      "\n",
      "5m 10s (- 70m 18s) (5150 6%) 4.0424\n",
      "['', 'i', 'wonder', 'if', 'they', 'have', 'a', 'the', 'the', 'to', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "5m 14s (- 70m 23s) (5200 6%) 4.6989\n",
      "['', 'i', 'i', 't', 'know', 'that', 'to', 'the', 'i', 'to', 'the', 'to', 'i', 'to', 'the', '<EOS>']\n",
      "\n",
      "5m 17s (- 70m 24s) (5250 7%) 4.5410\n",
      "['', 'i', 's', 'i', 'you', 'like', 'to', 'it', 'i', 'to', 'it', 'i', 'to', '<EOS>']\n",
      "\n",
      "5m 21s (- 70m 21s) (5300 7%) 3.8933\n",
      "['', 'i', 'is', 'that', 'is', 'i', 'to', 'it', '<EOS>']\n",
      "\n",
      "5m 24s (- 70m 19s) (5350 7%) 4.1910\n",
      "['', 'i', 'is', 'i', 'that', '', '', '<EOS>']\n",
      "\n",
      "5m 27s (- 70m 15s) (5400 7%) 4.6124\n",
      "['', 'i', 'would', 'have', 'to', 'it', 'i', 'it', 'the', '<EOS>']\n",
      "\n",
      "5m 29s (- 70m 3s) (5450 7%) 3.6926\n",
      "['', 'i', 'i', 'is', 't', 'that', 'the', 'you', '<EOS>']\n",
      "\n",
      "5m 31s (- 69m 53s) (5500 7%) 3.6973\n",
      "['', 'i', 'i', 'is', 'that', 'a', 'that', 'there', 'are', 'the', 'is', 'to', 'the', 'to', '<EOS>']\n",
      "\n",
      "5m 34s (- 69m 47s) (5550 7%) 4.7578\n",
      "['', 'i', 'is', 'is', 'a', 'that', 'the', '', 'to', '', '<EOS>']\n",
      "\n",
      "5m 37s (- 69m 42s) (5600 7%) 4.3725\n",
      "['', 'i', 'do', 'like', 'a', 'lot', 'of', 'the', 'time', '', '<EOS>']\n",
      "\n",
      "5m 40s (- 69m 34s) (5650 7%) 3.9955\n",
      "['', 'yeah', 'i', 'have', 'been', 'to', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "5m 42s (- 69m 29s) (5700 7%) 4.6630\n",
      "['', 'yes', 'it', 'is', 'a', 'lot', 'of', '', '', '<EOS>']\n",
      "\n",
      "5m 45s (- 69m 23s) (5750 7%) 4.5059\n",
      "['', 'i', 'that', 'is', 'that', 'the', 'that', 'is', 'that', 'the', '<EOS>']\n",
      "\n",
      "5m 48s (- 69m 16s) (5800 7%) 4.1885\n",
      "['', 'no', 'i', 'heard', 'that', 'is', 'a', 'that', 'i', 'was', 'a', '<EOS>']\n",
      "\n",
      "5m 51s (- 69m 9s) (5850 7%) 4.3506\n",
      "['', 'i', 'wonder', 'how', 'that', 'the', 'do', 'you', '<EOS>']\n",
      "\n",
      "5m 53s (- 69m 3s) (5900 7%) 4.2119\n",
      "['', 'i', 'wonder', 'i', 'i', 'like', 'to', 'the', 'the', '', 'i', '', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5m 56s (- 68m 53s) (5950 7%) 3.9413\n",
      "['', 'yeah', 'i', 'like', 'to', 'a', 'of', 'the', 'to', '<EOS>']\n",
      "\n",
      "5m 58s (- 68m 46s) (6000 8%) 4.4376\n",
      "['', 'i', 'i', 'a', 'the', 'of', 'the', 'the', 'a', 'the', 'of', 'the', 'the', 'a', 'the', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "6m 1s (- 68m 40s) (6050 8%) 4.2866\n",
      "['', 'i', 'i', 'have', 'you', 'the', 'a', 'the', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "6m 5s (- 68m 44s) (6100 8%) 4.8435\n",
      "['', 'that', 'is', 'a', 'i', 'the', 'you', 'the', '', '', '<EOS>']\n",
      "\n",
      "6m 8s (- 68m 40s) (6150 8%) 4.4695\n",
      "['', 'i', 'is', 'that', 'you', 'a', 'of', '', '<EOS>']\n",
      "\n",
      "6m 11s (- 68m 41s) (6200 8%) 4.1883\n",
      "['', 'i', 'is', 'i', 'that', 'i', 'that', 'i', 'that', 't', 'i', 'that', 'i', 'that', 'i', '<EOS>']\n",
      "\n",
      "6m 15s (- 68m 49s) (6250 8%) 4.6602\n",
      "['', 'i', 'is', 'i', 'of', 'a', 'i', 'of', 'the', '<EOS>']\n",
      "\n",
      "6m 18s (- 68m 51s) (6300 8%) 3.9627\n",
      "['', 'i', 'i', 'i', 'of', 'i', 'of', 'the', '<EOS>']\n",
      "\n",
      "6m 22s (- 68m 53s) (6350 8%) 4.1596\n",
      "['', 'i', 'think', 'i', 'it', 'the', 'to', 'the', 'it', '', '<EOS>']\n",
      "\n",
      "6m 26s (- 68m 59s) (6400 8%) 4.1155\n",
      "['', 'i', 'i', 'is', 'that', 'to', 'i', 'you', 'of', 'the', 'to', '', '<EOS>']\n",
      "\n",
      "6m 29s (- 69m 4s) (6450 8%) 4.8287\n",
      "['', 'i', 'that', 'i', 'that', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "6m 33s (- 69m 7s) (6500 8%) 4.4360\n",
      "['', 'i', 'is', 'i', 'to', 'the', 'the', 'the', 'to', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "6m 36s (- 69m 4s) (6550 8%) 4.4680\n",
      "['', 'yeah', 'i', 'is', 'it', 'the', 'it', 'to', 'the', '', 'i', 'it', '<EOS>']\n",
      "\n",
      "6m 39s (- 68m 58s) (6600 8%) 4.1107\n",
      "['', 'i', 'i', 'i', 'i', 'a', 'a', 'i', 'the', 'a', 'that', 'i', 'the', 'a', 'i', 'the', '<EOS>']\n",
      "\n",
      "6m 42s (- 68m 54s) (6650 8%) 4.6010\n",
      "['', 'i', 'you', 'i', 'it', 'you', 'know', 'that', 'is', 'i', 'it', 'you', 'can', 't', 'you', 'know', 'that', 'the', 'is', 'it', '<EOS>']\n",
      "\n",
      "6m 45s (- 68m 48s) (6700 8%) 4.2690\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'to', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "6m 47s (- 68m 44s) (6750 9%) 4.0972\n",
      "['', 'i', 'think', 'i', 'think', 'that', 'is', 'the', 'is', 'and', '<EOS>']\n",
      "\n",
      "6m 50s (- 68m 41s) (6800 9%) 4.0655\n",
      "['', 'i', 'think', 'that', 'is', 'a', 'that', 'the', 's', 'of', 'the', 's', 'and', '<EOS>']\n",
      "\n",
      "6m 54s (- 68m 40s) (6850 9%) 4.5671\n",
      "['', 'i', 'i', 'i', 'i', 'to', 'i', 'to', 'i', 'to', 'i', 'to', 'i', 'to', 'do', 'i', 'to', 'do', 'i', 'i', 'to', 'do', 'i', 'do', 'i', 'think', 'to', '<EOS>']\n",
      "\n",
      "6m 57s (- 68m 39s) (6900 9%) 4.2649\n",
      "['', 'that', 's', 'the', 'the', 'the', 'the', 'the', 'the', 'and', 'of', 'the', '<EOS>']\n",
      "\n",
      "7m 0s (- 68m 32s) (6950 9%) 4.5174\n",
      "['', 'i', 'i', 't', 'a', 'that', 'to', 'a', 'to', 'that', 'to', '', 'a', 'to', '', '<EOS>']\n",
      "\n",
      "7m 2s (- 68m 27s) (7000 9%) 4.5492\n",
      "['', 'i', 'think', 'that', 's', 'a', 'that', 'to', 'that', 'the', '<EOS>']\n",
      "\n",
      "7m 5s (- 68m 19s) (7050 9%) 3.9602\n",
      "['', 'yes', 'i', 'do', 'like', 'it', 'i', 'like', 'the', 'the', '<EOS>']\n",
      "\n",
      "7m 8s (- 68m 14s) (7100 9%) 4.5200\n",
      "['', 'yeah', 'i', 'is', 'it', 'i', 'the', 'it', 'i', 'it', 'was', 'the', 'the', 'i', '', 'the', 'it', '<EOS>']\n",
      "\n",
      "7m 11s (- 68m 10s) (7150 9%) 4.5827\n",
      "['', 'i', 'i', 'i', 'you', '', 'that', 'a', 'i', '', '<EOS>']\n",
      "\n",
      "7m 13s (- 68m 5s) (7200 9%) 4.0884\n",
      "['', 'i', 'guess', 'it', 's', 'a', 'a', '<EOS>']\n",
      "\n",
      "7m 16s (- 67m 58s) (7250 9%) 3.9754\n",
      "['', 'i', 's', 'interesting', 'that', 'is', 'a', 'a', 'i', '', '<EOS>']\n",
      "\n",
      "7m 19s (- 67m 52s) (7300 9%) 4.0095\n",
      "['', 'i', 'i', 'the', 'the', 'is', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "7m 22s (- 67m 49s) (7350 9%) 4.0686\n",
      "['', 'i', 'that', 'is', 'the', 'i', 'the', 'the', '', 'the', '', 'is', 'the', '', '', '<EOS>']\n",
      "\n",
      "7m 25s (- 67m 50s) (7400 9%) 4.4942\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'it', 'i', 'i', 'in', 'it', 'i', '', 'i', '<EOS>']\n",
      "\n",
      "7m 28s (- 67m 48s) (7450 9%) 4.4836\n",
      "['', 'i', 'do', 'i', 'know', 'that', 'the', 'of', 'the', '<EOS>']\n",
      "\n",
      "7m 31s (- 67m 45s) (7500 10%) 3.9851\n",
      "['', 'yes', 'i', 'saw', 'that', 'i', 'was', 'a', 'that', 'i', 'you', 'to', 'it', 'was', '', '<EOS>']\n",
      "\n",
      "7m 34s (- 67m 43s) (7550 10%) 4.4429\n",
      "['', 'yes', 'i', 'think', 'i', 'the', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "7m 37s (- 67m 39s) (7600 10%) 4.0407\n",
      "['', 'i', 'you', 'i', 'a', 'that', '', '', '<EOS>']\n",
      "\n",
      "7m 40s (- 67m 34s) (7650 10%) 3.8964\n",
      "['', 'i', 's', 'a', 'that', 'i', 'a', 'of', 'a', 'i', 'a', '<EOS>']\n",
      "\n",
      "7m 43s (- 67m 28s) (7700 10%) 4.0998\n",
      "['', 'i', 'i', 't', 'the', 'the', 'the', 'the', 'is', 'the', 'to', '', '<EOS>']\n",
      "\n",
      "7m 45s (- 67m 23s) (7750 10%) 4.5757\n",
      "['', 'i', 'i', 'a', 'a', 'a', 'is', 'i', 'the', 'a', '<EOS>']\n",
      "\n",
      "7m 48s (- 67m 16s) (7800 10%) 4.3660\n",
      "['', 'yes', 'i', 'think', 'that', 'is', 'i', '', 'i', '<EOS>']\n",
      "\n",
      "7m 50s (- 67m 8s) (7850 10%) 3.9588\n",
      "['', 'i', 'that', 'that', 's', 'that', 'that', 's', 'that', 'i', 'to', 'that', 'of', 'that', 'that', 's', 'you', '', '<EOS>']\n",
      "\n",
      "7m 53s (- 67m 2s) (7900 10%) 3.8837\n",
      "['', 'i', 'that', 'that', 'i', 'the', 'is', 'the', 'i', 'the', 'the', '', 'i', 'the', '', 'i', 'the', '<EOS>']\n",
      "\n",
      "7m 56s (- 66m 54s) (7950 10%) 4.5247\n",
      "['', 'i', 'do', 'you', 'like', 'the', '', '<EOS>']\n",
      "\n",
      "7m 58s (- 66m 49s) (8000 10%) 4.2843\n",
      "['', 'i', 'think', 'that', 'is', 'the', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "8m 1s (- 66m 48s) (8050 10%) 4.5617\n",
      "['', 'i', 'i', 'is', 'a', 'i', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "8m 4s (- 66m 44s) (8100 10%) 4.4468\n",
      "['', 'i', 'i', 'a', 'a', 'a', 'a', '<EOS>']\n",
      "\n",
      "8m 7s (- 66m 36s) (8150 10%) 4.4790\n",
      "['', 'i', 'i', 'the', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "8m 9s (- 66m 31s) (8200 10%) 4.0346\n",
      "['', 'yeah', 'that', 's', 'a', 'good', 'to', 'the', 'of', 'the', 'the', 'of', 'the', '<EOS>']\n",
      "\n",
      "8m 12s (- 66m 25s) (8250 11%) 4.3762\n",
      "['', 'i', 'i', 'you', 'the', 'you', 'know', 'the', 'of', 'the', 'of', 'the', 'of', 'the', '', 'and', 'the', '', '<EOS>']\n",
      "\n",
      "8m 15s (- 66m 23s) (8300 11%) 4.6946\n",
      "['', 'i', 's', 'i', 'a', 'that', 'to', 'that', 'i', 'a', 'to', 'is', 'that', 'that', 'you', 'a', 'to', 'is', 'that', '<EOS>']\n",
      "\n",
      "8m 18s (- 66m 17s) (8350 11%) 4.1167\n",
      "['', 'i', 's', 'i', 'i', 'a', 'i', 'the', '<EOS>']\n",
      "\n",
      "8m 20s (- 66m 10s) (8400 11%) 4.0222\n",
      "['', 'i', 'i', 'you', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "8m 23s (- 66m 3s) (8450 11%) 3.7551\n",
      "['', 'i', 'i', 'a', 'a', 'a', 'a', 'the', '<EOS>']\n",
      "\n",
      "8m 25s (- 65m 56s) (8500 11%) 4.2726\n",
      "['', 'i', 's', 'is', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "8m 28s (- 65m 50s) (8550 11%) 3.8963\n",
      "['', 'i', 'i', 'i', 'the', 'i', 'the', 'the', 'it', 'to', 'the', '<EOS>']\n",
      "\n",
      "8m 30s (- 65m 43s) (8600 11%) 4.1732\n",
      "['', 'i', 'guess', 't', 'know', 'that', 'i', 'he', 'is', 'a', 'the', '<EOS>']\n",
      "\n",
      "8m 33s (- 65m 36s) (8650 11%) 3.9817\n",
      "['', 'i', 'i', 'a', 'a', 'a', 'to', 'a', 'the', '<EOS>']\n",
      "\n",
      "8m 35s (- 65m 31s) (8700 11%) 3.8861\n",
      "['', 'i', 'i', 'i', 'a', 'the', 'you', 'the', 'the', '<EOS>']\n",
      "\n",
      "8m 38s (- 65m 28s) (8750 11%) 4.1300\n",
      "['', 'i', 'i', 'is', 'a', 'the', 'you', 'i', 'the', 'it', '', 'i', '<EOS>']\n",
      "\n",
      "8m 41s (- 65m 24s) (8800 11%) 4.6728\n",
      "['', 'i', 'you', 'the', 'the', 'to', 'the', 'the', 'the', 'it', 'to', 'the', 'the', '<EOS>']\n",
      "\n",
      "8m 44s (- 65m 20s) (8850 11%) 4.6378\n",
      "['', 'i', 'is', 'that', 'i', 'to', 'the', 'that', 'and', 'i', '<EOS>']\n",
      "\n",
      "8m 47s (- 65m 18s) (8900 11%) 4.2005\n",
      "['', 'i', 'is', 'i', 'i', 'to', 'i', 'the', 'to', 'i', '<EOS>']\n",
      "\n",
      "8m 50s (- 65m 14s) (8950 11%) 4.3359\n",
      "['', 'i', 'i', 'i', 'to', 'i', 'to', 'i', 'to', 'i', 'to', 'i', '<EOS>']\n",
      "\n",
      "8m 53s (- 65m 11s) (9000 12%) 4.5012\n",
      "['', 'i', 'i', 'a', 'you', 'to', 'a', 'to', 'to', 'a', 'to', '', 'to', 'the', '', 'to', '', '<EOS>']\n",
      "\n",
      "8m 56s (- 65m 8s) (9050 12%) 4.3382\n",
      "['', 'i', 'i', 'you', 'a', 'a', 'a', 'a', 'you', 'a', 'a', '<EOS>']\n",
      "\n",
      "8m 59s (- 65m 5s) (9100 12%) 4.4028\n",
      "['', 'i', 'i', 'a', 'to', 'a', 'the', 'to', '', 'to', 'the', '<EOS>']\n",
      "\n",
      "9m 2s (- 65m 1s) (9150 12%) 4.2954\n",
      "['', 'i', 'guess', 'that', 'is', 'a', 'the', 'you', '<EOS>']\n",
      "\n",
      "9m 5s (- 64m 57s) (9200 12%) 4.1617\n",
      "['', 'i', 'am', 'not', 'sure', 'to', 'the', 'i', 'i', 'to', 'a', '<EOS>']\n",
      "\n",
      "9m 8s (- 64m 55s) (9250 12%) 4.4657\n",
      "['', 'i', 'i', 'a', 'i', 'i', 'i', 'a', 'i', 'the', 'i', 'i', 'to', 'the', 'that', 'i', '<EOS>']\n",
      "\n",
      "9m 11s (- 64m 53s) (9300 12%) 4.2588\n",
      "['', 'i', 'do', 'it', 'i', 'a', 'it', '<EOS>']\n",
      "\n",
      "9m 13s (- 64m 49s) (9350 12%) 4.1974\n",
      "['', 'i', 'would', 'be', 'a', 'that', '', '<EOS>']\n",
      "\n",
      "9m 17s (- 64m 47s) (9400 12%) 4.2949\n",
      "['', 'i', 'think', 'i', 'the', 'of', 'the', 'a', '<EOS>']\n",
      "\n",
      "9m 20s (- 64m 45s) (9450 12%) 3.9675\n",
      "['', 'i', 'i', 'the', 'that', 'the', 'i', 'the', 'that', '', 'the', '<EOS>']\n",
      "\n",
      "9m 22s (- 64m 40s) (9500 12%) 4.1328\n",
      "['', 'i', 'do', 'like', 'i', 'you', 'like', '', '<EOS>']\n",
      "\n",
      "9m 26s (- 64m 39s) (9550 12%) 4.1323\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'it', 'of', 'a', 'of', 'it', 'a', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9m 29s (- 64m 39s) (9600 12%) 4.0759\n",
      "['', 'i', 'i', 'the', 'i', 'to', 'be', 'the', '', '', '<EOS>']\n",
      "\n",
      "9m 32s (- 64m 39s) (9650 12%) 4.5993\n",
      "['', 'i', 'i', 'i', 'the', 'the', 'i', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "9m 35s (- 64m 37s) (9700 12%) 4.2049\n",
      "['', 'i', 'i', 'i', 'a', 'the', 'that', 'the', 'that', 'the', '', '<EOS>']\n",
      "\n",
      "9m 38s (- 64m 34s) (9750 13%) 4.2297\n",
      "['', 'i', 's', 'i', 'you', 'i', 'the', 'you', '', '<EOS>']\n",
      "\n",
      "9m 41s (- 64m 31s) (9800 13%) 4.1379\n",
      "['', 'i', 'agree', 'i', 'a', 'you', 'i', 'to', 'a', 'a', 'of', 'a', '<EOS>']\n",
      "\n",
      "9m 44s (- 64m 27s) (9850 13%) 4.4195\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'to', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "9m 47s (- 64m 21s) (9900 13%) 3.8485\n",
      "['', 'i', 'think', 'it', 'i', 'a', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "9m 49s (- 64m 16s) (9950 13%) 4.1454\n",
      "['', 'i', 'i', 'to', 'i', 'the', 'the', 'the', 'i', 'the', 'to', 'the', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "9m 52s (- 64m 14s) (10000 13%) 4.3418\n",
      "['', 'i', 'think', 'it', 'is', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "9m 55s (- 64m 8s) (10050 13%) 4.3406\n",
      "['', 'i', 'think', 'that', 'i', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "9m 58s (- 64m 8s) (10100 13%) 4.7531\n",
      "['', 'i', 'i', 'i', 'i', 'to', 'the', 'i', 'the', 'i', 'the', 'the', 'i', 'to', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "10m 1s (- 64m 4s) (10150 13%) 4.0214\n",
      "['', 'i', 'i', 'i', 'the', 'i', 'the', 'i', 'the', 'i', 'the', 'i', 'the', 'i', 'the', 'i', 'the', 'i', 'the', 'i', 'the', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "10m 4s (- 64m 1s) (10200 13%) 4.6131\n",
      "['', 'i', 'is', 'a', 'to', 'you', 'to', 'know', 'i', 'to', 'to', 'the', '', '', '<EOS>']\n",
      "\n",
      "10m 7s (- 63m 55s) (10250 13%) 4.1238\n",
      "['', 'i', 'i', 'the', 'to', 'the', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "10m 10s (- 63m 52s) (10300 13%) 4.1719\n",
      "['', 'i', 'is', 'a', 'i', 'a', 'the', 'of', 'a', 'is', 'the', 'a', 'in', 'the', '', '<EOS>']\n",
      "\n",
      "10m 12s (- 63m 47s) (10350 13%) 4.2212\n",
      "['', 'i', 'i', 'not', 'the', 'the', 'to', 'the', 'the', 'the', 'to', 'the', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "10m 15s (- 63m 42s) (10400 13%) 4.3279\n",
      "['', 'i', 'i', 'not', 'you', 'to', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "10m 18s (- 63m 38s) (10450 13%) 4.4913\n",
      "['', 'i', 'i', 't', 'like', 'that', 'you', 'a', 'i', 'to', 'a', 'i', '<EOS>']\n",
      "\n",
      "10m 21s (- 63m 35s) (10500 14%) 4.5440\n",
      "['', 'i', 'is', 'a', 'i', 'to', 'that', 'a', 'to', 'to', 'in', '', '<EOS>']\n",
      "\n",
      "10m 23s (- 63m 31s) (10550 14%) 4.4690\n",
      "['', 'i', 'is', 'not', 'you', 'the', '', '', '', '<EOS>']\n",
      "\n",
      "10m 26s (- 63m 27s) (10600 14%) 4.5398\n",
      "['', 'i', 'i', 't', 'that', 'the', 'the', 'i', 'the', 'the', 'i', 'the', 'the', 'the', 'i', 'to', '<EOS>']\n",
      "\n",
      "10m 29s (- 63m 22s) (10650 14%) 4.3494\n",
      "['', 'i', 'is', 'i', 'a', 'i', 'to', '<EOS>']\n",
      "\n",
      "10m 31s (- 63m 14s) (10700 14%) 3.5246\n",
      "['', 'i', 'is', 'i', 'a', 'is', 'a', '', 'the', '', '<EOS>']\n",
      "\n",
      "10m 33s (- 63m 8s) (10750 14%) 4.1068\n",
      "['', 'i', 'i', 'that', 'of', 'you', 'of', 'the', 'i', '', '<EOS>']\n",
      "\n",
      "10m 36s (- 63m 4s) (10800 14%) 4.4133\n",
      "['', 'i', 'do', 'i', 'the', 'to', '<EOS>']\n",
      "\n",
      "10m 39s (- 63m 1s) (10850 14%) 4.3127\n",
      "['', 'i', 'i', 'i', 'the', 'i', 'to', 'the', 'i', 'to', 'the', 'i', '<EOS>']\n",
      "\n",
      "10m 42s (- 62m 58s) (10900 14%) 4.3374\n",
      "['', 'i', 'i', 'a', 'the', 'that', 'the', 'the', '<EOS>']\n",
      "\n",
      "10m 45s (- 62m 55s) (10950 14%) 3.9753\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "10m 48s (- 62m 51s) (11000 14%) 4.7841\n",
      "['', 'i', 'i', 'it', 'it', 'it', '', 'it', '', 'it', '', 'it', '<EOS>']\n",
      "\n",
      "10m 50s (- 62m 46s) (11050 14%) 4.3632\n",
      "['', 'i', 'i', 'a', '', 'the', '<EOS>']\n",
      "\n",
      "10m 53s (- 62m 43s) (11100 14%) 4.0895\n",
      "['', 'i', 'did', 'know', 'that', 'the', 'movie', 'that', 'is', 'a', 'in', '<EOS>']\n",
      "\n",
      "10m 56s (- 62m 36s) (11150 14%) 3.8938\n",
      "['', 'i', 'i', 'a', 'i', 'a', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "10m 58s (- 62m 33s) (11200 14%) 4.1452\n",
      "['', 'i', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'i', 'the', 'the', 'the', 'the', '', 'the', 'i', '', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "11m 1s (- 62m 28s) (11250 15%) 4.2502\n",
      "['', 'i', 'i', 'is', 'so', 'do', 'you', 'like', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "11m 4s (- 62m 25s) (11300 15%) 4.2140\n",
      "['', 'i', 'is', 'is', 'i', 'the', 'a', 'the', 'the', 'i', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "11m 7s (- 62m 22s) (11350 15%) 4.6880\n",
      "['', 'i', 'that', 'is', 'a', 'a', 'that', 'a', 'and', 'i', '', 'a', '<EOS>']\n",
      "\n",
      "11m 10s (- 62m 19s) (11400 15%) 4.3263\n",
      "['', 'i', 'i', 'i', 'a', 'i', 'that', 'the', '', '<EOS>']\n",
      "\n",
      "11m 12s (- 62m 14s) (11450 15%) 3.9611\n",
      "['', 'i', 'i', 'i', 'i', 'that', 'i', 'a', 'i', '', '<EOS>']\n",
      "\n",
      "11m 15s (- 62m 11s) (11500 15%) 4.4086\n",
      "['', 'i', 'i', 'is', 'a', 'a', 'i', 'to', 'a', 'the', 'to', 'i', 'a', '<EOS>']\n",
      "\n",
      "11m 18s (- 62m 9s) (11550 15%) 4.6287\n",
      "['', 'i', 'i', 'i', 'that', 'that', 'of', 'the', 'i', 'that', 'the', 'i', 'of', 'the', 'that', 'the', '', 'i', '', 'i', 'i', 'that', 'of', 'the', '', '', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "11m 21s (- 62m 6s) (11600 15%) 4.5141\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'of', '', '', '', '<EOS>']\n",
      "\n",
      "11m 24s (- 62m 4s) (11650 15%) 4.0571\n",
      "['', 'i', 'm', 'not', 'a', 'good', 'fan', 'of', 'the', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "11m 28s (- 62m 5s) (11700 15%) 4.7522\n",
      "['', 'i', 'm', 'not', 'a', 'the', 'i', 'a', '', '', '<EOS>']\n",
      "\n",
      "11m 31s (- 62m 3s) (11750 15%) 4.2386\n",
      "['', 'i', 'i', 'is', 'you', 'i', '', '', '<EOS>']\n",
      "\n",
      "11m 35s (- 62m 3s) (11800 15%) 4.2726\n",
      "['', 'i', 'i', 't', 'i', 'it', '', '', '', '<EOS>']\n",
      "\n",
      "11m 38s (- 62m 1s) (11850 15%) 4.2365\n",
      "['', 'i', 'i', 'it', 's', 'that', 'a', 'of', 'that', '', '', '', '<EOS>']\n",
      "\n",
      "11m 41s (- 62m 1s) (11900 15%) 4.5976\n",
      "['', 'i', 'i', 'it', 'i', 'you', 'i', 'i', 'to', 'the', 'i', '', 'it', '<EOS>']\n",
      "\n",
      "11m 45s (- 62m 0s) (11950 15%) 4.6898\n",
      "['', 'i', 'i', 'a', 'i', 'a', 'the', 'i', '', 'a', '', '<EOS>']\n",
      "\n",
      "11m 48s (- 61m 57s) (12000 16%) 3.8840\n",
      "['', 'i', 'i', 'it', 'it', 's', 'a', 'i', 'i', 'it', '<EOS>']\n",
      "\n",
      "11m 51s (- 61m 56s) (12050 16%) 4.0606\n",
      "['', 'yeah', 'i', 's', 'a', 'it', 'i', 'a', '<EOS>']\n",
      "\n",
      "11m 54s (- 61m 55s) (12100 16%) 3.9579\n",
      "['', 'i', 'don', 't', 'know', 'how', 'they', 'did', 'you', 'know', 'that', 'the', 'best', 'of', 'the', '<EOS>']\n",
      "\n",
      "11m 58s (- 61m 54s) (12150 16%) 4.6943\n",
      "['', 'yeah', 'i', 'do', 'it', 'a', 'a', 'a', 'a', '', 'to', 'a', '', 'a', '', 'and', 'a', '', '<EOS>']\n",
      "\n",
      "12m 1s (- 61m 54s) (12200 16%) 4.6094\n",
      "['', 'i', 'have', 'that', 'you', 'a', 'a', 'a', 'a', '', 'a', '', '<EOS>']\n",
      "\n",
      "12m 4s (- 61m 49s) (12250 16%) 4.0749\n",
      "['', 'i', 'i', 'i', 'you', 'the', 'to', 'i', '', 'a', '', '', '<EOS>']\n",
      "\n",
      "12m 7s (- 61m 46s) (12300 16%) 4.4063\n",
      "['', 'i', 'i', 'that', 'and', 'that', 'i', '<EOS>']\n",
      "\n",
      "12m 9s (- 61m 42s) (12350 16%) 4.2837\n",
      "['', 'i', 'i', 'you', 'the', 'the', 'i', 'of', 'the', '<EOS>']\n",
      "\n",
      "12m 12s (- 61m 37s) (12400 16%) 3.9304\n",
      "['', 'i', 'is', 'i', 'a', 'you', 'i', 'the', 'it', 'of', 'the', 'to', 'the', 'it', '<EOS>']\n",
      "\n",
      "12m 15s (- 61m 32s) (12450 16%) 4.6071\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'you', 'the', '<EOS>']\n",
      "\n",
      "12m 17s (- 61m 27s) (12500 16%) 3.4840\n",
      "['', 'i', 'would', 'not', 'be', 'that', 'that', 'to', 'that', 'i', '<EOS>']\n",
      "\n",
      "12m 20s (- 61m 22s) (12550 16%) 4.3222\n",
      "['', 'i', 'i', 'the', 'you', 'the', 'a', 'the', 'the', 'a', 'the', 'to', 'the', '', '', '<EOS>']\n",
      "\n",
      "12m 22s (- 61m 18s) (12600 16%) 4.3464\n",
      "['', 'i', 'i', 'i', 'you', 'are', 'you', 'the', 'you', 'they', 'are', 'the', 'i', 'the', 'i', '', '', '<EOS>']\n",
      "\n",
      "12m 25s (- 61m 13s) (12650 16%) 4.3930\n",
      "['', 'i', 'i', 'i', 'the', 'you', 'the', '', '', '<EOS>']\n",
      "\n",
      "12m 27s (- 61m 9s) (12700 16%) 3.8397\n",
      "['', 'i', 'i', 'a', 'i', 'a', 'the', 'a', 'i', '<EOS>']\n",
      "\n",
      "12m 30s (- 61m 4s) (12750 17%) 3.8902\n",
      "['', 'i', 'i', 'not', 'but', 'it', 'is', 'the', 'i', 'to', '<EOS>']\n",
      "\n",
      "12m 33s (- 61m 1s) (12800 17%) 4.3489\n",
      "['', 'yes', 'that', 'is', 'a', 'great', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "12m 35s (- 60m 54s) (12850 17%) 3.6067\n",
      "['', 'i', 'i', 'i', 'you', 'i', '<EOS>']\n",
      "\n",
      "12m 38s (- 60m 49s) (12900 17%) 3.9258\n",
      "['', 'i', 'i', 'a', 'i', 'you', 'the', 'a', 'the', 'a', '<EOS>']\n",
      "\n",
      "12m 40s (- 60m 45s) (12950 17%) 4.3547\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "12m 43s (- 60m 40s) (13000 17%) 4.2674\n",
      "['', 'i', 'i', 't', 'i', 'the', 'a', 'i', 'of', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "12m 46s (- 60m 36s) (13050 17%) 4.2348\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'that', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'that', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'that', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'that', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'that', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12m 48s (- 60m 33s) (13100 17%) 3.9963\n",
      "['', 'yes', 'you', 'are', 'a', 'the', 'the', 'in', 'the', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "12m 51s (- 60m 28s) (13150 17%) 4.4111\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'the', 'and', 'the', 'that', 'and', 'the', '<EOS>']\n",
      "\n",
      "12m 54s (- 60m 25s) (13200 17%) 4.4594\n",
      "['', 'i', 'i', 'you', 'to', 'i', 'to', 'i', 'to', 'the', '', 'to', '', 'i', 'to', '', '<EOS>']\n",
      "\n",
      "12m 57s (- 60m 22s) (13250 17%) 4.5856\n",
      "['', 'i', 'he', 's', 'a', 'lot', 'of', 'that', 'i', '<EOS>']\n",
      "\n",
      "12m 59s (- 60m 18s) (13300 17%) 4.1911\n",
      "['', 'i', 'i', 'a', 'i', 'a', 'i', 'i', 'a', '<EOS>']\n",
      "\n",
      "13m 2s (- 60m 14s) (13350 17%) 4.1123\n",
      "['', 'i', 'i', 'i', 'you', 'the', 'the', 'i', '', '<EOS>']\n",
      "\n",
      "13m 5s (- 60m 10s) (13400 17%) 3.8645\n",
      "['', 'i', 'i', 'a', 'i', 'i', 'a', 'that', 'i', 'the', 'a', '<EOS>']\n",
      "\n",
      "13m 8s (- 60m 6s) (13450 17%) 3.8610\n",
      "['', 'i', 'i', 'you', 'a', 'of', 'i', '<EOS>']\n",
      "\n",
      "13m 10s (- 60m 2s) (13500 18%) 4.0996\n",
      "['', 'i', 'i', 'you', 'the', 'a', 't', 'a', '', 'the', '', '<EOS>']\n",
      "\n",
      "13m 13s (- 59m 58s) (13550 18%) 4.2391\n",
      "['', 'i', 'i', 'i', 'you', 'the', '', 'a', '', '', '<EOS>']\n",
      "\n",
      "13m 16s (- 59m 55s) (13600 18%) 3.8174\n",
      "['', 'i', 'i', 'i', 'i', 'to', 'do', 'a', 'of', '', '<EOS>']\n",
      "\n",
      "13m 19s (- 59m 51s) (13650 18%) 4.1681\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'i', 'to', 'the', 'a', 'i', '<EOS>']\n",
      "\n",
      "13m 21s (- 59m 47s) (13700 18%) 4.4368\n",
      "['', 'i', 'love', 'that', 'i', 'wonder', 'if', 'i', 'wonder', 'if', 'i', 'was', 'the', 'a', '<EOS>']\n",
      "\n",
      "13m 24s (- 59m 44s) (13750 18%) 4.2532\n",
      "['', 'i', 'guess', 'i', 'that', 'the', 'of', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "13m 27s (- 59m 40s) (13800 18%) 4.0694\n",
      "['', 'i', 'was', 'a', 'good', 'i', 'i', 'the', 'to', 'the', '', '', '<EOS>']\n",
      "\n",
      "13m 30s (- 59m 37s) (13850 18%) 4.5239\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "13m 33s (- 59m 33s) (13900 18%) 4.0457\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'the', '<EOS>']\n",
      "\n",
      "13m 36s (- 59m 31s) (13950 18%) 4.4938\n",
      "['', 'i', 'i', 't', 'a', 'i', 'the', 'a', 'the', 'a', 'i', 'the', '<EOS>']\n",
      "\n",
      "13m 38s (- 59m 27s) (14000 18%) 4.1087\n",
      "['', 'i', 'i', 'that', 't', 'that', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "13m 41s (- 59m 23s) (14050 18%) 3.9679\n",
      "['', 'i', 'i', 'i', 'you', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "13m 43s (- 59m 18s) (14100 18%) 4.3204\n",
      "['', 'i', 'i', 'that', 'to', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "13m 46s (- 59m 15s) (14150 18%) 4.3413\n",
      "['', 'i', 'i', 'i', 'a', 'a', 'you', 'a', 'i', 'a', 'a', 'i', 'a', 'i', '', 'a', 'a', '<EOS>']\n",
      "\n",
      "13m 49s (- 59m 12s) (14200 18%) 4.2101\n",
      "['', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'i', 'i', '<EOS>']\n",
      "\n",
      "13m 52s (- 59m 11s) (14250 19%) 4.2556\n",
      "['', 'i', 'i', 't', 'a', 'that', 'a', 'i', '<EOS>']\n",
      "\n",
      "13m 56s (- 59m 11s) (14300 19%) 4.4613\n",
      "['', 'i', 'i', 'i', 't', 'i', 'i', 'you', 'that', 'i', 'that', 'you', '<EOS>']\n",
      "\n",
      "13m 59s (- 59m 6s) (14350 19%) 4.3437\n",
      "['', 'i', 'do', 'it', 'you', 'the', 'the', 'the', 'a', 'the', 'the', 'a', 'the', '<EOS>']\n",
      "\n",
      "14m 1s (- 59m 2s) (14400 19%) 4.2209\n",
      "['', 'i', 'do', 'i', 'you', 'i', 'you', 'that', 'a', 'you', '<EOS>']\n",
      "\n",
      "14m 4s (- 59m 0s) (14450 19%) 4.2203\n",
      "['', 'i', 'i', 'i', 'i', 'to', 'you', 'i', 'of', 'the', '<EOS>']\n",
      "\n",
      "14m 7s (- 58m 56s) (14500 19%) 4.2636\n",
      "['', 'i', 'love', 'the', 'same', 'time', 'i', 'love', 'the', 'same', 'time', 'i', 'love', 'the', 'same', 'time', '', '<EOS>']\n",
      "\n",
      "14m 10s (- 58m 52s) (14550 19%) 3.8302\n",
      "['', 'i', 'do', 'i', 'a', 'you', 'the', '', '', 'a', '', '<EOS>']\n",
      "\n",
      "14m 13s (- 58m 50s) (14600 19%) 3.7184\n",
      "['', 'i', 's', 'i', 'you', 'that', '<EOS>']\n",
      "\n",
      "14m 15s (- 58m 46s) (14650 19%) 3.6503\n",
      "['', 'i', 'know', 'i', 'it', 'that', 'that', 'you', 'the', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "14m 18s (- 58m 42s) (14700 19%) 4.1459\n",
      "['', 'i', 'i', 't', 'a', 'i', 'the', 'you', 'a', '<EOS>']\n",
      "\n",
      "14m 21s (- 58m 39s) (14750 19%) 3.7412\n",
      "['', 'i', 'i', 'is', 'a', 'i', 'i', 'i', 'a', 'i', 'i', '<EOS>']\n",
      "\n",
      "14m 24s (- 58m 35s) (14800 19%) 4.1140\n",
      "['', 'i', 'i', 'i', 'i', 'the', 'i', 'the', 'the', 'i', 'of', 'the', '<EOS>']\n",
      "\n",
      "14m 27s (- 58m 32s) (14850 19%) 4.2367\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'the', 'i', 'the', 'i', 'the', 'the', 'i', 'the', 'i', 'the', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "14m 29s (- 58m 28s) (14900 19%) 4.0579\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'it', '', 'i', '<EOS>']\n",
      "\n",
      "14m 32s (- 58m 24s) (14950 19%) 4.1297\n",
      "['', 'i', 'i', 'a', 'a', 'i', '', 'a', '', 'i', '', 'a', '', '<EOS>']\n",
      "\n",
      "14m 35s (- 58m 22s) (15000 20%) 3.8560\n",
      "['', 'i', 'do', 'i', 'you', 'a', 't', 'i', '', 'i', '', 'i', '', '<EOS>']\n",
      "\n",
      "14m 38s (- 58m 18s) (15050 20%) 3.8550\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "14m 41s (- 58m 15s) (15100 20%) 3.9982\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'it', 'i', 'i', 'that', 'i', '', 'i', 'it', '', 'i', '<EOS>']\n",
      "\n",
      "14m 43s (- 58m 12s) (15150 20%) 3.9915\n",
      "['', 'i', 'do', 'not', 'i', 'that', 'that', 'the', 'the', '<EOS>']\n",
      "\n",
      "14m 47s (- 58m 9s) (15200 20%) 3.8818\n",
      "['', 'i', 'is', 'i', 'you', 'that', 'i', 'the', 'you', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "14m 49s (- 58m 5s) (15250 20%) 4.1184\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'that', 'i', '<EOS>']\n",
      "\n",
      "14m 52s (- 58m 3s) (15300 20%) 4.4291\n",
      "['', 'i', 'i', 'i', 'you', 'a', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "14m 55s (- 58m 0s) (15350 20%) 4.2780\n",
      "['', 'i', 'i', 'i', 'you', 'a', 'i', 'that', 'you', 'a', 'i', '', 'the', '', 'a', '', '<EOS>']\n",
      "\n",
      "14m 58s (- 57m 56s) (15400 20%) 4.5194\n",
      "['', 'i', 'i', 'a', 'you', 'a', 'of', 'a', 'and', '', '<EOS>']\n",
      "\n",
      "15m 1s (- 57m 53s) (15450 20%) 4.1388\n",
      "['', 'i', 'i', 't', 'know', 'i', 'you', 'the', 'the', 'you', '', 'the', '', '<EOS>']\n",
      "\n",
      "15m 4s (- 57m 51s) (15500 20%) 3.9611\n",
      "['', 'i', 'i', 't', 'i', 'i', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "15m 6s (- 57m 47s) (15550 20%) 4.0746\n",
      "['', 'i', 'did', 'not', 'know', 'i', 'know', 'that', 'the', 'you', 'the', 'the', 'i', 'the', 'the', 'you', '<EOS>']\n",
      "\n",
      "15m 10s (- 57m 45s) (15600 20%) 4.6828\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "15m 12s (- 57m 41s) (15650 20%) 4.5997\n",
      "['', 'i', 'heard', 'about', 'that', 'i', 'm', 'not', 'sure', 'i', 'like', 'that', 'the', '', '<EOS>']\n",
      "\n",
      "15m 15s (- 57m 37s) (15700 20%) 3.7442\n",
      "['', 'i', 'i', 'not', 'i', 'of', 'you', 'a', 'of', 'in', 'the', 'a', 'of', 'in', 'the', '<EOS>']\n",
      "\n",
      "15m 18s (- 57m 34s) (15750 21%) 4.2088\n",
      "['', 'i', 'wonder', 'if', 'i', 'was', 'a', 'i', 'in', 'to', 'i', 'the', 'a', 'i', '<EOS>']\n",
      "\n",
      "15m 21s (- 57m 32s) (15800 21%) 4.3974\n",
      "['', 'i', 'is', 'i', 'i', 'to', 'it', 'i', 'was', 'was', 'the', 'a', 'i', '', 'i', 'was', 'the', '', 'i', '<EOS>']\n",
      "\n",
      "15m 24s (- 57m 28s) (15850 21%) 4.3858\n",
      "['', 'yeah', 'you', 'like', 'to', 'to', 'is', 'to', 'in', 'the', '', '<EOS>']\n",
      "\n",
      "15m 27s (- 57m 26s) (15900 21%) 4.0485\n",
      "['', 'i', 'i', 'the', 'you', 'the', 'the', 'i', 'the', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "15m 30s (- 57m 26s) (15950 21%) 4.0764\n",
      "['', 'i', 'i', 'to', 'the', 'the', 'i', 'the', 'to', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "15m 34s (- 57m 24s) (16000 21%) 4.4182\n",
      "['', 'i', 'i', 'i', 'the', 'you', 'the', 'the', 'the', 'the', 'that', 'the', '<EOS>']\n",
      "\n",
      "15m 37s (- 57m 23s) (16050 21%) 4.3930\n",
      "['', 'i', 'i', 'a', 'i', 'a', 'you', 'a', 'a', 'a', 'i', '', 'a', 'a', '', 'a', 'a', '', 'a', '', 'a', '<EOS>']\n",
      "\n",
      "15m 40s (- 57m 19s) (16100 21%) 3.9369\n",
      "['', 'i', 'it', 'is', 'the', 'the', 'you', 'a', 'the', 'the', 'the', 'a', 'the', 'the', '', 'the', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "15m 43s (- 57m 16s) (16150 21%) 4.1814\n",
      "['', 'i', 'i', 'a', 'you', 'a', 'the', 'a', 'i', '', 'a', '', '', '', '<EOS>']\n",
      "\n",
      "15m 45s (- 57m 13s) (16200 21%) 4.4869\n",
      "['', 'i', 'i', 'a', 'a', 'i', 'a', 'the', 'a', 'i', '', 'a', '', 'a', '<EOS>']\n",
      "\n",
      "15m 48s (- 57m 9s) (16250 21%) 4.5683\n",
      "['', 'yes', 'i', 'is', 'is', 'the', 'you', 'the', '', 'a', 'the', '', '', '<EOS>']\n",
      "\n",
      "15m 51s (- 57m 6s) (16300 21%) 4.2320\n",
      "['', 'i', 'i', 'you', 'a', 'you', 'a', 'that', 'you', 'a', 'a', 'that', '', '', 'a', '', '', '<EOS>']\n",
      "\n",
      "15m 54s (- 57m 2s) (16350 21%) 4.7047\n",
      "['', 'i', 'i', 'you', 'you', 'that', 'you', 'the', 'that', 'you', 'have', 'to', 'you', 'you', 'the', 'you', '', '', '<EOS>']\n",
      "\n",
      "15m 57s (- 57m 2s) (16400 21%) 4.3831\n",
      "['', 'i', 'i', 'you', 'a', 'you', 'a', 'you', 'a', '', '', 'a', '', '', '', 'a', '', '', '', '', '<EOS>']\n",
      "\n",
      "16m 0s (- 56m 59s) (16450 21%) 4.3853\n",
      "['', 'i', 'i', 'you', 'you', 'a', 'in', '', 'a', '', '', '', '', '', '<EOS>']\n",
      "\n",
      "16m 3s (- 56m 56s) (16500 22%) 4.2022\n",
      "['', 'i', 'i', 'i', 'i', 'to', 'it', 'i', 'to', '<EOS>']\n",
      "\n",
      "16m 6s (- 56m 51s) (16550 22%) 3.7732\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'i', 'the', 'the', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16m 8s (- 56m 47s) (16600 22%) 3.9394\n",
      "['', 'i', 'it', 'is', 'it', 'to', 'it', 'the', 'it', 's', 'the', '<EOS>']\n",
      "\n",
      "16m 11s (- 56m 45s) (16650 22%) 4.4050\n",
      "['', 'i', 'i', 'a', 'you', 'a', 'that', 'the', 'that', 'the', 'that', 'the', '<EOS>']\n",
      "\n",
      "16m 14s (- 56m 41s) (16700 22%) 4.3304\n",
      "['', 'i', 'i', 'the', 'the', 'i', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "16m 16s (- 56m 37s) (16750 22%) 3.9340\n",
      "['', 'i', 'i', 'i', 'you', 'a', 'you', 'the', 'a', 'and', 'i', 'the', 'a', 'i', '<EOS>']\n",
      "\n",
      "16m 19s (- 56m 34s) (16800 22%) 4.1723\n",
      "['', 'i', 'i', 'it', 'it', '', 'the', '', 'the', 'a', '', 'the', '<EOS>']\n",
      "\n",
      "16m 22s (- 56m 31s) (16850 22%) 3.9486\n",
      "['', 'i', 'i', 'i', 'you', 'the', 'to', 'you', 'the', 'to', 'the', 'a', '', 'to', 'the', '', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "16m 25s (- 56m 29s) (16900 22%) 4.8585\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'the', 'i', 'the', 'the', 'i', 'the', 'a', 'the', 'to', '', 'the', 'the', '', 'to', '<EOS>']\n",
      "\n",
      "16m 28s (- 56m 26s) (16950 22%) 3.9710\n",
      "['', 'i', 'you', 't', 'i', 'it', 'you', '', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "16m 31s (- 56m 23s) (17000 22%) 4.2823\n",
      "['', 'i', 'i', 'the', 'you', 'the', 'i', 'the', 'the', 'the', 'the', 'the', 'i', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "16m 34s (- 56m 19s) (17050 22%) 4.2016\n",
      "['', 'i', 'i', 'have', 'a', 'a', 'i', 'the', 'a', 'the', 'a', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "16m 37s (- 56m 17s) (17100 22%) 4.0150\n",
      "['', 'i', 'have', 'to', 'it', 'i', 'the', 'a', 'the', 'the', 'to', 'you', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "16m 40s (- 56m 13s) (17150 22%) 4.1446\n",
      "['', 'i', 'i', 'i', 'it', 'i', 'it', 'to', 'i', 'the', 'it', 'and', 'i', 'it', 'it', 'to', 'i', 'the', '<EOS>']\n",
      "\n",
      "16m 42s (- 56m 10s) (17200 22%) 4.7769\n",
      "['', 'i', 'i', 'the', 'a', 'you', 'the', 'the', 'a', '', 'the', '', 'to', 'the', '<EOS>']\n",
      "\n",
      "16m 46s (- 56m 7s) (17250 23%) 4.1700\n",
      "['', 'i', 'i', 'a', 'you', 'a', 'i', 'the', 'a', '', 'the', '<EOS>']\n",
      "\n",
      "16m 48s (- 56m 4s) (17300 23%) 3.9786\n",
      "['', 'i', 'i', 'a', 'i', '', '', '', '', '', '', '<EOS>']\n",
      "\n",
      "16m 51s (- 56m 1s) (17350 23%) 4.1992\n",
      "['', 'i', 'i', 'you', 'a', 'a', 'you', 'a', 'i', '<EOS>']\n",
      "\n",
      "16m 54s (- 55m 58s) (17400 23%) 4.3085\n",
      "['', 'i', 'you', 'not', 'the', 'the', 'a', 'you', 'the', '', '', '', '<EOS>']\n",
      "\n",
      "16m 57s (- 55m 55s) (17450 23%) 4.0826\n",
      "['', 'i', 'is', 'a', 'that', 'a', 'that', 'you', 'a', 'that', '<EOS>']\n",
      "\n",
      "17m 0s (- 55m 52s) (17500 23%) 3.6101\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'the', 'i', 'i', 'the', 'i', 'i', '<EOS>']\n",
      "\n",
      "17m 2s (- 55m 48s) (17550 23%) 4.2990\n",
      "['', 'i', 'i', 'is', 'the', 'you', 'a', 'i', 'the', 'the', 'in', '', '<EOS>']\n",
      "\n",
      "17m 5s (- 55m 45s) (17600 23%) 3.9464\n",
      "['', 'i', 'i', 'the', 'a', 'the', 'i', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "17m 8s (- 55m 42s) (17650 23%) 4.3106\n",
      "['', 'i', 'i', 'it', 'you', 'i', 'the', 'i', '', 'the', '', '<EOS>']\n",
      "\n",
      "17m 12s (- 55m 40s) (17700 23%) 4.6048\n",
      "['', 'i', 'i', 'a', 'i', 'the', '', '', '', '', '<EOS>']\n",
      "\n",
      "17m 14s (- 55m 37s) (17750 23%) 4.0599\n",
      "['', 'i', 'i', 'you', 'to', 'i', 'of', 'to', 'that', 'i', 'and', 'i', '<EOS>']\n",
      "\n",
      "17m 17s (- 55m 34s) (17800 23%) 4.3010\n",
      "['', 'i', 'do', 'not', 'that', 'that', 'that', 'a', 'that', '<EOS>']\n",
      "\n",
      "17m 20s (- 55m 30s) (17850 23%) 4.4970\n",
      "['', 'i', 'i', 'to', 'i', 'to', 'of', 'to', 'and', 'i', '<EOS>']\n",
      "\n",
      "17m 23s (- 55m 29s) (17900 23%) 4.5539\n",
      "['', 'i', 'i', 'i', 'the', 'to', 'i', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "17m 26s (- 55m 27s) (17950 23%) 4.0094\n",
      "['', 'i', 'i', 'i', 'to', 'i', 'i', 'the', 'to', 'i', '<EOS>']\n",
      "\n",
      "17m 29s (- 55m 24s) (18000 24%) 4.4144\n",
      "['', 'yes', 'i', 'think', 'the', 'i', 'the', 'the', 'i', 'the', 'i', 'the', 'the', 'i', 'the', 'i', 'the', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "17m 32s (- 55m 22s) (18050 24%) 4.4468\n",
      "['', 'i', 'i', 'you', 'i', 'the', 'i', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "17m 36s (- 55m 19s) (18100 24%) 3.7688\n",
      "['', 'i', 'really', 'like', 'the', 'same', 'time', 'i', 'am', 'a', 'fan', 'of', 'the', '<EOS>']\n",
      "\n",
      "17m 38s (- 55m 15s) (18150 24%) 3.7304\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'a', 'i', 'it', 'you', 'a', 'i', '<EOS>']\n",
      "\n",
      "17m 41s (- 55m 12s) (18200 24%) 4.1491\n",
      "['', 'i', 'do', 'i', 'have', 'a', 'a', 'you', 'a', 'the', 'a', '', 'the', 'a', '', '<EOS>']\n",
      "\n",
      "17m 44s (- 55m 9s) (18250 24%) 4.3870\n",
      "['', 'i', 'i', 'i', 'you', 'i', 'i', '<EOS>']\n",
      "\n",
      "17m 47s (- 55m 7s) (18300 24%) 4.0697\n",
      "['', 'i', 'i', 't', 'a', 'the', 'the', 'the', 'a', 'the', 'the', 'of', 'the', '<EOS>']\n",
      "\n",
      "17m 50s (- 55m 5s) (18350 24%) 4.1332\n",
      "['', 'i', 'think', 'it', 'a', 'a', 'a', 'you', 'a', '<EOS>']\n",
      "\n",
      "17m 53s (- 55m 3s) (18400 24%) 4.0884\n",
      "['', 'i', 'is', 't', 'that', 'a', 'you', 'that', 'a', 'to', 'that', '<EOS>']\n",
      "\n",
      "17m 56s (- 55m 0s) (18450 24%) 3.7791\n",
      "['', 'i', 'you', 'you', 'you', 'you', 'have', 'to', 'you', 'a', 'you', 'you', 'a', 'to', 'you', 'a', '<EOS>']\n",
      "\n",
      "18m 0s (- 54m 58s) (18500 24%) 3.7963\n",
      "['', 'i', 'really', 'like', 'the', 'same', 'i', 'm', 'really', 'like', 'the', 'same', 'way', 'to', 'the', 'radio', '', '<EOS>']\n",
      "\n",
      "18m 3s (- 54m 56s) (18550 24%) 4.0098\n",
      "['', 'it', 'is', 'a', 'lot', 'you', 'you', 'have', 'a', 'to', '', '<EOS>']\n",
      "\n",
      "18m 5s (- 54m 53s) (18600 24%) 3.8565\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "18m 8s (- 54m 49s) (18650 24%) 3.9966\n",
      "['', 'i', 'i', 'i', 'to', 'i', 'a', 'to', 'that', '', '<EOS>']\n",
      "\n",
      "18m 12s (- 54m 48s) (18700 24%) 4.3347\n",
      "['', 'i', 'love', 'the', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "18m 15s (- 54m 45s) (18750 25%) 4.0373\n",
      "['', 'i', 'love', 'the', 'a', 'i', 'you', 'a', 'of', 'the', 'a', 'i', '<EOS>']\n",
      "\n",
      "18m 17s (- 54m 41s) (18800 25%) 3.6647\n",
      "['', 'i', 'i', 'is', 'a', 'i', 'a', 'i', '<EOS>']\n",
      "\n",
      "18m 20s (- 54m 38s) (18850 25%) 4.1455\n",
      "['', 'i', 'i', 't', 'i', 'that', 'it', 'that', 'the', 'that', 'the', 'that', '', '<EOS>']\n",
      "\n",
      "18m 24s (- 54m 37s) (18900 25%) 4.3647\n",
      "['', 'i', 'i', 'you', 'a', 'that', 'that', 'that', 'a', 'it', 'to', '', 'that', '<EOS>']\n",
      "\n",
      "18m 26s (- 54m 33s) (18950 25%) 3.6286\n",
      "['', 'i', 'i', 'a', 'it', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "18m 29s (- 54m 30s) (19000 25%) 3.8628\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'to', 'that', '<EOS>']\n",
      "\n",
      "18m 32s (- 54m 28s) (19050 25%) 4.2939\n",
      "['', 'i', 'i', 'i', 'that', 'that', 'i', 'it', 'the', '', 'that', '<EOS>']\n",
      "\n",
      "18m 35s (- 54m 25s) (19100 25%) 3.7936\n",
      "['', 'i', 'really', 'really', 'like', 'how', 'about', 'you', '', '<EOS>']\n",
      "\n",
      "18m 38s (- 54m 22s) (19150 25%) 4.2134\n",
      "['', 'yes', 'it', 's', 'been', 'nice', 'chatting', 'with', 'you', '', '<EOS>']\n",
      "\n",
      "18m 41s (- 54m 19s) (19200 25%) 3.7872\n",
      "['', 'i', 'i', 'that', 'that', 'to', 'that', 'that', 'the', 'that', 'to', 'that', 'the', 'of', 'the', 'to', '', 'that', 'the', '<EOS>']\n",
      "\n",
      "18m 44s (- 54m 16s) (19250 25%) 4.6298\n",
      "['', 'i', 'i', 'it', 'to', '', 'to', '', 'the', '<EOS>']\n",
      "\n",
      "18m 47s (- 54m 13s) (19300 25%) 4.1520\n",
      "['', 'yes', 'i', 'like', 'to', 'i', '', 'to', '', '<EOS>']\n",
      "\n",
      "18m 50s (- 54m 11s) (19350 25%) 4.2555\n",
      "['', 'i', 'is', 'it', 'to', 'i', '', 'to', '<EOS>']\n",
      "\n",
      "18m 53s (- 54m 9s) (19400 25%) 3.9182\n",
      "['', 'i', 'i', 't', 'i', 'i', 'the', 'you', 'the', 'the', '', 'to', '', '<EOS>']\n",
      "\n",
      "18m 56s (- 54m 5s) (19450 25%) 3.8509\n",
      "['', 'i', 'am', 'a', 'you', 'a', 'i', 'that', 'a', 'that', 'a', 'you', '<EOS>']\n",
      "\n",
      "18m 59s (- 54m 3s) (19500 26%) 4.0434\n",
      "['', 'i', 'i', 't', 'i', 'i', 'i', 'i', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "19m 2s (- 54m 0s) (19550 26%) 4.2715\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'the', 'i', '', 'that', 'the', '<EOS>']\n",
      "\n",
      "19m 5s (- 53m 58s) (19600 26%) 4.2608\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', '', 'a', 'i', '', 'i', '<EOS>']\n",
      "\n",
      "19m 8s (- 53m 54s) (19650 26%) 3.9511\n",
      "['', 'i', 'i', 'the', 'i', 'the', 'the', 'i', 'the', 'the', '<EOS>']\n",
      "\n",
      "19m 11s (- 53m 52s) (19700 26%) 4.0415\n",
      "['', 'i', 'is', 'you', 'a', 'the', 'to', '', 'a', '<EOS>']\n",
      "\n",
      "19m 14s (- 53m 48s) (19750 26%) 3.7690\n",
      "['', 'i', 'i', 'the', 'i', 'the', 'i', 'the', 'you', '<EOS>']\n",
      "\n",
      "19m 17s (- 53m 46s) (19800 26%) 3.8134\n",
      "['', 'i', 'i', 'you', 'the', 'i', 'the', 'in', 'the', 'the', 'the', 'in', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "19m 20s (- 53m 43s) (19850 26%) 4.0837\n",
      "['', 'i', 'i', 'i', 'the', 'to', 'i', 'the', 'to', 'the', 'i', 'to', 'the', 'the', '<EOS>']\n",
      "\n",
      "19m 23s (- 53m 42s) (19900 26%) 4.0946\n",
      "['', 'i', 'i', 'you', 'a', 'i', '', 'a', 'the', 'i', '<EOS>']\n",
      "\n",
      "19m 27s (- 53m 40s) (19950 26%) 4.2964\n",
      "['', 'i', 'i', 'a', 'a', 'in', 'a', '', 'i', '', 'a', '<EOS>']\n",
      "\n",
      "19m 30s (- 53m 38s) (20000 26%) 4.1899\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'the', 'i', 'i', 'the', '<EOS>']\n",
      "\n",
      "19m 33s (- 53m 35s) (20050 26%) 4.2948\n",
      "['', 'i', 'have', 'i', 'i', 'the', 'the', 'the', '', 'i', 'the', '', '', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19m 36s (- 53m 33s) (20100 26%) 3.8067\n",
      "['', 'i', 'have', 'a', 'i', 'you', '', 'a', '', 'a', '<EOS>']\n",
      "\n",
      "19m 40s (- 53m 32s) (20150 26%) 4.2935\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'that', 'i', 'the', 'i', 'the', 'the', '', 'i', 'the', '<EOS>']\n",
      "\n",
      "19m 43s (- 53m 31s) (20200 26%) 4.0059\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'that', 'the', 'i', '', '<EOS>']\n",
      "\n",
      "19m 47s (- 53m 30s) (20250 27%) 3.6275\n",
      "['', 'i', 'i', 'a', 'that', 'i', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "19m 50s (- 53m 28s) (20300 27%) 3.6623\n",
      "['', 'yeah', 'you', 're', 'right', 'about', 'the', 'same', 'time', '', '<EOS>']\n",
      "\n",
      "19m 53s (- 53m 24s) (20350 27%) 3.8980\n",
      "['', 'i', 'is', 'a', 'i', 'a', 'a', 'i', 'that', 'a', '<EOS>']\n",
      "\n",
      "19m 57s (- 53m 24s) (20400 27%) 4.2281\n",
      "['', 'i', 'i', 'you', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "20m 0s (- 53m 23s) (20450 27%) 4.1315\n",
      "['', 'i', 'think', 'it', 'i', 'have', 'been', 'to', 'a', '<EOS>']\n",
      "\n",
      "20m 4s (- 53m 22s) (20500 27%) 4.0981\n",
      "['', 'i', 'think', 'i', 'the', 'the', 'i', 'the', 'the', 'the', 'the', 'i', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "20m 8s (- 53m 21s) (20550 27%) 3.9204\n",
      "['', 'i', 's', 'not', 'a', 'i', 'the', 'i', 'the', 'that', 'the', 'the', '<EOS>']\n",
      "\n",
      "20m 12s (- 53m 21s) (20600 27%) 4.1366\n",
      "['', 'i', 'do', 'not', 'like', 'i', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "20m 15s (- 53m 20s) (20650 27%) 4.4427\n",
      "['', 'i', 'i', 'i', 'i', 'a', 'i', 'i', 'i', 'a', 'i', 'a', 'the', 'i', 'i', 'a', 'the', '', 'the', 'a', '<EOS>']\n",
      "\n",
      "20m 19s (- 53m 19s) (20700 27%) 4.4556\n",
      "['', 'i', 'm', 'not', 'really', 'good', 'but', 'i', 'have', 'not', 'seen', 'it', 'but', 'i', 'do', 'not', 'like', 'it', 'i', 'm', 'not', 'sure', 'it', 's', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "20m 23s (- 53m 18s) (20750 27%) 4.2198\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'the', 'i', 'i', 'to', 'i', 'the', 'i', '', 'i', 'i', '<EOS>']\n",
      "\n",
      "20m 27s (- 53m 17s) (20800 27%) 4.4506\n",
      "['', 'i', 'i', 'a', 'you', 'the', 'the', 'a', 'the', 'the', 'i', 'the', 'to', '<EOS>']\n",
      "\n",
      "20m 30s (- 53m 16s) (20850 27%) 3.9965\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'what', 'they', 'are', 'the', 'to', 'they', 'are', 'on', 'the', '', '<EOS>']\n",
      "\n",
      "20m 34s (- 53m 14s) (20900 27%) 3.8412\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'how', 'much', 'about', 'it', '', '<EOS>']\n",
      "\n",
      "20m 37s (- 53m 12s) (20950 27%) 4.1833\n",
      "['', 'i', 'have', 'not', 'a', 'it', 'it', '<EOS>']\n",
      "\n",
      "20m 41s (- 53m 11s) (21000 28%) 3.9979\n",
      "['', 'i', 'think', 'i', 'a', 'i', 'was', 'i', '', '<EOS>']\n",
      "\n",
      "20m 44s (- 53m 10s) (21050 28%) 4.2410\n",
      "['', 'i', 'm', 'not', 'sure', 'what', 'about', 'you', '', '<EOS>']\n",
      "\n",
      "20m 48s (- 53m 8s) (21100 28%) 4.3427\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'to', 'i', '', '<EOS>']\n",
      "\n",
      "20m 52s (- 53m 8s) (21150 28%) 4.1477\n",
      "['', 'i', 'i', 'have', 'to', 'i', 'to', 'the', 'the', 'to', 'the', 'the', 'i', '<EOS>']\n",
      "\n",
      "20m 55s (- 53m 6s) (21200 28%) 4.4341\n",
      "['', 'i', 'i', 'a', 'you', 'to', 'with', 'a', 'you', '<EOS>']\n",
      "\n",
      "20m 58s (- 53m 3s) (21250 28%) 4.1979\n",
      "['', 'i', 'do', 'not', 'know', 'i', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "21m 1s (- 53m 0s) (21300 28%) 3.8430\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'you', '<EOS>']\n",
      "\n",
      "21m 4s (- 52m 57s) (21350 28%) 4.0455\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'a', 'to', 'but', 'i', 'the', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "21m 7s (- 52m 55s) (21400 28%) 4.0731\n",
      "['', 'i', 'have', 'to', 'i', 'i', 'the', 'to', 'of', '', '<EOS>']\n",
      "\n",
      "21m 10s (- 52m 52s) (21450 28%) 4.5733\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'do', 'that', 'the', 'that', 'the', '', 'a', 'the', '<EOS>']\n",
      "\n",
      "21m 13s (- 52m 50s) (21500 28%) 4.2651\n",
      "['', 'i', 'have', 'to', 'not', 'a', '', '', '', '<EOS>']\n",
      "\n",
      "21m 16s (- 52m 46s) (21550 28%) 4.2136\n",
      "['', 'i', 'have', 'you', 'seen', 'it', '', '<EOS>']\n",
      "\n",
      "21m 19s (- 52m 44s) (21600 28%) 4.3336\n",
      "['', 'i', 'do', 'not', 'like', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "21m 22s (- 52m 40s) (21650 28%) 4.0187\n",
      "['', 'i', 'i', 'a', 'i', 'i', 'that', 'the', '', '', '<EOS>']\n",
      "\n",
      "21m 25s (- 52m 38s) (21700 28%) 4.3821\n",
      "['', 'i', 'do', 'not', 'know', 'i', 'i', 'i', 'of', 'the', '<EOS>']\n",
      "\n",
      "21m 28s (- 52m 35s) (21750 28%) 4.5300\n",
      "['', 'i', 'i', 'i', 'i', 'that', 'i', 'i', 'that', 'the', '<EOS>']\n",
      "\n",
      "21m 31s (- 52m 32s) (21800 29%) 4.2822\n",
      "['', 'i', 'think', 'i', 'i', 'i', 'the', 'about', 'that', 'the', 'i', 'i', '', 'the', '<EOS>']\n",
      "\n",
      "21m 35s (- 52m 30s) (21850 29%) 4.1668\n",
      "['', 'i', 'do', 'like', 'a', 'lot', 'of', 'the', '<EOS>']\n",
      "\n",
      "21m 38s (- 52m 27s) (21900 29%) 3.8581\n",
      "['', 'i', 'i', 'the', 'i', 'the', 'i', 'the', 'the', '<EOS>']\n",
      "\n",
      "21m 41s (- 52m 24s) (21950 29%) 4.2173\n",
      "['', 'i', 'i', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "21m 43s (- 52m 21s) (22000 29%) 3.6004\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'they', '', '<EOS>']\n",
      "\n",
      "21m 47s (- 52m 18s) (22050 29%) 4.0540\n",
      "['', 'i', 'm', 'not', 'what', 'about', 'you', '', '<EOS>']\n",
      "\n",
      "21m 50s (- 52m 15s) (22100 29%) 4.2682\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'of', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "21m 53s (- 52m 13s) (22150 29%) 4.0508\n",
      "['', 'i', 'have', 'a', 'it', 'the', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "21m 56s (- 52m 10s) (22200 29%) 4.1358\n",
      "['', 'yes', 'it', 'is', 'a', 'great', 'that', 'the', 'you', 'can', 't', 'imagine', 'that', 'the', 's', 'the', '<EOS>']\n",
      "\n",
      "21m 59s (- 52m 7s) (22250 29%) 4.6183\n",
      "['', 'i', 'have', 'a', 't', 'the', 'it', '', 'the', '<EOS>']\n",
      "\n",
      "22m 2s (- 52m 5s) (22300 29%) 4.1829\n",
      "['', 'i', 'love', 'the', 'same', 'time', 'i', 'love', 'to', 'watch', 'them', '', '<EOS>']\n",
      "\n",
      "22m 5s (- 52m 2s) (22350 29%) 4.3623\n",
      "['', 'i', 'do', 'i', 'the', 'i', 'the', 'the', 'a', 'i', 'the', '', 'the', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "22m 8s (- 52m 0s) (22400 29%) 4.3201\n",
      "['', 'i', 'is', 'it', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "22m 11s (- 51m 57s) (22450 29%) 4.4332\n",
      "['', 'i', 'like', 'to', 'i', 'the', '', '', '<EOS>']\n",
      "\n",
      "22m 14s (- 51m 54s) (22500 30%) 4.0977\n",
      "['', 'i', 'i', 'it', 'i', 'that', '<EOS>']\n",
      "\n",
      "22m 17s (- 51m 51s) (22550 30%) 3.4344\n",
      "['', 'i', 'am', 'a', 'fan', 'i', 'am', 'a', 'fan', 'of', 'my', 'favorite', '', '<EOS>']\n",
      "\n",
      "22m 20s (- 51m 48s) (22600 30%) 4.4755\n",
      "['', 'i', 'am', 'a', 'i', 'you', 'i', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "22m 23s (- 51m 45s) (22650 30%) 4.1113\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'have', 'a', '<EOS>']\n",
      "\n",
      "22m 26s (- 51m 42s) (22700 30%) 3.7262\n",
      "['', 'i', 'i', 'i', 'the', 'a', 'i', 'the', '<EOS>']\n",
      "\n",
      "22m 29s (- 51m 39s) (22750 30%) 3.7585\n",
      "['', 'yeah', 'i', 'think', 'it', 'is', 'a', 'i', '<EOS>']\n",
      "\n",
      "22m 32s (- 51m 36s) (22800 30%) 4.2748\n",
      "['', 'i', 'i', 'a', 'you', 'i', 'a', 'i', 'a', '<EOS>']\n",
      "\n",
      "22m 35s (- 51m 33s) (22850 30%) 4.2782\n",
      "['', 'i', 'i', 'a', 'you', 'the', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "22m 38s (- 51m 30s) (22900 30%) 4.3383\n",
      "['', 'i', 'is', 'is', 'you', 'a', 'that', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "22m 41s (- 51m 26s) (22950 30%) 4.4713\n",
      "['', 'i', 'don', 'a', 'you', 'a', '<EOS>']\n",
      "\n",
      "22m 43s (- 51m 23s) (23000 30%) 4.0671\n",
      "['', 'i', 'am', 'a', 'fan', 'of', 'the', 'i', 'am', 'a', 'fan', 'of', 'the', 'i', '<EOS>']\n",
      "\n",
      "22m 46s (- 51m 19s) (23050 30%) 4.2294\n",
      "['', 'i', 'i', 'i', 'i', 'you', 'a', 'i', '', '<EOS>']\n",
      "\n",
      "22m 49s (- 51m 16s) (23100 30%) 4.4021\n",
      "['', 'i', 'm', 'not', 'a', 'i', 'i', 'i', 'i', 'i', 'i', 'the', 'the', 'i', 'the', 'to', 'the', 'the', '<EOS>']\n",
      "\n",
      "22m 52s (- 51m 13s) (23150 30%) 4.6216\n",
      "['', 'i', 'm', 'i', 'i', 'i', 'i', 'i', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "22m 55s (- 51m 10s) (23200 30%) 3.7967\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'a', 'i', 'i', '<EOS>']\n",
      "\n",
      "22m 57s (- 51m 6s) (23250 31%) 4.2736\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'a', 'i', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "23m 0s (- 51m 3s) (23300 31%) 4.2154\n",
      "['', 'i', 'm', 'not', 'sure', 'a', 'a', 'i', '<EOS>']\n",
      "\n",
      "23m 2s (- 50m 58s) (23350 31%) 4.1498\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'a', 'the', 'the', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "23m 5s (- 50m 55s) (23400 31%) 4.3966\n",
      "['', 'i', 'i', 't', 'that', 'that', 'the', 'the', '', '', '', '<EOS>']\n",
      "\n",
      "23m 8s (- 50m 51s) (23450 31%) 4.3535\n",
      "['', 'i', 'am', 'not', 'a', 'fan', 'of', 'the', 'i', 'i', 'a', '<EOS>']\n",
      "\n",
      "23m 11s (- 50m 48s) (23500 31%) 4.3647\n",
      "['', 'i', 'i', 'i', 'i', 'that', 'i', '', 'to', '', '<EOS>']\n",
      "\n",
      "23m 13s (- 50m 45s) (23550 31%) 4.6896\n",
      "['', 'i', 'do', 'i', 'i', '', '', '', '<EOS>']\n",
      "\n",
      "23m 16s (- 50m 42s) (23600 31%) 3.9620\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'of', 'and', 'i', '<EOS>']\n",
      "\n",
      "23m 19s (- 50m 38s) (23650 31%) 4.1915\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'of', '<EOS>']\n",
      "\n",
      "23m 22s (- 50m 34s) (23700 31%) 4.2423\n",
      "['', 'i', 'i', 'a', 'i', 'i', 'i', 'of', 'the', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "23m 24s (- 50m 30s) (23750 31%) 4.0420\n",
      "['', 'i', 'do', 'i', 'think', 'i', '', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23m 27s (- 50m 27s) (23800 31%) 4.1467\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'the', 'i', 'the', 'the', 'the', '', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "23m 30s (- 50m 24s) (23850 31%) 4.2968\n",
      "['', 'i', 'don', 't', 'know', 'that', 'the', 'show', 'is', 'a', 'good', 'thing', '<EOS>']\n",
      "\n",
      "23m 33s (- 50m 21s) (23900 31%) 4.2154\n",
      "['', 'i', 'i', 'i', 'i', 'the', 'i', 'the', 'i', 'the', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "23m 35s (- 50m 17s) (23950 31%) 4.4320\n",
      "['', 'i', 'do', 'not', 'know', 'i', 'i', 'of', 'the', 'a', 'of', '<EOS>']\n",
      "\n",
      "23m 38s (- 50m 14s) (24000 32%) 4.0757\n",
      "['', 'i', 'have', 'you', 'a', '', 'the', 'a', '', '<EOS>']\n",
      "\n",
      "23m 41s (- 50m 11s) (24050 32%) 4.0773\n",
      "['', 'i', 'do', 'not', 'i', 'you', 'you', 'the', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "23m 44s (- 50m 8s) (24100 32%) 4.3596\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'there', 'are', 'a', 'lot', 'of', 'money', 'i', 'm', 'not', 'sure', 'about', 'that', 'but', 'i', 'do', 'know', 'that', 'it', 's', 'a', 'great', 'chatting', 'with', 'you', '', '<EOS>']\n",
      "\n",
      "23m 47s (- 50m 5s) (24150 32%) 4.2533\n",
      "['', 'i', 'do', 'i', 'to', 'i', 'the', 'and', 'to', 'that', 'the', '', '<EOS>']\n",
      "\n",
      "23m 50s (- 50m 2s) (24200 32%) 4.2507\n",
      "['', 'i', 'do', 'i', 'that', 'i', 'that', 'you', '', '', '<EOS>']\n",
      "\n",
      "23m 52s (- 49m 58s) (24250 32%) 4.2634\n",
      "['', 'i', 'am', 'not', 'sure', 'about', 'that', 'but', 'but', 'i', 'am', 'not', 'sure', 'how', 'many', 'of', 'the', 'internet', '', '<EOS>']\n",
      "\n",
      "23m 55s (- 49m 55s) (24300 32%) 4.4144\n",
      "['', 'i', 'i', 'it', 'that', 'i', 'that', 'i', '<EOS>']\n",
      "\n",
      "23m 58s (- 49m 51s) (24350 32%) 4.1466\n",
      "['', 'i', 'i', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "24m 0s (- 49m 48s) (24400 32%) 3.7888\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'the', '<EOS>']\n",
      "\n",
      "24m 3s (- 49m 44s) (24450 32%) 4.1380\n",
      "['', 'i', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "24m 6s (- 49m 41s) (24500 32%) 3.7692\n",
      "['', 'i', 'i', 'a', 'i', 'you', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "24m 8s (- 49m 37s) (24550 32%) 4.1650\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "24m 11s (- 49m 34s) (24600 32%) 4.0583\n",
      "['', 'i', 'i', 'like', 'to', 'i', 'the', 'i', 'i', '<EOS>']\n",
      "\n",
      "24m 14s (- 49m 30s) (24650 32%) 4.1984\n",
      "['', 'yeah', 'that', 'is', 'a', 'good', '<EOS>']\n",
      "\n",
      "24m 17s (- 49m 27s) (24700 32%) 4.0008\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', '', 'i', '<EOS>']\n",
      "\n",
      "24m 19s (- 49m 23s) (24750 33%) 3.8972\n",
      "['', 'i', 'do', 'i', 'a', 'i', 'a', 'i', 'to', 'i', 'a', '', 'i', '<EOS>']\n",
      "\n",
      "24m 22s (- 49m 20s) (24800 33%) 4.2878\n",
      "['', 'i', 'do', 'a', 'it', 'you', '', 'a', '', 'to', 'you', '', '<EOS>']\n",
      "\n",
      "24m 25s (- 49m 17s) (24850 33%) 4.1728\n",
      "['', 'i', 'do', 'i', '', 'i', '<EOS>']\n",
      "\n",
      "24m 27s (- 49m 13s) (24900 33%) 3.8347\n",
      "['', 'i', 'm', 'not', 'to', 'know', 'the', 'it', 's', '', '<EOS>']\n",
      "\n",
      "24m 30s (- 49m 9s) (24950 33%) 4.2968\n",
      "['', 'i', 'i', 'is', 'to', '', 'to', '<EOS>']\n",
      "\n",
      "24m 33s (- 49m 6s) (25000 33%) 3.9457\n",
      "['', 'i', 'did', 'not', 'know', 'that', 'the', 'a', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "24m 35s (- 49m 3s) (25050 33%) 4.3747\n",
      "['', 'i', 'i', 'to', 'the', 'the', 'to', 'the', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "24m 38s (- 48m 59s) (25100 33%) 4.3204\n",
      "['', 'i', 'like', 'do', 'you', 'like', 'the', 'a', 'is', '', 'to', '<EOS>']\n",
      "\n",
      "24m 40s (- 48m 55s) (25150 33%) 3.7385\n",
      "['', 'i', 'do', 'i', 'you', 'i', '', '<EOS>']\n",
      "\n",
      "24m 43s (- 48m 51s) (25200 33%) 3.7922\n",
      "['', 'i', 'do', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "24m 46s (- 48m 48s) (25250 33%) 3.8721\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', '', 'i', '<EOS>']\n",
      "\n",
      "24m 49s (- 48m 45s) (25300 33%) 4.2379\n",
      "['', 'i', 'i', 'like', 'the', 'the', 'you', '', '<EOS>']\n",
      "\n",
      "24m 51s (- 48m 41s) (25350 33%) 4.0094\n",
      "['', 'i', 'am', 'a', 'good', 'i', 'i', '<EOS>']\n",
      "\n",
      "24m 54s (- 48m 37s) (25400 33%) 3.8824\n",
      "['', 'i', 'you', 'the', 'the', 'a', 'to', 'the', 'of', 'the', 'to', 'of', 'the', '<EOS>']\n",
      "\n",
      "24m 57s (- 48m 35s) (25450 33%) 4.3765\n",
      "['', 'i', 'i', 'you', 'a', 'i', 'the', 'i', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "25m 0s (- 48m 33s) (25500 34%) 3.9757\n",
      "['', 'i', 'agree', 'i', 'you', 'to', 'a', 'to', 'of', 'the', 'i', '', '<EOS>']\n",
      "\n",
      "25m 4s (- 48m 31s) (25550 34%) 4.2138\n",
      "['', 'i', 'm', 'not', 'sure', 'about', 'it', '', '<EOS>']\n",
      "\n",
      "25m 7s (- 48m 29s) (25600 34%) 3.8916\n",
      "['', 'i', 'i', 'a', 'a', 'i', 'i', 'to', 'i', '', '<EOS>']\n",
      "\n",
      "25m 10s (- 48m 26s) (25650 34%) 4.2363\n",
      "['', 'i', 'i', 'like', 'a', 'i', 'you', 'in', '<EOS>']\n",
      "\n",
      "25m 13s (- 48m 24s) (25700 34%) 4.0928\n",
      "['', 'i', 'do', 'not', 'i', 'you', 'i', 'it', '', 'it', '<EOS>']\n",
      "\n",
      "25m 16s (- 48m 20s) (25750 34%) 4.2092\n",
      "['', 'no', 'i', 'have', 'not', 'i', 'i', 'the', 'that', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "25m 19s (- 48m 17s) (25800 34%) 4.4348\n",
      "['', 'no', 'i', 'am', 'not', 'sure', 'i', 'but', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "25m 22s (- 48m 14s) (25850 34%) 4.3692\n",
      "['', 'i', 'i', 't', 'that', 'a', 'you', 'a', 'that', 'you', 'you', '<EOS>']\n",
      "\n",
      "25m 24s (- 48m 10s) (25900 34%) 4.5231\n",
      "['', 'yes', 'i', 'i', 'like', 'the', 'the', 'the', 'i', 'the', 'the', '<EOS>']\n",
      "\n",
      "25m 27s (- 48m 7s) (25950 34%) 4.2184\n",
      "['', 'i', 'i', 'i', 'that', 'to', 'that', 'you', 'to', 'that', '', '<EOS>']\n",
      "\n",
      "25m 30s (- 48m 5s) (26000 34%) 4.5701\n",
      "['', 'i', 'm', 'not', 'sure', 'about', 'it', 'i', 'you', 'a', '<EOS>']\n",
      "\n",
      "25m 33s (- 48m 1s) (26050 34%) 4.2680\n",
      "['', 'i', 'i', 'a', 'i', 'a', 'that', 'a', 'that', 'a', '<EOS>']\n",
      "\n",
      "25m 36s (- 47m 57s) (26100 34%) 4.1201\n",
      "['', 'i', 'am', 'not', 'sure', 'how', 'about', 'it', '', '<EOS>']\n",
      "\n",
      "25m 38s (- 47m 54s) (26150 34%) 4.1252\n",
      "['', 'i', 'm', 'not', 'the', 'i', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "25m 41s (- 47m 51s) (26200 34%) 4.3352\n",
      "['', 'no', 'i', 'didn', 't', 'know', 'that', 'is', 'so', 'great', 'and', '<EOS>']\n",
      "\n",
      "25m 44s (- 47m 48s) (26250 35%) 4.3496\n",
      "['', 'i', 'have', 'you', 'heard', 'a', 'lot', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "25m 47s (- 47m 46s) (26300 35%) 4.2209\n",
      "['', 'no', 'i', 'have', 'not', 'seen', 'it', 'it', 'is', 'it', 'it', 's', 'a', 'good', '', '<EOS>']\n",
      "\n",
      "25m 50s (- 47m 42s) (26350 35%) 4.6907\n",
      "['', 'i', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "25m 53s (- 47m 39s) (26400 35%) 4.0759\n",
      "['', 'i', 'do', 'not', 'a', 'you', 'in', 'a', 'a', '<EOS>']\n",
      "\n",
      "25m 55s (- 47m 35s) (26450 35%) 4.2884\n",
      "['', 'i', 's', 'do', 'i', 'you', '', 'it', 's', '', '<EOS>']\n",
      "\n",
      "25m 59s (- 47m 33s) (26500 35%) 4.1348\n",
      "['', 'i', 'am', 'i', 'a', 'i', 'of', 'of', 'the', 'the', 'i', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "26m 1s (- 47m 29s) (26550 35%) 4.3065\n",
      "['', 'i', 'am', 'not', 'sure', 'i', '', '<EOS>']\n",
      "\n",
      "26m 4s (- 47m 26s) (26600 35%) 3.9718\n",
      "['', 'i', 'do', 'not', 'like', 'to', 'do', 'you', 'you', 'to', 'know', '', '<EOS>']\n",
      "\n",
      "26m 7s (- 47m 23s) (26650 35%) 4.1247\n",
      "['', 'i', 'do', 'not', 'i', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "26m 11s (- 47m 22s) (26700 35%) 4.1301\n",
      "['', 'i', 'do', 'not', 'like', 'to', 'it', 'to', 'to', 'it', '<EOS>']\n",
      "\n",
      "26m 14s (- 47m 20s) (26750 35%) 4.2533\n",
      "['', 'i', 'i', 'the', 'i', 'to', 'that', 'i', 'the', 'to', 'that', '<EOS>']\n",
      "\n",
      "26m 18s (- 47m 18s) (26800 35%) 4.0772\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'is', 'the', 'the', 'the', 'the', 'to', '<EOS>']\n",
      "\n",
      "26m 21s (- 47m 15s) (26850 35%) 4.0497\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'that', 'the', 'i', '<EOS>']\n",
      "\n",
      "26m 24s (- 47m 12s) (26900 35%) 3.7127\n",
      "['', 'i', 'do', 'you', 'like', 'the', 'simpsons', '', '<EOS>']\n",
      "\n",
      "26m 27s (- 47m 9s) (26950 35%) 4.1032\n",
      "['', 'i', 'am', 'not', 'but', 'i', 'am', 'not', 'fan', 'the', 'the', 'the', 'the', 'to', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "26m 30s (- 47m 6s) (27000 36%) 4.3488\n",
      "['', 'i', 'am', 'not', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "26m 32s (- 47m 3s) (27050 36%) 4.2510\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'you', '', 'i', '<EOS>']\n",
      "\n",
      "26m 35s (- 47m 0s) (27100 36%) 4.1755\n",
      "['', 'i', 'i', 'a', 'a', 'a', 'a', 'the', 'a', '', 'a', '', '<EOS>']\n",
      "\n",
      "26m 38s (- 46m 57s) (27150 36%) 4.5858\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'i', 'the', 'i', '', '<EOS>']\n",
      "\n",
      "26m 41s (- 46m 54s) (27200 36%) 4.1789\n",
      "['', 'i', 'do', 'you', '', 'i', 'a', 'to', 'a', '', '<EOS>']\n",
      "\n",
      "26m 44s (- 46m 51s) (27250 36%) 3.8197\n",
      "['', 'i', 'i', 'know', 'i', 'i', 'the', 'the', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "26m 47s (- 46m 48s) (27300 36%) 4.3919\n",
      "['', 'i', 'i', 'it', 'a', 'you', '', '<EOS>']\n",
      "\n",
      "26m 50s (- 46m 45s) (27350 36%) 4.0506\n",
      "['', 'i', 'am', 'not', 'i', 'i', 'you', 'i', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "26m 53s (- 46m 42s) (27400 36%) 3.9291\n",
      "['', 'i', 'i', 'the', 'you', 'you', 'i', 'the', 'the', '', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26m 56s (- 46m 39s) (27450 36%) 3.8266\n",
      "['', 'i', 'i', 'i', 'you', 'the', 'the', 'the', 'to', '', 'the', '', '<EOS>']\n",
      "\n",
      "26m 59s (- 46m 37s) (27500 36%) 4.4414\n",
      "['', 'i', 'do', 'i', 'you', 'you', 'i', 'you', '<EOS>']\n",
      "\n",
      "27m 1s (- 46m 33s) (27550 36%) 3.8435\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'the', 'you', '<EOS>']\n",
      "\n",
      "27m 4s (- 46m 30s) (27600 36%) 3.9911\n",
      "['', 'i', 'i', 'a', 'i', 'i', 'you', 'i', 'i', 'the', 'to', '<EOS>']\n",
      "\n",
      "27m 7s (- 46m 27s) (27650 36%) 4.3535\n",
      "['', 'i', 'do', 'i', 'you', 'i', 'you', '<EOS>']\n",
      "\n",
      "27m 10s (- 46m 24s) (27700 36%) 3.9951\n",
      "['', 'i', 'i', 'i', 'i', 'a', 'i', 'i', '<EOS>']\n",
      "\n",
      "27m 13s (- 46m 21s) (27750 37%) 4.3806\n",
      "['', 'i', 'do', 'that', 'the', 'a', 'the', 'the', 'a', '', 'the', '<EOS>']\n",
      "\n",
      "27m 16s (- 46m 17s) (27800 37%) 4.2496\n",
      "['', 'i', 'am', 'you', 'you', '', '<EOS>']\n",
      "\n",
      "27m 18s (- 46m 14s) (27850 37%) 4.3242\n",
      "['', 'i', 'i', 'i', 'you', 'i', 'you', 'i', '', 'i', '', '<EOS>']\n",
      "\n",
      "27m 21s (- 46m 11s) (27900 37%) 4.5221\n",
      "['', 'i', 'do', 'not', 'know', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "27m 24s (- 46m 8s) (27950 37%) 3.9603\n",
      "['', 'i', 'do', 'i', 'you', 'i', 'you', '', 'i', '', '<EOS>']\n",
      "\n",
      "27m 27s (- 46m 4s) (28000 37%) 4.1208\n",
      "['', 'i', 'do', 'i', 'you', 'i', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "27m 29s (- 46m 1s) (28050 37%) 4.3045\n",
      "['', 'i', 'i', 'i', 'you', 'i', 'i', 'i', 'you', 'i', 'the', 'the', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "27m 32s (- 45m 58s) (28100 37%) 4.3316\n",
      "['', 'i', 'am', 'not', 'sure', 'the', 'you', 'of', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "27m 35s (- 45m 55s) (28150 37%) 4.2750\n",
      "['', 'i', 'do', 'you', 'you', 'to', 'the', 'you', '', 'the', '<EOS>']\n",
      "\n",
      "27m 38s (- 45m 52s) (28200 37%) 4.4573\n",
      "['', 'i', 'am', 'you', '', 'the', '<EOS>']\n",
      "\n",
      "27m 41s (- 45m 49s) (28250 37%) 3.9455\n",
      "['', 'i', 'am', 'a', 'fan', 'of', 'the', '<EOS>']\n",
      "\n",
      "27m 43s (- 45m 45s) (28300 37%) 3.8003\n",
      "['', 'i', 'am', 'you', 'a', 'fan', 'of', 'the', '<EOS>']\n",
      "\n",
      "27m 46s (- 45m 42s) (28350 37%) 4.2190\n",
      "['', 'i', 'am', 'you', 'fan', 'of', 'the', 'simpsons', '', '<EOS>']\n",
      "\n",
      "27m 49s (- 45m 38s) (28400 37%) 3.9033\n",
      "['', 'i', 'am', 'you', 'i', 'a', '<EOS>']\n",
      "\n",
      "27m 51s (- 45m 35s) (28450 37%) 4.2425\n",
      "['', 'i', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "27m 54s (- 45m 32s) (28500 38%) 3.7657\n",
      "['', 'i', 'have', 'been', 'a', 'good', 'fan', 'i', 'have', 'been', 'so', 'nice', 'chatting', 'with', 'you', '', '<EOS>']\n",
      "\n",
      "27m 57s (- 45m 28s) (28550 38%) 3.6392\n",
      "['', 'i', 'is', 'is', 'a', '<EOS>']\n",
      "\n",
      "27m 59s (- 45m 25s) (28600 38%) 4.0601\n",
      "['', 'i', 'i', 'you', 'the', 'the', 'a', 'the', 'the', '<EOS>']\n",
      "\n",
      "28m 2s (- 45m 22s) (28650 38%) 4.2698\n",
      "['', 'i', 'do', 'you', 'the', 'the', 'a', 'the', 'the', 'to', 'the', '', 'the', 'a', 'the', '<EOS>']\n",
      "\n",
      "28m 5s (- 45m 19s) (28700 38%) 4.2695\n",
      "['', 'i', 'i', 'do', 'it', '<EOS>']\n",
      "\n",
      "28m 8s (- 45m 16s) (28750 38%) 3.8280\n",
      "['', 'i', 'i', 'the', 'the', 'a', 'the', 'the', '<EOS>']\n",
      "\n",
      "28m 11s (- 45m 12s) (28800 38%) 4.0929\n",
      "['', 'i', 'am', 'you', 'i', '<EOS>']\n",
      "\n",
      "28m 13s (- 45m 9s) (28850 38%) 4.2622\n",
      "['', 'i', 'am', 'not', 'you', 'know', 'that', '<EOS>']\n",
      "\n",
      "28m 16s (- 45m 6s) (28900 38%) 4.0324\n",
      "['', 'i', 'do', 'i', 'of', 'the', 'the', 'of', 'the', '<EOS>']\n",
      "\n",
      "28m 19s (- 45m 2s) (28950 38%) 4.0053\n",
      "['', 'i', 'i', 'i', 'i', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "28m 21s (- 44m 59s) (29000 38%) 4.0320\n",
      "['', 'i', 'have', 'a', 'i', 'it', '<EOS>']\n",
      "\n",
      "28m 24s (- 44m 55s) (29050 38%) 3.8852\n",
      "['', 'i', 'i', 'is', 'i', 'i', 'like', 'the', 'a', 'i', 'to', '<EOS>']\n",
      "\n",
      "28m 27s (- 44m 52s) (29100 38%) 4.2359\n",
      "['', 'i', 'do', 'i', 'watch', 'the', 'same', 'time', '', '<EOS>']\n",
      "\n",
      "28m 30s (- 44m 50s) (29150 38%) 4.3181\n",
      "['', 'i', 'am', 'i', 'a', 'you', 'a', 'i', 'the', 'a', 'the', '', 'a', '<EOS>']\n",
      "\n",
      "28m 33s (- 44m 46s) (29200 38%) 3.9571\n",
      "['', 'i', 'i', 'a', 'i', 'a', 'i', 'the', 'the', '<EOS>']\n",
      "\n",
      "28m 35s (- 44m 43s) (29250 39%) 3.6987\n",
      "['', 'i', 'i', 'you', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "28m 38s (- 44m 40s) (29300 39%) 3.9339\n",
      "['', 'i', 'am', 'not', 'i', 'you', 'a', '<EOS>']\n",
      "\n",
      "28m 40s (- 44m 36s) (29350 39%) 3.6075\n",
      "['', 'i', 'do', 'not', 'know', 'that', 'a', 'a', 'it', '', 'the', '<EOS>']\n",
      "\n",
      "28m 43s (- 44m 33s) (29400 39%) 4.1788\n",
      "['', 'i', 'i', 'it', 'it', 'the', 'i', 'it', 'the', 'i', 'it', 'the', '<EOS>']\n",
      "\n",
      "28m 47s (- 44m 31s) (29450 39%) 4.1351\n",
      "['', 'i', 'i', 'i', 'it', 'i', 'it', 'it', 'like', 'it', 'the', '<EOS>']\n",
      "\n",
      "28m 50s (- 44m 28s) (29500 39%) 4.1797\n",
      "['', 'i', 'i', 'i', 'good', 'i', '', 'i', '', '<EOS>']\n",
      "\n",
      "28m 53s (- 44m 26s) (29550 39%) 3.9911\n",
      "['', 'i', 'i', 'good', 'i', '', 'to', '', '', '<EOS>']\n",
      "\n",
      "28m 56s (- 44m 23s) (29600 39%) 3.8246\n",
      "['', 'i', 'do', 'not', 'i', 'the', 'the', 'the', 'of', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "28m 59s (- 44m 21s) (29650 39%) 4.2586\n",
      "['', 'i', 'i', 'i', 'the', 'a', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "29m 3s (- 44m 18s) (29700 39%) 3.6171\n",
      "['', 'i', 'am', 'a', 'you', 'of', 'the', '<EOS>']\n",
      "\n",
      "29m 5s (- 44m 15s) (29750 39%) 3.8444\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "29m 9s (- 44m 13s) (29800 39%) 4.0520\n",
      "['', 'i', 'have', 'i', 'i', 'i', 'the', 'the', 'i', '', 'the', '', '<EOS>']\n",
      "\n",
      "29m 12s (- 44m 10s) (29850 39%) 3.9432\n",
      "['', 'i', 'am', 'a', 'that', 'the', 'that', 'the', 'the', '<EOS>']\n",
      "\n",
      "29m 15s (- 44m 8s) (29900 39%) 3.8876\n",
      "['', 'i', 'am', 'a', 'the', 'you', 'the', '', 'the', '', 'the', '', 'the', '', '', '<EOS>']\n",
      "\n",
      "29m 18s (- 44m 5s) (29950 39%) 4.0631\n",
      "['', 'i', 'am', 'i', 'i', 'the', 'the', 'the', 'you', '', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "29m 22s (- 44m 3s) (30000 40%) 4.2045\n",
      "['', 'i', 'am', 'i', 'a', 'the', 'the', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "29m 25s (- 44m 0s) (30050 40%) 4.2806\n",
      "['', 'i', 'i', 'like', 'the', 'you', 'to', '', 'the', '', '<EOS>']\n",
      "\n",
      "29m 29s (- 43m 59s) (30100 40%) 4.3720\n",
      "['', 'i', 'do', 'i', 'the', 'the', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "29m 31s (- 43m 55s) (30150 40%) 4.0729\n",
      "['', 'i', 'i', 'i', 'it', 'it', 'i', 'it', 'the', 'the', '<EOS>']\n",
      "\n",
      "29m 34s (- 43m 52s) (30200 40%) 4.2310\n",
      "['', 'i', 'i', 'i', 'i', 'the', '<EOS>']\n",
      "\n",
      "29m 37s (- 43m 48s) (30250 40%) 4.0479\n",
      "['', 'i', 'have', 'heard', 'about', 'the', 'the', 'i', '<EOS>']\n",
      "\n",
      "29m 39s (- 43m 45s) (30300 40%) 3.4992\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'of', 'i', 'i', '<EOS>']\n",
      "\n",
      "29m 42s (- 43m 42s) (30350 40%) 4.0061\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "29m 45s (- 43m 39s) (30400 40%) 4.2744\n",
      "['', 'i', 'am', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'to', 'the', 'and', 'have', 'the', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "29m 48s (- 43m 36s) (30450 40%) 4.1652\n",
      "['', 'i', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "29m 50s (- 43m 32s) (30500 40%) 3.9302\n",
      "['', 'i', 'am', 'i', 'you', 'i', 'the', 'the', 'the', '', 'the', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "29m 53s (- 43m 29s) (30550 40%) 4.0760\n",
      "['', 'i', 'did', 'the', 'you', 'was', 'a', 'the', '<EOS>']\n",
      "\n",
      "29m 56s (- 43m 26s) (30600 40%) 3.9672\n",
      "['', 'i', 'have', 'a', 'the', 'the', 'to', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "29m 59s (- 43m 23s) (30650 40%) 4.3258\n",
      "['', 'i', 'did', 'not', 'know', 'that', 'you', 'i', 'the', 'the', 'a', 'know', 'you', '<EOS>']\n",
      "\n",
      "30m 2s (- 43m 20s) (30700 40%) 4.0095\n",
      "['', 'i', 'do', 'like', 'it', 'i', '', 'a', '', '<EOS>']\n",
      "\n",
      "30m 4s (- 43m 17s) (30750 41%) 3.6655\n",
      "['', 'i', 'i', 'to', 'of', 'the', 'i', 'to', 'of', 'the', 'to', '<EOS>']\n",
      "\n",
      "30m 7s (- 43m 14s) (30800 41%) 3.9688\n",
      "['', 'i', 'i', 'the', 'the', 'you', 'i', 'the', 'to', 'the', 'i', 'the', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "30m 10s (- 43m 10s) (30850 41%) 4.2124\n",
      "['', 'i', 'do', 'i', 'the', 'the', 'like', 'the', '', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "30m 13s (- 43m 7s) (30900 41%) 4.1134\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'the', 'i', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "30m 15s (- 43m 4s) (30950 41%) 4.0541\n",
      "['', 'i', 'do', 'i', 'you', 'i', '', 'the', '<EOS>']\n",
      "\n",
      "30m 18s (- 43m 0s) (31000 41%) 4.0992\n",
      "['', 'i', 'am', 'a', 'you', 'fan', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "30m 20s (- 42m 57s) (31050 41%) 3.9220\n",
      "['', 'i', 'am', 'a', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "30m 23s (- 42m 54s) (31100 41%) 4.1310\n",
      "['', 'i', 'i', 'i', 'the', 'i', 'you', 'i', 'the', 'i', '', 'the', '', '<EOS>']\n",
      "\n",
      "30m 26s (- 42m 50s) (31150 41%) 4.4720\n",
      "['', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "30m 28s (- 42m 47s) (31200 41%) 4.2255\n",
      "['', 'i', 'am', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "30m 31s (- 42m 44s) (31250 41%) 3.8043\n",
      "['', 'i', 'is', 'i', 'the', 'the', 'the', 'the', 'the', '', 'the', '', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30m 34s (- 42m 40s) (31300 41%) 4.0079\n",
      "['', 'it', 'is', 'a', 'you', 'like', 'the', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "30m 36s (- 42m 37s) (31350 41%) 4.4376\n",
      "['', 'i', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'i', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "30m 39s (- 42m 34s) (31400 41%) 4.2487\n",
      "['', 'i', 'you', 'to', 'have', 'a', 'to', 'to', 'you', 'to', 'the', 'to', 'the', 'to', '<EOS>']\n",
      "\n",
      "30m 42s (- 42m 31s) (31450 41%) 4.2438\n",
      "['', 'i', 'the', 'the', 'the', 'the', 'a', 'the', '<EOS>']\n",
      "\n",
      "30m 45s (- 42m 28s) (31500 42%) 3.9635\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'i', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "30m 48s (- 42m 25s) (31550 42%) 4.3437\n",
      "['', 'i', 'have', 'a', 'i', 'you', 'i', 'the', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "30m 51s (- 42m 22s) (31600 42%) 3.8235\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'a', 'the', '<EOS>']\n",
      "\n",
      "30m 53s (- 42m 19s) (31650 42%) 4.0281\n",
      "['', 'i', 'have', 'a', 'the', 'the', 'the', '', 'the', 'the', '<EOS>']\n",
      "\n",
      "30m 56s (- 42m 15s) (31700 42%) 4.1844\n",
      "['', 'i', 'have', 'a', 'the', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "30m 59s (- 42m 12s) (31750 42%) 3.9756\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'i', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "31m 2s (- 42m 9s) (31800 42%) 4.2631\n",
      "['', 'i', 'have', 'not', 'i', 'i', 'to', 'the', 'to', '', 'the', 'the', 'to', '', 'the', '<EOS>']\n",
      "\n",
      "31m 4s (- 42m 6s) (31850 42%) 4.0964\n",
      "['', 'i', 'have', 'to', 'a', 'you', 'to', '<EOS>']\n",
      "\n",
      "31m 7s (- 42m 3s) (31900 42%) 4.2272\n",
      "['', 'i', 'am', 'a', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "31m 10s (- 41m 59s) (31950 42%) 3.8150\n",
      "['', 'i', 'do', 't', 'i', 'you', 'you', 'i', 'to', '', 'you', '<EOS>']\n",
      "\n",
      "31m 12s (- 41m 56s) (32000 42%) 4.0974\n",
      "['', 'i', 'do', 'not', 'like', 'the', 'of', 'the', 'you', 'to', 'you', '<EOS>']\n",
      "\n",
      "31m 15s (- 41m 53s) (32050 42%) 4.3578\n",
      "['', 'i', 'have', 'to', 'to', 'you', 'the', 'to', '', 'the', '<EOS>']\n",
      "\n",
      "31m 18s (- 41m 50s) (32100 42%) 3.8470\n",
      "['', 'i', 'have', 'a', 'the', 'the', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "31m 21s (- 41m 47s) (32150 42%) 4.0461\n",
      "['', 'i', 'have', 'a', 'i', 'the', 'to', 'i', '<EOS>']\n",
      "\n",
      "31m 24s (- 41m 44s) (32200 42%) 4.3739\n",
      "['', 'i', 'have', 'a', 'i', 'the', 'the', 'you', '', 'the', 'the', '', 'to', '<EOS>']\n",
      "\n",
      "31m 27s (- 41m 41s) (32250 43%) 4.7697\n",
      "['', 'i', 'have', 'a', 'i', 'the', 'to', 'the', 'the', '<EOS>']\n",
      "\n",
      "31m 29s (- 41m 38s) (32300 43%) 4.0584\n",
      "['', 'i', 'do', 'have', 'a', 'the', 'you', '', 'the', 'to', '', '<EOS>']\n",
      "\n",
      "31m 32s (- 41m 35s) (32350 43%) 3.9554\n",
      "['', 'i', 'you', 'a', 'the', 'the', 'the', 'the', 'you', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "31m 35s (- 41m 32s) (32400 43%) 4.4825\n",
      "['', 'i', 'do', 'you', 'to', 'like', 'to', 'i', '', 'to', '', '<EOS>']\n",
      "\n",
      "31m 37s (- 41m 28s) (32450 43%) 4.1092\n",
      "['', 'i', 'do', 'i', 'i', 'the', 'you', 'i', 'the', 'the', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "31m 40s (- 41m 25s) (32500 43%) 4.3823\n",
      "['', 'i', 'am', 'not', 'i', 'i', 'you', 'i', 'the', 'i', 'the', '', 'the', '', 'i', '<EOS>']\n",
      "\n",
      "31m 43s (- 41m 22s) (32550 43%) 4.6710\n",
      "['', 'i', 'do', 'i', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "31m 46s (- 41m 20s) (32600 43%) 4.3699\n",
      "['', 'i', 'have', 'a', 'i', 'the', 'the', 'you', 'the', 'a', 'the', 'the', 'i', '', 'the', 'a', 'the', '<EOS>']\n",
      "\n",
      "31m 49s (- 41m 17s) (32650 43%) 4.3730\n",
      "['', 'i', 'have', 'the', 'you', 'the', 'the', 'the', 'you', 'the', 'the', '', 'to', 'the', '<EOS>']\n",
      "\n",
      "31m 52s (- 41m 14s) (32700 43%) 4.2740\n",
      "['', 'yes', 'i', 'do', 'like', 'the', 'you', 'you', 'have', 'you', 'to', 'have', 'you', 'the', 'a', '', '<EOS>']\n",
      "\n",
      "31m 55s (- 41m 11s) (32750 43%) 4.5308\n",
      "['', 'i', 'am', 'a', 'little', 'you', 'have', 'the', 'internet', 'and', 'the', 'internet', 'is', 'a', 'lot', 'of', 'the', '<EOS>']\n",
      "\n",
      "31m 59s (- 41m 9s) (32800 43%) 4.3835\n",
      "['', 'i', 'do', 'the', 'a', 'the', 'the', 'a', 'the', 'a', 'it', 'i', '<EOS>']\n",
      "\n",
      "32m 2s (- 41m 6s) (32850 43%) 4.4402\n",
      "['', 'i', 'do', 'the', 'the', 'the', 'a', 'the', 'the', 'a', 'the', 'a', '<EOS>']\n",
      "\n",
      "32m 5s (- 41m 3s) (32900 43%) 4.3806\n",
      "['', 'i', 'do', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "32m 8s (- 41m 0s) (32950 43%) 4.0814\n",
      "['', 'i', 'do', 'like', 'the', 'the', 'you', '<EOS>']\n",
      "\n",
      "32m 10s (- 40m 56s) (33000 44%) 3.3915\n",
      "['', 'i', 'i', 'i', 'i', 'you', 'the', 'the', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m attn_decoder1 \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# encoder1.load_state_dict(torch.load(PATH_ENC)) # можно и другую директорию, но вот это прямо внутри вашего гугл диска\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# attn_decoder1.load_state_dict(torch.load(PATH_DECO))\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_decoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m75000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m training_pair[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m target_tensor \u001b[38;5;241m=\u001b[39m training_pair[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     24\u001b[0m plot_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     64\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     65\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m target_length\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "# encoder1.load_state_dict(torch.load(PATH_ENC)) # можно и другую директорию, но вот это прямо внутри вашего гугл диска\n",
    "# attn_decoder1.load_state_dict(torch.load(PATH_DECO))\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_wUdnU1kf-s",
    "outputId": "b26cb3f8-a345-4bf7-93cb-dd6da9204a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> wow that is a lot of knowledge for the future of cinematography .\n",
      "= he was married to a white woman but the marriage was not legal until !\n",
      "< he s ! good the not of not not not not not not not not have ! the university of the from not have of not have ! have ! of have ! have of have of have of have of have the of not not not not not not not have of not have of not have ! of have of have of have ! of have of have ! without of the white not not not not not not have the moon of not of not have of not not not not not not not have of not not have ! of the moon not have not not not not not not not not have of not have ! of have of have of have ! of the white not have of not have not have of not have of not have of not have ! of have the moon of not have ! have ! of have of have of have of have the moon of not have of not not not not not not not not not not not not have of not have not have of not not not have ! of the moon but he have the of not not not not have of not not have of not not have of not not have of not have of not have of not not have ! of have of have of have of have the moon but he have of have of have of have of have of have s of have of have of the moon but he have ! without of have of have of have of have of have of have of have of have of have of have s the not not not have of not have ! have of have of have ! have of have of have of have ! of have of have ! have the moon of have of have of have of have of have of have of have of have of have of the white woman have the of not have of not have ! have of have of have of have the of not have of not have of not have not have ! have of have of have ! of have of have ! of have ! have of have ! have of have of have ! of have the moon of have of have of have of have of have of have of have of have of have the of not not not not not not not not have of not have ! have of have the of not have ! have the moon but he have ! of have ! without of the moon not not not not not have of not not not have of not not of not have ! of have the of not not have of not have not have ! of the moon but he have of have of have of have of have of have of have of have of have of have of have ! of have of have ! without of have of have of have ! of have of have of have of have ! of have the of not not have ! have of have the moon but he of have the of not not not not not have of not have of not have ! of the white not have of not have ! of the white not have of not have of not have ! of have ! have of have ! of have of have the of not have of not not have ! of have the moon not not not not not not have not not not not not not not not not not not have not not not not not not have of not have ! without of have of have the university but he have of have of have s the he but he have of have s the he but he of have of have s of have of have of have of have of have s of the moon but he have of have s the he have of have of have of have of have of have of have s of have of have of have of have of have of have of have s of have of have of have of have of have of have s of have of have of have of have of have of have of have of have of have of have of have of have s but he have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of without of without from but he of have of have of have of have of have of without of the university but he of have of have of have of have of have of have of have of have of have of but he have of have s the he have of have s of have of have of have of have of have of have of s google from but he of have of have of have of have of have of have of have of have of have of have of have of have ! of have of have of have of have of have of have of have of have of have of have s not have of have of have of have of have of have of have of have of have s not have of have of have of have of have of have of have of have of have s not have of have of have of have of have of have of have of have of have of have of have of have of have of have of good have of have of have of have of have of\n",
      "\n",
      "> well i believe that . that makes sense of course . its interesting since the s of the time the taller candidate has won .\n",
      "= interesting . i found funny the method of election they had in ancient greece it was by lottery\n",
      "< it ! have ! have ! have ! have ! have ! have ! i have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have\n",
      "\n",
      "> so true ! i wouldn t mind seeing a horror movie about flame throwers in vehicles over in south africa where it s legal to prevent car jacking . ha !\n",
      "= that would be scary ! mcdonalds was the first restaurant with a drive thru !\n",
      "< ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "\n",
      "> what s interesting though is that when earth was formed a day was only about hours long now it s hours i wonder if the days will keep getting longer\n",
      "= i guess they will the work day will get up to hours . we need jobs reforesting the planet anyway .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< he i think we get will get get he get get get get get get get get get get get get get get get get get get he lol get get get he lol get get get get get get he get get get get get get get get get get get he up will get he get will get he get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get will get get get get he get get get get will get he get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get will get he get will get get get to get will get will get will get will get he get will get will get will get will get will get will get will get will get will he will get will he will get will get will get will get will get he lol will get will get will get he lol will get will get will get will get he will get will get will get will get will get he lol will get will get will get will get will get will get will get will get will <EOS>\n",
      "\n",
      "> it was a little before my time . i m more of the tonka generation .\n",
      "= i am the transformers generation from the s . i collected the toys but did not see the movies\n",
      "< what ! have ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "\n",
      "> that is an interesting way to go about it . i cant believe it all started over a late fee .\n",
      "= pretty crazy because now netflix has over million subscribers and growing\n",
      "< yeah ! ! ! ! have ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "\n",
      "> that is a lot of shoes . they must need it for all the extreme moves they perform . kind of like michael jackson s extreme lean that he has a patent on .\n",
      "= you re probably right !\n",
      "< what ! have ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! have ! have ! ! have ! have ! have ! ! have ! have ! have ! have ! have ! have ! have ! have ! have ! ! ! ! ! have ! have ! have ! have ! have ! ! have ! ! ! have ! have ! have ! ! ! ! ! have ! have ! ! ! have ! have ! ! ! have ! have ! have ! have ! ! ! have ! ! ! ! ! have ! have ! have ! have ! have ! have ! ! ! have ! have ! have ! have ! have ! have ! have ! have ! have ! ! ! have ! have ! ! have ! ! ! ! ! ! have ! ! have ! ! have ! ! ! have ! have ! ! ! have ! ! ! have ! ! have ! ! have ! have ! ! ! have ! have ! ! ! have ! ! have ! have ! ! ! have ! ! have ! ! ! ! have ! ! have ! have ! have ! have ! have ! have ! have ! have ! ! have ! ! ! ! ! have ! ! have ! have ! have ! ! ! ! have ! have ! have ! ! have ! have ! ! ! have ! have ! have ! ! have ! ! ! have ! have ! have ! have ! ! have ! have ! ! ! ! have ! have ! ! have ! have ! ! ! ! ! ! have ! ! have ! ! have ! ! have ! ! have ! ! ! have ! have ! ! have ! have ! have ! ! have ! ! have ! ! have ! have ! have ! have ! have ! have ! have ! ! have ! have ! ! have ! have ! ! ! ! have ! ! have ! have ! have ! have ! ! ! have ! have ! have ! have ! have ! have ! ! ! have ! ! ! have ! have ! have ! ! ! ! have ! have ! have ! have ! have ! ! have ! have ! ! ! have ! have ! ! have ! ! have ! have ! have ! ! have ! ! have ! ! have ! have ! have ! have ! have ! have ! have ! ! ! ! ! ! ! ! have ! have ! ! ! have ! ! ! have ! ! ! ! ! ! have ! have ! ! ! have ! ! ! have\n",
      "\n",
      "> yes but if they are professional i think the ballet company pays for the shoes . do you prefer functional shoes or decorative fashionable shoes or both ?\n",
      "= i m a mixture of both . seems like the big money i spend for nice shoes ends up being a waste though because then i don t wear the shoes . lol\n",
      "< yeah ! <EOS>\n",
      "\n",
      "> i like watching it in person . not so much on tv . it s more exciting to be there . i ve only been to one pro game with a friend who s really into the game i think it was the nationals many years ago . i don t recall who they played or what the score was but it was a fun afternoon .\n",
      "= yes i like to watch it from home with my friends lol and did you know that baseball managers wear uniforms because they are technically able to play for their teams if the need arises ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< yeah i ! have ! of ! wear ! wear ! wear wear ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! have ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! have ! of ! of ! of ! have ! of ! of ! of ! of ! of ! of ! of ! of ! have ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! wear ! of ! of ! of ! of ! wear ! of ! of ! of ! of ! of ! of ! of ! be ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! have ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! wear ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! wear ! of ! wear ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! be of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of\n",
      "\n",
      "> there will still be helium i think . just not in the atmosphere . maybe they can create helium\n",
      "= i figure that a fusion plant could do that . probably require serious energy .\n",
      "< figure . i serious to serious to serious to serious serious energy . to serious to serious serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy . to serious energy to serious energy serious to serious to serious to serious to serious to serious to serious to serious court serious to require serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to serious to serious serious energy serious to serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious serious to serious to serious to serious serious to serious to serious to serious to serious energy serious to serious to serious to serious too to serious to serious to serious to serious to serious to serious to serious energy to serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy could to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to . serious to serious to serious serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious too serious to serious to serious to serious energy to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to too to . serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious serious to serious serious to serious to serious to serious to . serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to those to those to . serious serious to serious to serious to serious to serious to require serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy . to serious to serious to serious to serious to serious to require serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious serious serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious serious to serious to serious to serious to serious energy serious energy to serious to serious to serious to serious serious serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to . serious to serious to serious to serious to serious to serious to serious serious to serious to require serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious . serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to those to . to serious to serious to serious to serious too serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious energy to too to serious to serious to serious to serious to serious to serious to serious energy to serious serious to serious serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACG4Q06bkf-z"
   },
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-  Try with a different dataset\n",
    "\n",
    "   -  Another language pair\n",
    "   -  Human → Machine (e.g. IOT commands)\n",
    "   -  Chat → Response\n",
    "   -  Question → Answer\n",
    "\n",
    "-  Replace the embeddings with pre-trained word embeddings such as word2vec or\n",
    "   GloVe\n",
    "-  Try with more layers, more hidden units, and more sentences. Compare\n",
    "   the training time and results.\n",
    "-  If you use a translation file where pairs have two of the same phrase\n",
    "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
    "   this:\n",
    "\n",
    "   -  Train as an autoencoder\n",
    "   -  Save only the Encoder network\n",
    "   -  Train a new Decoder for translation from there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "cXeLJunHBsjD"
   },
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(encoder1, attn_decoder1, 'what are you doing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'don', 't', 'think', 'going', 'to', 'jazz', '?', '<EOS>']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq_bot.ipynb\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "061670ab7df04ec59ac89a79141d5773": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ad9f6135f4b4405b5facba733d28865": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0db735ce391a424c87a3f287340ab7fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f35c10168a74023b503c589e0a3e48e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a279709b86e47e6b44117900e352dde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f2392865fc8646509d45a5bfcdbdaad5",
       "IPY_MODEL_9d1ff43c43ee4dfd9d3d6db33f3fef7f",
       "IPY_MODEL_5bf2e4b9333d4909b661e69dc5e5bb5c"
      ],
      "layout": "IPY_MODEL_061670ab7df04ec59ac89a79141d5773"
     }
    },
    "1ecd56e5cd334e90ac1963e6255723b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2ea006d2693a46b7a6f73ad98c3ad71f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f35c10168a74023b503c589e0a3e48e",
      "placeholder": "​",
      "style": "IPY_MODEL_ed65250433ee4013acda7747b672123b",
      "value": "Downloading: 100%"
     }
    },
    "37b45c0d2e3f421f8f8dba60722b1762": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "489ea138854d48008e1904c9e46dd6c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5bf2e4b9333d4909b661e69dc5e5bb5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa1455eaad504e11b44f2a9ca79a6697",
      "placeholder": "​",
      "style": "IPY_MODEL_63a6cde631a947a6bb85d1cbb89d9f21",
      "value": " 446k/446k [00:00&lt;00:00, 683kB/s]"
     }
    },
    "5bf945ae3c7540f6ba472f20e2fdacb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7129a21f55af4ccb8bb7544ee34b6831",
       "IPY_MODEL_ffe6cf8b83c649ec8aa08d270316b730",
       "IPY_MODEL_c3382ae68dd84a4ba848cf70807658e0"
      ],
      "layout": "IPY_MODEL_0db735ce391a424c87a3f287340ab7fa"
     }
    },
    "5e76746f82b5427ab86238f4af8f0990": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a688e92efa5740e3ae9a4dc973a06df8",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fecd7f53f93644bfa8c70dc8946a7919",
      "value": 665
     }
    },
    "5f0035855dfe4cd597ee7030acf16d72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63a6cde631a947a6bb85d1cbb89d9f21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69de0ae6cbbe4eb7812e5884318d20b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a228332a0f44ba29f6e211f43c884a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e6527f3e6c24c69ab4d669c28207a84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7129a21f55af4ccb8bb7544ee34b6831": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69de0ae6cbbe4eb7812e5884318d20b6",
      "placeholder": "​",
      "style": "IPY_MODEL_6e6527f3e6c24c69ab4d669c28207a84",
      "value": "Downloading: 100%"
     }
    },
    "7740d64e27724da6ba665101cbe4fd74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fe87edbe4a945e3b48b4640aefc9aa0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9002278fd22143c69a6ab1dd360bcbcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d1ff43c43ee4dfd9d3d6db33f3fef7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ad9f6135f4b4405b5facba733d28865",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ecd56e5cd334e90ac1963e6255723b6",
      "value": 456318
     }
    },
    "a688e92efa5740e3ae9a4dc973a06df8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3382ae68dd84a4ba848cf70807658e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2f168b45b1745a3ac7fb3c21699faa9",
      "placeholder": "​",
      "style": "IPY_MODEL_37b45c0d2e3f421f8f8dba60722b1762",
      "value": " 0.99M/0.99M [00:00&lt;00:00, 2.39MB/s]"
     }
    },
    "df887e109e674daeb07ea2a6a9291202": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed65250433ee4013acda7747b672123b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "efc568a8b8644795b1ac2e3e6a533a3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df887e109e674daeb07ea2a6a9291202",
      "placeholder": "​",
      "style": "IPY_MODEL_7740d64e27724da6ba665101cbe4fd74",
      "value": " 665/665 [00:00&lt;00:00, 16.9kB/s]"
     }
    },
    "f2392865fc8646509d45a5bfcdbdaad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a228332a0f44ba29f6e211f43c884a7",
      "placeholder": "​",
      "style": "IPY_MODEL_9002278fd22143c69a6ab1dd360bcbcf",
      "value": "Downloading: 100%"
     }
    },
    "f2f168b45b1745a3ac7fb3c21699faa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f60dee956d9d444f98a69ea9a6c8f17d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2ea006d2693a46b7a6f73ad98c3ad71f",
       "IPY_MODEL_5e76746f82b5427ab86238f4af8f0990",
       "IPY_MODEL_efc568a8b8644795b1ac2e3e6a533a3c"
      ],
      "layout": "IPY_MODEL_7fe87edbe4a945e3b48b4640aefc9aa0"
     }
    },
    "fa1455eaad504e11b44f2a9ca79a6697": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fecd7f53f93644bfa8c70dc8946a7919": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ffe6cf8b83c649ec8aa08d270316b730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f0035855dfe4cd597ee7030acf16d72",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_489ea138854d48008e1904c9e46dd6c1",
      "value": 1042301
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
