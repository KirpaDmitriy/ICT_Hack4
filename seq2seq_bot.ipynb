{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhSCyy2BpoBj",
    "outputId": "27eedf05-cee7-455b-8b2a-a15145c63cc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 21 22:47:35 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.68.02    Driver Version: 512.77       CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   43C    P8    12W /  N/A |      0MiB /  6144MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pI5nKY8JpoBn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkSgEe-Jku-r"
   },
   "source": [
    "# Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QNNAb0a9kuEE"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3v2Ww3lkf9x"
   },
   "source": [
    "В этом проекте мы будем учиться делать перевод с французского на английский. Примерно так:\n",
    "\n",
    "    [KEY: > input, = target, < output]\n",
    "\n",
    "    > il est en train de peindre un tableau .\n",
    "    = he is painting a picture .\n",
    "    < he is painting a picture .\n",
    "\n",
    "    > pourquoi ne pas essayer ce vin delicieux ?\n",
    "    = why not try that delicious wine ?\n",
    "    < why not try that delicious wine ?\n",
    "\n",
    "    > elle n est pas poete mais romanciere .\n",
    "    = she is not a poet but a novelist .\n",
    "    < she not not a poet but a novelist .\n",
    "\n",
    "    > vous etes trop maigre .\n",
    "    = you re too skinny .\n",
    "    < you re all alone .\n",
    "\n",
    "Для этого мы будем исплользовать мощную идею «sequence-to-sequence» сетей (https://arxiv.org/abs/1409.3215), в которых две рекуррентные сети обучаются вместе для преоразования одной последовательности в другую.\n",
    "\n",
    "* Encoder-сеть сжимает входную последовательность в вектор.\n",
    "* Decoder-сеть разжимает этот вектор в новую последовательность.\n",
    "\n",
    "Всё как с автоэнкодерами, только encoder и decoder из разных доменов.\n",
    "\n",
    "Чтобы вся эта схема обучалась стабильнее, мы будем использовать механизм attention (https://arxiv.org/abs/1409.0473), позволяющий декодеру «фокусироваться» на специфичных токенах входной последовательности.\n",
    "\n",
    "**Рекомендуемое чтение:**\n",
    "\n",
    "-  Learning Phrase Representations using RNN Encoder-Decoder for\n",
    "   Statistical Machine Translation (https://arxiv.org/abs/1406.1078)\n",
    "-  Sequence to Sequence Learning with Neural\n",
    "   Networks (https://arxiv.org/abs/1409.3215)\n",
    "-  Neural Machine Translation by Jointly Learning to Align and\n",
    "   Translate 9https://arxiv.org/abs/1409.0473)\n",
    "-  A Neural Conversational Model (https://arxiv.org/abs/1506.05869>)\n",
    "\n",
    "Если кто-то пропустил предыдущие занатия, то лучше сначала сделать их: основные концепции такие же, как в языковых моделях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zt1ZcvSZkf9y"
   },
   "outputs": [],
   "source": [
    "# осторожно: тетрадка старая\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A9f1xpMkf90"
   },
   "source": [
    "## Данные\n",
    "\n",
    "В этом проекте мы будем работать с кучей пар предложений на английском и французском.\n",
    "\n",
    "Скачайте данные отсюда (https://download.pytorch.org/tutorial/data.zip) и возьмите оттуда файлик eng-fra.txt. В нём должно быть много строчек примерно такого формата:\n",
    "\n",
    "    I am cold.    J'ai froid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pAoNlcCkf92"
   },
   "source": [
    "Делать предобработку будем по аналогии с char-level RNN-ками из предыдущих туториалов, только на этот раз нам важно отдельно запариться с EOS (end-of-sequence) — специальным токеном, который сеть будет генерировать при окончании генерации предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zYgxYCIZkf93"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def __repr__(self):\n",
    "        most_popular_words = sorted(\n",
    "            self.word2count.keys(), key=lambda word: self.word2count[word], reverse=True\n",
    "        )[:10]\n",
    "        most_popular_words = \", \".join(most_popular_words)\n",
    "        return f\"Language: {self.name} | Num words: {self.n_words} | Most popular: {most_popular_words}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G--d0a55kf95"
   },
   "source": [
    "Все файлы в юникоде. Чтобы  облегчить нам работу, мы переведем все в ASCII, сделаем lowercase и выкинем большинство пунктуации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ygu9l499kf96"
   },
   "outputs": [],
   "source": [
    "# \"hello!\" -> hello, ! \n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# def normalizeString(s):\n",
    "#     s = unicodeToAscii(s.lower().strip())\n",
    "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "#     return s\n",
    "\n",
    "def normalizeString(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\sa-zA-Z0-9@\\[\\]]',' ',text) # Удаляет пунктцацию\n",
    "    text = re.sub(r'\\w*\\d+\\w*', '', text) # Удаляет цифры\n",
    "    text = re.sub('\\s{2,}', \" \", text) # Удаляет ненужные пробелы\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nExIwuorkf98"
   },
   "source": [
    "При чтении данных разделим файл на строки, а строки на пары.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "kR6MaACekf99"
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('happiness_provokers', encoding='utf-8')\n",
    "\n",
    "    # Get pairs and normalize\n",
    "    pairs = list(zip([normalizeString(s) for s in list(df['query'].values)], [normalizeString(s) for s in list(df.reply.values)]))\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAvHyswFrMj7",
    "outputId": "28b73ea3-9bbf-496a-b4cb-db94bf59ae5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/utsx/.local/lib/python3.8/site-packages (4.19.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: filelock in /home/utsx/.local/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/utsx/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/utsx/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NMwAc5_8rPuY"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "5bf945ae3c7540f6ba472f20e2fdacb1",
      "7129a21f55af4ccb8bb7544ee34b6831",
      "ffe6cf8b83c649ec8aa08d270316b730",
      "c3382ae68dd84a4ba848cf70807658e0",
      "0db735ce391a424c87a3f287340ab7fa",
      "69de0ae6cbbe4eb7812e5884318d20b6",
      "6e6527f3e6c24c69ab4d669c28207a84",
      "5f0035855dfe4cd597ee7030acf16d72",
      "489ea138854d48008e1904c9e46dd6c1",
      "f2f168b45b1745a3ac7fb3c21699faa9",
      "37b45c0d2e3f421f8f8dba60722b1762",
      "1a279709b86e47e6b44117900e352dde",
      "f2392865fc8646509d45a5bfcdbdaad5",
      "9d1ff43c43ee4dfd9d3d6db33f3fef7f",
      "5bf2e4b9333d4909b661e69dc5e5bb5c",
      "061670ab7df04ec59ac89a79141d5773",
      "6a228332a0f44ba29f6e211f43c884a7",
      "9002278fd22143c69a6ab1dd360bcbcf",
      "0ad9f6135f4b4405b5facba733d28865",
      "1ecd56e5cd334e90ac1963e6255723b6",
      "fa1455eaad504e11b44f2a9ca79a6697",
      "63a6cde631a947a6bb85d1cbb89d9f21",
      "f60dee956d9d444f98a69ea9a6c8f17d",
      "2ea006d2693a46b7a6f73ad98c3ad71f",
      "5e76746f82b5427ab86238f4af8f0990",
      "efc568a8b8644795b1ac2e3e6a533a3c",
      "7fe87edbe4a945e3b48b4640aefc9aa0",
      "0f35c10168a74023b503c589e0a3e48e",
      "ed65250433ee4013acda7747b672123b",
      "a688e92efa5740e3ae9a4dc973a06df8",
      "fecd7f53f93644bfa8c70dc8946a7919",
      "df887e109e674daeb07ea2a6a9291202",
      "7740d64e27724da6ba665101cbe4fd74"
     ]
    },
    "id": "9D0dSudIra29",
    "outputId": "ea4ff828-33a6-417c-df78-aa39e7cf5854"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dxwJGabVrl9Y"
   },
   "outputs": [],
   "source": [
    "seq = \"\"\"Hi bruh wtf r u doing ahh?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ld2s6T5IrpRY",
    "outputId": "b4df701d-72c0-4cc0-e314-d415a34f27ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      " bru\n",
      "h\n",
      " w\n",
      "tf\n",
      " r\n",
      " u\n",
      " doing\n",
      " a\n",
      "hh\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer.encode(seq):\n",
    "  print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSo1cu0rkf-C"
   },
   "source": [
    "Полный процесс такой:\n",
    "\n",
    "- Считать текстовый файл, просплитить по линиям, а затем по парам.\n",
    "- Нормализовать текст, профильтровать по длине.\n",
    "- Сделать готовые списки слов из сырых предложений в каждом языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "1G_BKtnukf-D",
    "outputId": "f3e4fd1e-3fdd-46c7-9ded-85397918896d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 32885 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_user 18346\n",
      "eng_emotion_provoker 18295\n",
      "(' no i m not did you think this is most important for everyone ', ' i think everyone should stand up against hate ')\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    pairs = pairs[::]\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng_user', 'eng_emotion_provoker')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "OM1rYFl6lRi3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language: eng_user | Num words: 18346 | Most popular: , i, the, that, a, to, it, of, is, in"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "GmEuofeTlPKw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language: eng_emotion_provoker | Num words: 18295 | Most popular: , i, the, that, a, to, it, of, is, you"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "4dMte9MFpoBy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32885"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "bBIv4CpwpoBy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' i don t know but i am at the end of knowledge on this subject what do you think about the facts of comics i would have thought us invented them but it s japan or uk ',\n",
       " ' japan has a great printing history i am not surprised about this do you read comics books i have read a few but not about super heroes ')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmijlK-akf-G"
   },
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
    "black cat\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzGBS-alkf-H"
   },
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtVkq0VQBhdx"
   },
   "source": [
    "(https://blog.floydhub.com/content/images/2019/07/image17-1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "qpcrvvXakf-I"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # num_embedding = vocab_size_fra\n",
    "        self.embedder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # (batch_size, num_words) -> (batch_size, num_words, dim_1)\n",
    "        embeddings = self.embedder(input).view(1, 1, -1)\n",
    "        # (batch_size, num_words, dim_2)\n",
    "        output, hidden = self.gru(embeddings, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "-WEZjgj0poBz"
   },
   "outputs": [],
   "source": [
    "tokens = torch.randint(0, 1000, size=(128, 40))\n",
    "embedder = nn.Embedding(1000, 128)  # здесь лежит матрица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "6-Ick8dBpoBz"
   },
   "outputs": [],
   "source": [
    "onehot = torch.nn.functional.one_hot(tokens, num_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "w3kNebFKpoBz"
   },
   "outputs": [],
   "source": [
    "embeddingds_first_way = embedder(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "-LYlliqEtLay"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 40, 128])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddingds_first_way.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNZcay6Hkf-K"
   },
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWJW6eKikf-L"
   },
   "source": [
    "Simple Decoder\n",
    "_________________\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string ``<SOS>`` or `<BOS>`\n",
    "token, and the first hidden state is the context vector (the encoder's\n",
    "last hidden state).\n",
    "The last token is `<EOS>` whats mean end of string\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "WtM1ZGRskf-M"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedder = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # (batch_size, num_words, dim)\n",
    "        # (1, 1, num_words * dim)\n",
    "        output = self.embedder(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # (batch_size, num_words, dim) -> (batch_size, num_words, num_classes)\n",
    "        # (batch_size, num_words, vocab_size_eng)\n",
    "        output = self.out(output[0])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfTzzPptkf-O"
   },
   "source": [
    "I encourage you to train and observe the results of this model, but to\n",
    "save space we'll be going straight for the gold and introducing the\n",
    "Attention Mechanism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNllTJbIkf-P"
   },
   "source": [
    "Attention Decoder\n",
    "____________\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1sZQGIJpoB0"
   },
   "outputs": [],
   "source": [
    "# v^TWm\n",
    "# U^T tanh (W_1 v + W_2 m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "LZwO_Gg2kf-P"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(\n",
    "                torch.cat((embedded[0], hidden[0]), 1)\n",
    "            ), \n",
    "            dim=1\n",
    "        )\n",
    "        attn_applied = torch.bmm(\n",
    "            attn_weights.unsqueeze(0),\n",
    "            encoder_outputs.unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLtuThIQkf-R"
   },
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
    "  limitation by using a relative position approach. Read about \"local\n",
    "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
    "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Nmb8OwO7kf-S"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3OzldY0kf-U"
   },
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "na-c3f0nkf-V"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(\n",
    "    input_tensor, \n",
    "    target_tensor,\n",
    "    encoder, \n",
    "    decoder, \n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer, \n",
    "    criterion,\n",
    "    max_length=MAX_LENGTH\n",
    "):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        # y_true: [sos, i, love, pizza, eos]\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # [0.9, 0.1, 0.0]\n",
    "            # [1, 0, 0]\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # beam_search is betters\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                # y_true: [sos, i, eos]\n",
    "                # [sos, i, eos, love]\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfmlhEzDkf-X"
   },
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "WNbWUTxfkf-Y"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc8pY8ZCkf-a"
   },
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "skZhUHw1kf-a"
   },
   "outputs": [],
   "source": [
    "PATH_ENC = \"blablaenc.pt\"\n",
    "PATH_DECO = \"blabladeco.pt\"\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % 100 == 0:\n",
    "          torch.save(encoder.state_dict(), PATH_ENC)\n",
    "          torch.save(decoder.state_dict(), PATH_DECO)\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "            output_words_check, _ = evaluate(encoder, decoder, 'what are you doing my dear friend')\n",
    "            print(output_words_check)\n",
    "            print()\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT4JmR_fkf-d"
   },
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Zf3Fhj-5kf-e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgHOJ5Ugkf-j"
   },
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "XvYgjVG_kf-j"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-Xly1n3kf-l"
   },
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "58m1z4kJkf-m"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrSEC280kf-o"
   },
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n",
    ".. Note::\n",
    "   If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBS0Q3gokf-p",
    "outputId": "8851f1df-8bc3-4052-b25f-4aa9d8e07a17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 11s (- 292m 55s) (50 0%) 6.3003\n",
      "['', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to']\n",
      "\n",
      "0m 15s (- 199m 4s) (100 0%) 4.0903\n",
      "['', 'i', 'is', 'is', 'is', 'is', 'is', '', '<EOS>']\n",
      "\n",
      "0m 19s (- 163m 54s) (150 0%) 5.7676\n",
      "['', 'i', 'know', '', '<EOS>']\n",
      "\n",
      "0m 22s (- 141m 55s) (200 0%) 5.1937\n",
      "['', 'did', 'you', 'know', 'that', 'the', 'a', '', '<EOS>']\n",
      "\n",
      "0m 25s (- 127m 5s) (250 0%) 5.3416\n",
      "['', 'that', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', '', '<EOS>']\n",
      "\n",
      "0m 28s (- 117m 59s) (300 0%) 5.2108\n",
      "['', 'yeah', 'that', 'it', 'is', 'it', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', '', '<EOS>']\n",
      "\n",
      "0m 31s (- 111m 9s) (350 0%) 5.3657\n",
      "['', 'yes', 'you', 'know', 'that', 'you', '', '<EOS>']\n",
      "\n",
      "0m 34s (- 106m 36s) (400 0%) 4.9736\n",
      "['', 'maybe', 'and', 'that', 'a', 'the', 'a', '<EOS>']\n",
      "\n",
      "0m 36s (- 101m 30s) (450 0%) 4.3648\n",
      "['', 'i', 'i', 'like', 'i', 'like', 'i', 'like', 'i', 'like', 'i', 'like', 'the', 'i', 'like', 'i', 'like', 'i', 'like', 'the', 'i', 'like', 'i', 'like', 'the', '', '<EOS>']\n",
      "\n",
      "0m 39s (- 98m 36s) (500 0%) 4.6247\n",
      "['', 'i', 'like', 'to', 'they', 'like', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "0m 42s (- 96m 58s) (550 0%) 5.6065\n",
      "['', 'i', 'think', 'to', 'the', 'the', 'to', 'the', 'the', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "0m 45s (- 94m 47s) (600 0%) 4.9780\n",
      "['', 'i', 'do', 'you', 'know', 'that', 'the', '<EOS>']\n",
      "\n",
      "0m 48s (- 93m 2s) (650 0%) 4.9535\n",
      "['', 'i', 'like', 'that', 'the', 'the', 'to', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "0m 52s (- 92m 1s) (700 0%) 5.0770\n",
      "['', 'i', 'guess', 'a', 'i', 'was', 'a', 'i', '<EOS>']\n",
      "\n",
      "0m 55s (- 91m 47s) (750 1%) 5.4467\n",
      "['', 'i', 'think', 'the', 'the', '<EOS>']\n",
      "\n",
      "0m 58s (- 90m 55s) (800 1%) 5.1025\n",
      "['', 'i', 'i', 'like', 'i', 'don', 't', 't', 'know', 'he', '', '<EOS>']\n",
      "\n",
      "1m 2s (- 90m 12s) (850 1%) 4.9114\n",
      "['', 'i', 'guess', 'to', 'be', 'the', 'to', 'be', 'the', '', '<EOS>']\n",
      "\n",
      "1m 4s (- 89m 10s) (900 1%) 5.0500\n",
      "['', 'i', 'did', 'you', 'know', 'that', 'to', 'the', 'i', 'wonder', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "1m 7s (- 87m 55s) (950 1%) 4.8429\n",
      "['', 'i', 'i', 't', 'know', 'i', 'think', 'to', 't', 'i', 'think', 'to', 't', 'i', 'think', 'to', 't', 'i', 'think', 'to', '<EOS>']\n",
      "\n",
      "1m 10s (- 87m 4s) (1000 1%) 4.8438\n",
      "['', 'yeah', 'i', 'is', 'the', 'and', 'i', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'i', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'the', 'and', 'and', 'i', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'the', 'and', 'and', 'i', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'i', 'and', 'the', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'i', 'and', 'the', 'and', 'and', 'the', 'and', 'and', 'the', 'and', 'the', 'and', 'and', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 13s (- 86m 47s) (1050 1%) 4.8336\n",
      "['', 'i', 'is', 'the', 'i', 'of', 'the', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "1m 16s (- 85m 56s) (1100 1%) 4.5669\n",
      "['', 'i', 'don', 't', 'know', 'that', 'is', 'the', '', '<EOS>']\n",
      "\n",
      "1m 19s (- 85m 2s) (1150 1%) 4.5388\n",
      "['', 'i', 'would', 'be', 'i', 'think', 'it', 'was', 'it', 'was', 'a', 'i', '', '<EOS>']\n",
      "\n",
      "1m 22s (- 84m 23s) (1200 1%) 4.9359\n",
      "['', 'yes', 'you', 'know', 'the', '', '<EOS>']\n",
      "\n",
      "1m 24s (- 83m 29s) (1250 1%) 4.5608\n",
      "['', 'i', 'know', 'that', 'i', 'know', 'that', 'of', 'the', 'on', 'the', 'on', 'the', '', '<EOS>']\n",
      "\n",
      "1m 27s (- 82m 54s) (1300 1%) 4.8308\n",
      "['', 'that', 's', 'you', 'a', 'a', 'good', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "1m 30s (- 82m 33s) (1350 1%) 5.0135\n",
      "['', 'i', 'i', 'was', 'a', 'i', 'was', 'a', 'i', 'was', 'a', 'i', 'was', 'a', 'i', 'was', 'a', 'i', 'was', 'a', 'i', 'was', 'a', 'i', 'was', 'a', 'i', 'was', 'a', 'i', 'was', 'a', 'i', 'was', 'a', 'i', '', '<EOS>']\n",
      "\n",
      "1m 33s (- 82m 7s) (1400 1%) 4.7195\n",
      "['', 'i', 'i', 'the', 'that', 'i', 'the', 'that', 'i', 'the', 'that', 'the', 'that', '', '<EOS>']\n",
      "\n",
      "1m 36s (- 81m 32s) (1450 1%) 4.6032\n",
      "['', 'that', 'is', 'the', 'that', 'is', 'the', 'a', 'in', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "1m 39s (- 81m 13s) (1500 2%) 4.8082\n",
      "['', 'that', 'i', 'am', 'you', 'know', 'the', 'that', 'to', 'the', 'that', 'to', 'the', 'that', '', '<EOS>']\n",
      "\n",
      "1m 42s (- 80m 59s) (1550 2%) 4.7528\n",
      "['', 'yeah', 'it', 'is', 'a', 'that', 'is', 'a', 'that', 'is', 'a', 'that', 'is', 'a', 'that', '<EOS>']\n",
      "\n",
      "1m 45s (- 80m 37s) (1600 2%) 4.8282\n",
      "['', 'that', 's', 'you', 'know', 'that', 's', 'you', '', '<EOS>']\n",
      "\n",
      "1m 48s (- 80m 11s) (1650 2%) 4.4084\n",
      "['', 'i', 'think', 'it', 's', 'a', 'i', 'that', 'a', 'lot', 'of', 'the', 'it', '', '<EOS>']\n",
      "\n",
      "1m 51s (- 80m 3s) (1700 2%) 4.7513\n",
      "['', 'i', 'that', 'i', 'have', 'to', 'a', 'that', 'i', 'have', 'to', 'do', 'you', 'have', 'to', '', '<EOS>']\n",
      "\n",
      "1m 54s (- 79m 33s) (1750 2%) 4.5839\n",
      "['', 'i', 'you', 'that', 'i', 'you', 'of', 'the', 'that', 'i', 'you', 'of', 'the', '<EOS>']\n",
      "\n",
      "1m 56s (- 79m 9s) (1800 2%) 4.7352\n",
      "['', 'yes', 'i', 'would', 'have', 'a', 'you', '', '<EOS>']\n",
      "\n",
      "1m 59s (- 78m 59s) (1850 2%) 4.7574\n",
      "['', 'yeah', 'they', 'is', 'a', 'a', 'a', 'a', 'is', 'a', 'a', '<EOS>']\n",
      "\n",
      "2m 2s (- 78m 48s) (1900 2%) 4.2030\n",
      "['', 'that', 's', 'they', 'are', 'a', 'they', 'are', 'a', '<EOS>']\n",
      "\n",
      "2m 6s (- 79m 0s) (1950 2%) 5.1984\n",
      "['', 'i', 'think', 'the', 'you', 'know', 'the', 'you', 'know', 'the', 'you', '', '<EOS>']\n",
      "\n",
      "2m 10s (- 79m 10s) (2000 2%) 5.0456\n",
      "['', 'i', 'it', 'was', 'the', 'it', 'was', '<EOS>']\n",
      "\n",
      "2m 13s (- 79m 20s) (2050 2%) 5.1922\n",
      "['', 'i', 'do', 'you', 'know', 'the', 'i', 'wonder', 'if', 'it', 's', 'it', 's', 'a', 'on', 'the', 'on', 'a', '', '<EOS>']\n",
      "\n",
      "2m 17s (- 79m 30s) (2100 2%) 5.1261\n",
      "['', 'i', 'think', 'that', 'is', 'a', 'a', 'a', 'for', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "2m 20s (- 79m 17s) (2150 2%) 4.9605\n",
      "['', 'i', 'agree', 'i', 'guess', 'i', 'guess', 'they', 'are', 'to', 'be', 'a', 'the', 'i', '<EOS>']\n",
      "\n",
      "2m 23s (- 79m 13s) (2200 2%) 4.8720\n",
      "['', 'that', 'is', 'the', 'i', 'a', 'i', 'a', 'i', 'a', 'on', 'the', 'a', 'on', '', '<EOS>']\n",
      "\n",
      "2m 26s (- 78m 56s) (2250 3%) 4.6594\n",
      "['', 'i', 'never', 'a', 'that', 'is', 'a', 'and', 'it', 'is', 'a', 'and', 'it', 'is', 'a', '<EOS>']\n",
      "\n",
      "2m 29s (- 78m 54s) (2300 3%) 4.7093\n",
      "['', 'i', 'i', 'to', 'i', 'to', 'the', 'i', 'to', 'i', 'to', '<EOS>']\n",
      "\n",
      "2m 33s (- 79m 8s) (2350 3%) 5.1241\n",
      "['', 'i', 'was', 'a', 'to', 'the', 'you', 'to', 'be', 'in', 'the', 'was', 'to', 'be', 'in', 'the', 'was', 'to', 'be', 'in', 'the', 'was', 'to', 'be', 'in', 'the', '', '<EOS>']\n",
      "\n",
      "2m 36s (- 78m 50s) (2400 3%) 4.5610\n",
      "['', 'i', 'it', 'was', 'a', 'the', 'to', 'the', 'the', 'to', 'the', 'to', 'the', 'the', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "2m 39s (- 78m 53s) (2450 3%) 4.9240\n",
      "['', 'i', 'that', 'that', 'is', 'a', 'that', 'that', 'is', 'to', 'a', 'you', 'that', 'that', 'they', 'can', 't', 'know', 'that', 'they', 'can', 't', 'to', 'a', 'that', '', '<EOS>']\n",
      "\n",
      "2m 43s (- 78m 56s) (2500 3%) 4.8381\n",
      "['', 'i', 'think', 'i', 'think', 'i', 'think', 'that', 'is', 'the', 'i', '', '<EOS>']\n",
      "\n",
      "2m 46s (- 78m 47s) (2550 3%) 4.9142\n",
      "['', 'i', 'have', 'have', 'a', 'the', 'the', 'to', 'the', 'the', 'the', 'to', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "2m 49s (- 78m 42s) (2600 3%) 4.9262\n",
      "['', 'yes', 'and', 'is', 'a', 'of', 'is', 'of', 'a', 'of', 'is', 'of', 'a', 'of', 'is', 'of', 'a', 'of', 'is', 'of', 'a', 'of', 'the', 'is', 'of', 'a', 'of', 'is', 'of', 'a', 'of', 'is', 'of', 'a', 'of', 'is', '', '<EOS>']\n",
      "\n",
      "2m 52s (- 78m 21s) (2650 3%) 4.5559\n",
      "['', 'i', 'think', 'that', 'is', 'that', 'is', 'that', 'is', 'of', 'the', '<EOS>']\n",
      "\n",
      "2m 55s (- 78m 17s) (2700 3%) 4.6349\n",
      "['', 'i', 'i', 'a', 'the', 'a', 'i', 'the', 'a', 'a', '', '<EOS>']\n",
      "\n",
      "2m 58s (- 78m 2s) (2750 3%) 4.7195\n",
      "['', 'that', 'is', 'a', 'i', 'i', 'i', 'a', 'i', 'i', 'i', 'a', 'i', 'i', 'i', 'a', 'i', 'i', 'i', 'a', 'i', 'i', '<EOS>']\n",
      "\n",
      "3m 1s (- 77m 59s) (2800 3%) 4.8953\n",
      "['', 'i', 'i', 'that', 'a', 'a', 'the', 'a', 'to', 'the', '', 'that', 's', 'a', 'the', '', 'that', 's', 'a', '', 'that', 's', 'a', '', '<EOS>']\n",
      "\n",
      "3m 4s (- 78m 1s) (2850 3%) 4.6562\n",
      "['', 'yeah', 'and', 'they', 'are', 'a', 'a', 'it', 'in', 'a', '', '<EOS>']\n",
      "\n",
      "3m 8s (- 77m 59s) (2900 3%) 4.8325\n",
      "['', 'yeah', 'i', 'the', 't', 'the', 'the', 'in', 'the', 'the', 'in', 'the', '<EOS>']\n",
      "\n",
      "3m 11s (- 77m 55s) (2950 3%) 4.6607\n",
      "['', 'i', 'would', 'have', 'have', 'you', 'know', 'that', 'would', 'be', 'a', 'lot', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "3m 14s (- 77m 56s) (3000 4%) 4.6211\n",
      "['', 'i', 's', 't', 'know', 'the', 'the', '<EOS>']\n",
      "\n",
      "3m 18s (- 77m 54s) (3050 4%) 4.7268\n",
      "['', 'yeah', 'that', 's', 'a', 'are', 'in', 'the', 'you', '', '<EOS>']\n",
      "\n",
      "3m 21s (- 77m 56s) (3100 4%) 4.4514\n",
      "['', 'i', 'i', 'a', 'a', 'great', 'of', '<EOS>']\n",
      "\n",
      "3m 24s (- 77m 53s) (3150 4%) 4.5816\n",
      "['', 'i', 'i', 'the', 'i', 'the', 'in', 'the', 'the', 'in', 'the', '', '<EOS>']\n",
      "\n",
      "3m 27s (- 77m 46s) (3200 4%) 4.1775\n",
      "['', 'i', 'i', 'i', 'a', 'a', 'you', 'a', '<EOS>']\n",
      "\n",
      "3m 31s (- 77m 48s) (3250 4%) 4.9281\n",
      "['', 'yeah', 'i', 'don', 't', 'know', 'the', 'the', 'that', 'the', 'to', 'the', 'like', 'to', 'the', 'the', 'to', 'the', 'the', 'to', '<EOS>']\n",
      "\n",
      "3m 34s (- 77m 43s) (3300 4%) 4.9892\n",
      "['', 'i', 'i', 'the', 'i', 'the', 'i', 'the', 'i', 'that', 'the', '<EOS>']\n",
      "\n",
      "3m 37s (- 77m 31s) (3350 4%) 4.9868\n",
      "['', 'i', 'i', 've', 'to', 'to', 'the', 'a', 'a', 'to', 'to', 'the', 'a', '', '<EOS>']\n",
      "\n",
      "3m 40s (- 77m 20s) (3400 4%) 4.9004\n",
      "['', 'yeah', 'it', 's', 'a', 'the', 'one', 'of', 'the', 'a', 's', 'in', 'the', 's', 'in', '', '<EOS>']\n",
      "\n",
      "3m 43s (- 77m 6s) (3450 4%) 4.1930\n",
      "['', 'i', 'do', 'you', 'know', 'that', 'the', 'the', 'i', 'the', 'the', '<EOS>']\n",
      "\n",
      "3m 45s (- 76m 51s) (3500 4%) 4.2729\n",
      "['', 'i', 'agree', 'i', 'that', 'it', 's', 'a', 'it', 's', 'to', '<EOS>']\n",
      "\n",
      "3m 48s (- 76m 36s) (3550 4%) 4.4483\n",
      "['', 'i', 'did', 'you', 'know', 'that', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "3m 50s (- 76m 17s) (3600 4%) 4.2006\n",
      "['', 'i', 's', 'a', 'a', 'i', 'the', 'and', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "3m 53s (- 76m 2s) (3650 4%) 4.4486\n",
      "['', 'i', 'that', 'is', 'a', 'that', 'the', 'that', 'i', 'like', 'that', 'the', 'that', 'the', '', '<EOS>']\n",
      "\n",
      "3m 56s (- 75m 50s) (3700 4%) 4.5711\n",
      "['', 'yeah', 'that', 'is', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "3m 59s (- 75m 42s) (3750 5%) 4.6225\n",
      "['', 'it', 's', 'a', 'lot', 'of', 'a', 'you', '<EOS>']\n",
      "\n",
      "4m 1s (- 75m 28s) (3800 5%) 4.4103\n",
      "['', 'that', 'is', 'it', 'is', 'a', 'they', 'in', 'it', '', '<EOS>']\n",
      "\n",
      "4m 4s (- 75m 23s) (3850 5%) 4.5903\n",
      "['', 'i', 'i', 'know', 'that', 'the', 'to', 'be', 'a', 'to', '<EOS>']\n",
      "\n",
      "4m 8s (- 75m 23s) (3900 5%) 4.5830\n",
      "['', 'i', 'i', 's', 'i', 'that', 'i', 's', '<EOS>']\n",
      "\n",
      "4m 11s (- 75m 20s) (3950 5%) 4.0679\n",
      "['', 'yeah', 'i', 'have', 'been', 'in', 'to', 'have', 'a', 'good', 'in', '<EOS>']\n",
      "\n",
      "4m 14s (- 75m 23s) (4000 5%) 4.6724\n",
      "['', 'i', 'don', 't', 'know', 'that', 'a', 'lot', 'of', 'the', 'a', 'is', 'of', 'the', 'a', '', '', '<EOS>']\n",
      "\n",
      "4m 17s (- 75m 10s) (4050 5%) 4.3477\n",
      "['', 'yes', 'they', 'are', 'you', '', '<EOS>']\n",
      "\n",
      "4m 21s (- 75m 13s) (4100 5%) 4.3137\n",
      "['', 'i', 'wonder', 'to', 'i', 'that', 'to', 'i', 'that', 'to', 'i', 'that', 'to', 'i', 'that', 'the', 'that', 'to', 'i', 'that', 'to', '', '<EOS>']\n",
      "\n",
      "4m 24s (- 75m 7s) (4150 5%) 4.7426\n",
      "['', 'i', 'i', 'that', 'they', 'have', 'a', 'lot', 'of', '<EOS>']\n",
      "\n",
      "4m 27s (- 75m 1s) (4200 5%) 4.3406\n",
      "['', 'yes', 'i', 'think', 'that', 'you', 'know', 'that', 'the', 'that', '<EOS>']\n",
      "\n",
      "4m 29s (- 74m 51s) (4250 5%) 4.4958\n",
      "['', 'i', 'i', 'think', 'they', 'are', 'that', 'a', 'that', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4m 32s (- 74m 45s) (4300 5%) 4.5182\n",
      "['', 'i', 'i', 'i', 'i', 'that', 'to', 'i', 'it', 'was', 'in', 'a', '', '<EOS>']\n",
      "\n",
      "4m 35s (- 74m 38s) (4350 5%) 4.7857\n",
      "['', 'i', 'i', 'i', 'the', 'that', 'i', 'the', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "4m 38s (- 74m 29s) (4400 5%) 4.3294\n",
      "['', 'i', 'do', 'not', 'sure', 'that', 'the', 'the', '', 'of', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "4m 41s (- 74m 28s) (4450 5%) 4.6180\n",
      "['', 'i', 'do', 'i', 'is', 'i', 'of', 'that', 'the', 'a', 'is', 'a', 'the', '<EOS>']\n",
      "\n",
      "4m 44s (- 74m 23s) (4500 6%) 4.5648\n",
      "['', 'i', 'agree', 'did', 'you', 'know', 'that', 'is', 'a', 'a', 'a', 'about', 'the', '<EOS>']\n",
      "\n",
      "4m 47s (- 74m 16s) (4550 6%) 4.7927\n",
      "['', 'yeah', 'that', 't', 'know', 'what', 'is', 'a', 'good', 'to', 'do', 'you', '', '<EOS>']\n",
      "\n",
      "4m 50s (- 74m 13s) (4600 6%) 4.7680\n",
      "['', 'i', 'i', 'that', 'that', 'to', 'i', 'that', 'that', 'to', 'i', 'that', 'that', 'to', 'you', '<EOS>']\n",
      "\n",
      "4m 54s (- 74m 10s) (4650 6%) 3.8601\n",
      "['', 'i', 'that', 'that', 'that', 'the', 'that', 'that', '', 'that', 'the', '', '<EOS>']\n",
      "\n",
      "4m 57s (- 74m 14s) (4700 6%) 4.5441\n",
      "['', 'i', 'think', 'that', 'is', 'the', 'a', 'the', 'you', '', '<EOS>']\n",
      "\n",
      "5m 0s (- 74m 10s) (4750 6%) 4.7115\n",
      "['', 'i', 'like', 'the', 't', 'know', 'that', 'the', 'to', 'the', 'you', '', '<EOS>']\n",
      "\n",
      "5m 4s (- 74m 10s) (4800 6%) 4.2374\n",
      "['', 'i', 'am', 'not', 'sure', 'i', 'do', 'like', 'a', 'you', '<EOS>']\n",
      "\n",
      "5m 7s (- 74m 5s) (4850 6%) 4.2339\n",
      "['', 'i', 'i', 'have', 'a', 'you', '', '', '<EOS>']\n",
      "\n",
      "5m 10s (- 73m 56s) (4900 6%) 4.1041\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'a', 'i', 'i', 'a', 'i', '<EOS>']\n",
      "\n",
      "5m 13s (- 73m 49s) (4950 6%) 4.9001\n",
      "['', 'i', 'i', 'of', 'i', 'of', 'that', 'of', 'i', 'of', '', 'the', '<EOS>']\n",
      "\n",
      "5m 15s (- 73m 42s) (5000 6%) 4.6174\n",
      "['', 'i', 'think', 'they', 'have', 'a', 'great', 'to', 'have', 'a', 'you', '', '<EOS>']\n",
      "\n",
      "5m 18s (- 73m 34s) (5050 6%) 4.3723\n",
      "['', 'i', 'm', 'not', 'sure', 'to', 'the', 'a', '', 'the', '<EOS>']\n",
      "\n",
      "5m 21s (- 73m 27s) (5100 6%) 4.3372\n",
      "['', 'i', 'agree', 't', 'know', 'that', 'i', 'was', 'a', 'lot', 'of', 'the', '', '', '<EOS>']\n",
      "\n",
      "5m 24s (- 73m 18s) (5150 6%) 4.4619\n",
      "['', 'i', 'that', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "5m 27s (- 73m 16s) (5200 6%) 4.2713\n",
      "['', 'i', 'think', 'i', 'of', 'the', 'i', 'the', 'i', 'of', 'the', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "5m 30s (- 73m 10s) (5250 7%) 4.6756\n",
      "['', 'i', 'know', 'that', 'the', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "5m 33s (- 73m 5s) (5300 7%) 4.2953\n",
      "['', 'i', 'i', 'a', 'you', 'a', 'you', 'a', 'it', '', '<EOS>']\n",
      "\n",
      "5m 36s (- 72m 59s) (5350 7%) 4.1568\n",
      "['', 'i', 'that', 'know', 'that', 'the', 'i', 'the', 'a', 'you', '', 'a', '<EOS>']\n",
      "\n",
      "5m 39s (- 72m 53s) (5400 7%) 4.6128\n",
      "['', 'i', 'would', 'not', 'know', 'that', 'a', 'the', 'of', '', '<EOS>']\n",
      "\n",
      "5m 42s (- 72m 47s) (5450 7%) 4.6635\n",
      "['', 'i', 'would', 'be', 'a', 'great', 'to', 'the', 'the', '<EOS>']\n",
      "\n",
      "5m 45s (- 72m 41s) (5500 7%) 4.1918\n",
      "['', 'i', 'm', 'not', 'sure', 'the', 'the', '<EOS>']\n",
      "\n",
      "5m 48s (- 72m 37s) (5550 7%) 4.4201\n",
      "['', 'i', 'did', 'not', 'know', 'that', 'the', 'is', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "5m 50s (- 72m 28s) (5600 7%) 3.9149\n",
      "['', 'i', 'i', 't', 'think', 'it', 'it', 'is', 'a', '', '', '<EOS>']\n",
      "\n",
      "5m 53s (- 72m 22s) (5650 7%) 4.2825\n",
      "['', 'i', 'wonder', 'if', 'the', 'the', 'the', 'the', 'the', 'to', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "5m 56s (- 72m 19s) (5700 7%) 4.5251\n",
      "['', 'i', 'guess', 'that', 'is', 'a', 'i', 'that', 'that', '', 'is', 'a', '', '<EOS>']\n",
      "\n",
      "6m 0s (- 72m 18s) (5750 7%) 4.4532\n",
      "['', 'i', 'i', 'that', 'is', 'to', 'and', 'the', 'to', 'and', 'the', 'to', 'and', 'the', 'to', 'and', 'the', 'to', '<EOS>']\n",
      "\n",
      "6m 3s (- 72m 15s) (5800 7%) 4.3385\n",
      "['', 'i', 'is', 'i', 'i', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "6m 6s (- 72m 8s) (5850 7%) 4.2436\n",
      "['', 'i', 'i', 'that', 'i', 'that', 'i', 'that', 'i', 'that', 'i', 'that', 'i', 'that', 'i', 'it', 'i', 'that', 'i', 'that', 'it', 'i', 'that', 'i', 'it', 'i', 'that', 'it', 'i', 'that', 'it', 'would', 'have', 'a', 'that', 'i', 'that', 'it', 'i', 'that', 'i', '<EOS>']\n",
      "\n",
      "6m 9s (- 72m 2s) (5900 7%) 4.4048\n",
      "['', 'i', 'i', 'is', 'crazy', 'to', '<EOS>']\n",
      "\n",
      "6m 11s (- 71m 55s) (5950 7%) 4.1502\n",
      "['', 'i', 'i', 't', 'the', 'the', 'the', 'i', '<EOS>']\n",
      "\n",
      "6m 14s (- 71m 48s) (6000 8%) 4.3420\n",
      "['', 'i', 'you', 'know', 'that', 'a', 'was', 'a', 'in', 'i', '', '<EOS>']\n",
      "\n",
      "6m 17s (- 71m 40s) (6050 8%) 4.2039\n",
      "['', 'that', 'is', 'i', 'that', 'i', 'that', 'of', 'the', 'i', '', '<EOS>']\n",
      "\n",
      "6m 20s (- 71m 35s) (6100 8%) 4.4832\n",
      "['', 'i', 'i', 'i', 'that', 'i', 'of', 'the', 'i', '<EOS>']\n",
      "\n",
      "6m 23s (- 71m 31s) (6150 8%) 4.7393\n",
      "['', 'i', 'is', 'like', 'not', 'like', 'to', 'the', 'the', '<EOS>']\n",
      "\n",
      "6m 26s (- 71m 27s) (6200 8%) 4.7985\n",
      "['', 'i', 'that', 'the', 'a', 'you', 'the', 'a', 'in', 'the', 'the', 'a', 'you', '', 'a', 'in', 'the', '', 'a', '', '', 'a', '<EOS>']\n",
      "\n",
      "6m 29s (- 71m 19s) (6250 8%) 4.2207\n",
      "['', 'i', 'wonder', 'how', 'they', 'are', 'a', 'i', 'the', 'i', '', 'the', '<EOS>']\n",
      "\n",
      "6m 32s (- 71m 15s) (6300 8%) 4.4201\n",
      "['', 'that', 'is', 'a', 'lot', 'of', 'the', 'is', 'the', 'the', 'is', 'the', '<EOS>']\n",
      "\n",
      "6m 34s (- 71m 7s) (6350 8%) 4.2918\n",
      "['', 'i', 'i', 'that', 'i', 'that', 'i', 'in', 'the', 'i', '', 'that', 'i', '<EOS>']\n",
      "\n",
      "6m 37s (- 71m 4s) (6400 8%) 4.3881\n",
      "['', 'yeah', 'that', 's', 'interesting', 'i', 'to', 'to', 'in', 'to', 'to', 'in', 'the', 'to', '<EOS>']\n",
      "\n",
      "6m 40s (- 70m 56s) (6450 8%) 4.5785\n",
      "['', 'i', 'guess', 'the', 's', 'the', 's', 'the', 'the', '<EOS>']\n",
      "\n",
      "6m 43s (- 70m 50s) (6500 8%) 3.9410\n",
      "['', 'i', 'didn', 't', 'know', 'that', 'the', 'the', 'the', 'he', 'the', 'the', 'the', 'he', 't', 'the', 'to', 'in', 'the', 'the', '<EOS>']\n",
      "\n",
      "6m 46s (- 70m 43s) (6550 8%) 4.2883\n",
      "['', 'i', 'is', 'i', 'to', 'the', 'the', 'he', 'is', '<EOS>']\n",
      "\n",
      "6m 49s (- 70m 41s) (6600 8%) 4.4385\n",
      "['', 'i', 'that', 'that', 'that', 'i', 'that', 'the', 'that', '<EOS>']\n",
      "\n",
      "6m 51s (- 70m 33s) (6650 8%) 3.7849\n",
      "['', 'i', 'is', 'i', 'like', 'that', 'the', 'the', 'a', 'the', 'the', '<EOS>']\n",
      "\n",
      "6m 54s (- 70m 27s) (6700 8%) 3.9543\n",
      "['', 'i', 'is', 'a', 'to', 'are', 'a', 'to', 'in', 'the', 'to', '', '<EOS>']\n",
      "\n",
      "6m 57s (- 70m 26s) (6750 9%) 4.3864\n",
      "['', 'i', 'i', 'the', 'to', 'the', 'the', 'a', '', 'the', '<EOS>']\n",
      "\n",
      "7m 1s (- 70m 22s) (6800 9%) 4.5795\n",
      "['', 'i', 'have', 'never', 'been', 'that', 'to', 'you', 'the', 'you', '', '<EOS>']\n",
      "\n",
      "7m 3s (- 70m 17s) (6850 9%) 4.7832\n",
      "['', 'that', 'is', 'a', 'you', 'a', 'you', 'that', 'a', 'you', '<EOS>']\n",
      "\n",
      "7m 6s (- 70m 13s) (6900 9%) 4.6535\n",
      "['', 'that', 's', 'i', 'that', 'you', 'the', '', '<EOS>']\n",
      "\n",
      "7m 9s (- 70m 5s) (6950 9%) 3.9698\n",
      "['', 'i', 'i', 'the', 'the', 'you', 'the', 'the', '<EOS>']\n",
      "\n",
      "7m 12s (- 70m 2s) (7000 9%) 4.4112\n",
      "['', 'i', 'is', 'that', 'the', 'that', 'the', 'the', 'that', 'the', 'the', 'that', 'the', 'the', '<EOS>']\n",
      "\n",
      "7m 15s (- 69m 58s) (7050 9%) 4.5803\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "7m 17s (- 69m 48s) (7100 9%) 3.9459\n",
      "['', 'i', 'think', 'it', 't', 'think', 'that', 'you', 'think', 'it', 'was', 'the', 'best', 'of', 'the', '<EOS>']\n",
      "\n",
      "7m 20s (- 69m 40s) (7150 9%) 4.1790\n",
      "['', 'i', 'i', 'that', 'i', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "7m 24s (- 69m 41s) (7200 9%) 4.2697\n",
      "['', 'i', 'guess', 'that', 'is', 'a', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "7m 27s (- 69m 43s) (7250 9%) 4.2335\n",
      "['', 'i', 'agree', 'i', 'would', 'have', 'a', 'great', 'i', 'and', 'the', 'a', 'of', '<EOS>']\n",
      "\n",
      "7m 31s (- 69m 50s) (7300 9%) 4.2490\n",
      "['', 'i', 'i', 'know', 'are', 'i', 'the', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "7m 35s (- 69m 52s) (7350 9%) 4.3501\n",
      "['', 'i', 'wonder', 'what', 'i', 'i', 'the', 'that', 'of', 'i', 'the', '', 'i', 'the', '<EOS>']\n",
      "\n",
      "7m 39s (- 69m 54s) (7400 9%) 4.6761\n",
      "['', 'yes', 'i', 'i', 'i', 'the', 'i', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "7m 42s (- 69m 56s) (7450 9%) 4.6576\n",
      "['', 'i', 'is', 'a', 'is', 'the', 'is', 'a', 'i', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "7m 45s (- 69m 52s) (7500 10%) 4.0289\n",
      "['', 'i', 'think', 'it', 's', 'a', 'to', 'i', '', '<EOS>']\n",
      "\n",
      "7m 48s (- 69m 48s) (7550 10%) 4.0149\n",
      "['', 'i', 'i', 'that', 'that', 'a', 'good', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "7m 51s (- 69m 42s) (7600 10%) 4.2760\n",
      "['', 'i', 'didn', 't', 'know', 'that', 'i', 'the', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "7m 54s (- 69m 39s) (7650 10%) 4.4584\n",
      "['', 'i', 'think', 'it', 'is', 'a', 'i', 'it', '', '<EOS>']\n",
      "\n",
      "7m 58s (- 69m 38s) (7700 10%) 4.2993\n",
      "['', 'i', 'think', 'is', 't', 'it', 'the', 'the', 'is', 'of', 'the', '', 'it', 'the', 'is', '', '<EOS>']\n",
      "\n",
      "8m 1s (- 69m 36s) (7750 10%) 4.1994\n",
      "['', 'i', 'i', 'you', 'a', 'i', 'the', 'you', '', 'the', '', '', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8m 4s (- 69m 34s) (7800 10%) 4.4246\n",
      "['', 'i', 'know', 'the', 'the', 'the', 'it', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "8m 7s (- 69m 32s) (7850 10%) 4.8036\n",
      "['', 'i', 'guess', 'that', 'i', 'a', 'to', 'of', 'the', 'i', 'to', 'of', 'a', 'to', 'of', 'a', 'i', 'to', 'of', 'a', 'the', 'i', 'to', 'of', 'a', 'to', 'of', 'a', '<EOS>']\n",
      "\n",
      "8m 11s (- 69m 34s) (7900 10%) 4.6581\n",
      "['', 'that', 'is', 'the', 'a', 'think', 'of', 'of', 'the', 'a', 'that', 'of', 'a', 'it', 'of', 'the', 'a', '<EOS>']\n",
      "\n",
      "8m 14s (- 69m 30s) (7950 10%) 4.4075\n",
      "['', 'wow', 'i', 'that', 'is', 'a', 'in', 'of', '', '', '<EOS>']\n",
      "\n",
      "8m 17s (- 69m 30s) (8000 10%) 4.5637\n",
      "['', 'that', 'is', 'the', 'the', 'the', 'the', 'the', 'to', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "8m 21s (- 69m 27s) (8050 10%) 4.3680\n",
      "['', 'i', 'know', 'that', 'i', 'of', 'the', '<EOS>']\n",
      "\n",
      "8m 24s (- 69m 25s) (8100 10%) 4.3312\n",
      "['', 'yeah', 'i', 'think', 'the', 'is', 'that', 'the', 'of', 'the', 'the', 'you', '', 'the', '<EOS>']\n",
      "\n",
      "8m 27s (- 69m 19s) (8150 10%) 4.3208\n",
      "['', 'i', 'that', 'that', 'the', 'that', 'to', 'that', '', 'the', '<EOS>']\n",
      "\n",
      "8m 30s (- 69m 17s) (8200 10%) 4.2318\n",
      "['', 'that', 'is', 'that', 'that', 'that', 'i', 'would', 'be', 'that', 'that', '', 'i', 'would', 'be', 'that', '', '<EOS>']\n",
      "\n",
      "8m 33s (- 69m 12s) (8250 11%) 4.0455\n",
      "['', 'i', 'is', 'that', 'the', 'know', 'that', 'i', 'the', 'that', 'the', '<EOS>']\n",
      "\n",
      "8m 36s (- 69m 13s) (8300 11%) 4.5884\n",
      "['', 'i', 'i', 'that', 'to', 'the', 'to', 'i', 'the', 'to', 'to', 'the', 'to', 'the', 'to', 'i', '<EOS>']\n",
      "\n",
      "8m 40s (- 69m 13s) (8350 11%) 4.6095\n",
      "['', 'i', 'wonder', 'how', 'they', 'the', 'the', 'to', 'the', 'the', 'to', 'the', 'to', 'the', 'the', 'to', 'the', '', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "8m 43s (- 69m 12s) (8400 11%) 4.4249\n",
      "['', 'that', 'is', 'that', 'the', 'that', '', 'that', '', '<EOS>']\n",
      "\n",
      "8m 47s (- 69m 11s) (8450 11%) 4.4948\n",
      "['', 'i', 'i', 'you', 'a', 'to', 'of', 'a', '', '', 'to', '<EOS>']\n",
      "\n",
      "8m 50s (- 69m 7s) (8500 11%) 4.3318\n",
      "['', 'i', 's', 'that', 'a', 'that', 'that', 'of', 'a', 'that', '<EOS>']\n",
      "\n",
      "8m 53s (- 69m 3s) (8550 11%) 3.9475\n",
      "['', 'yeah', 'i', 'a', 'a', 'know', 'that', 'a', '<EOS>']\n",
      "\n",
      "8m 56s (- 69m 0s) (8600 11%) 3.9276\n",
      "['', 'i', 's', 'i', 'to', 'of', 'a', '<EOS>']\n",
      "\n",
      "8m 59s (- 68m 56s) (8650 11%) 4.0033\n",
      "['', 'yes', 'that', 'is', 'that', 'are', 'you', 'a', 'good', '', '<EOS>']\n",
      "\n",
      "9m 2s (- 68m 53s) (8700 11%) 4.2488\n",
      "['', 'i', 'i', 'the', 'the', 'a', 'i', 'the', '<EOS>']\n",
      "\n",
      "9m 5s (- 68m 49s) (8750 11%) 3.6865\n",
      "['', 'i', 'is', 'it', 'i', 'to', 'a', 'the', '<EOS>']\n",
      "\n",
      "9m 8s (- 68m 48s) (8800 11%) 4.3692\n",
      "['', 'i', 'i', 't', 'i', 'that', 'i', 'i', '<EOS>']\n",
      "\n",
      "9m 12s (- 68m 46s) (8850 11%) 4.5500\n",
      "['', 'i', 'you', 't', 'know', 'that', 'i', 'you', 'the', 'of', 'the', 'that', 'of', 'the', 'you', '', 'you', 'the', 'of', 'the', '<EOS>']\n",
      "\n",
      "9m 15s (- 68m 44s) (8900 11%) 4.3660\n",
      "['', 'yeah', 'that', 'is', 'i', 'the', 'the', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "9m 18s (- 68m 41s) (8950 11%) 4.1936\n",
      "['', 'yeah', 'it', 'is', 'a', 'the', 'the', 'the', 'that', 'a', 'the', 'the', '<EOS>']\n",
      "\n",
      "9m 21s (- 68m 40s) (9000 12%) 4.3120\n",
      "['', 'i', 'i', 'that', 'i', 'i', 'the', 'that', 'i', 'the', 'i', 'the', 'that', 'i', 'the', 'that', 'i', 'the', 'i', 'of', 'the', 'that', '', 'i', 'the', 'that', 'i', 'i', 'the', 'that', '', '<EOS>']\n",
      "\n",
      "9m 25s (- 68m 39s) (9050 12%) 4.5731\n",
      "['', 'that', 'is', 'is', 'a', 'the', 'a', 'i', 'the', 'the', '<EOS>']\n",
      "\n",
      "9m 28s (- 68m 37s) (9100 12%) 4.0157\n",
      "['', 'i', 'is', 'is', 'not', 'to', 'a', 'the', 'the', 'i', 'the', 'the', 'that', '<EOS>']\n",
      "\n",
      "9m 31s (- 68m 33s) (9150 12%) 4.1730\n",
      "['', 'i', 'you', 'a', 'that', 'it', 'to', 'it', 'it', 'the', 'to', 'it', '<EOS>']\n",
      "\n",
      "9m 35s (- 68m 34s) (9200 12%) 4.5006\n",
      "['', 'that', 'is', 'the', 'that', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "9m 38s (- 68m 31s) (9250 12%) 4.3687\n",
      "['', 'that', 'is', 't', 'too', 'much', 'about', 'you', '', 'that', '<EOS>']\n",
      "\n",
      "9m 41s (- 68m 27s) (9300 12%) 4.2533\n",
      "['', 'that', 'is', 'a', 'good', 'i', 'i', 'is', 'to', 'a', '', 'a', '<EOS>']\n",
      "\n",
      "9m 44s (- 68m 25s) (9350 12%) 4.0600\n",
      "['', 'i', 'agree', 'they', 'are', 'to', 'you', 'that', 'to', 'that', '', 'to', '', '<EOS>']\n",
      "\n",
      "9m 48s (- 68m 24s) (9400 12%) 4.1349\n",
      "['', 'that', 's', 'a', 'that', 'the', 'that', 'of', 'the', 'that', '<EOS>']\n",
      "\n",
      "9m 51s (- 68m 23s) (9450 12%) 4.1770\n",
      "['', 'i', 'i', 'a', 'that', 'i', 'a', 'that', 'i', 'the', 'of', 'that', 'a', 'i', 'the', 'that', 'of', 'in', '', 'the', '<EOS>']\n",
      "\n",
      "9m 55s (- 68m 23s) (9500 12%) 4.0853\n",
      "['', 'i', 'i', 'a', 'a', 'i', 'a', 'that', 'i', 'the', '<EOS>']\n",
      "\n",
      "9m 58s (- 68m 21s) (9550 12%) 4.5627\n",
      "['', 'yeah', 'that', 'is', 'a', 'that', 'that', '<EOS>']\n",
      "\n",
      "10m 1s (- 68m 20s) (9600 12%) 4.0073\n",
      "['', 'i', 'know', 'that', 'is', 'you', 'a', '<EOS>']\n",
      "\n",
      "10m 5s (- 68m 19s) (9650 12%) 3.7074\n",
      "['', 'that', 'is', 'that', 'you', '', '<EOS>']\n",
      "\n",
      "10m 8s (- 68m 14s) (9700 12%) 3.7770\n",
      "['', 'it', 'is', 'you', 'a', 'a', 'to', '<EOS>']\n",
      "\n",
      "10m 10s (- 68m 8s) (9750 13%) 4.0468\n",
      "['', 'i', 'is', 'it', 'is', 'know', 'it', '', '<EOS>']\n",
      "\n",
      "10m 14s (- 68m 5s) (9800 13%) 4.3302\n",
      "['', 'i', 'you', 'is', 'a', 'that', 'you', 'a', 'and', 'of', 'the', '', 'you', 'the', '<EOS>']\n",
      "\n",
      "10m 17s (- 68m 1s) (9850 13%) 4.2775\n",
      "['', 'yeah', 'i', 'is', 'is', 'not', 'that', 'i', 'a', 'i', 'to', 'of', 'is', 'a', '', '<EOS>']\n",
      "\n",
      "10m 20s (- 67m 58s) (9900 13%) 4.0843\n",
      "['', 'i', 'i', 'to', 'that', 'the', 'that', 'the', 'the', 'that', '', '<EOS>']\n",
      "\n",
      "10m 23s (- 67m 56s) (9950 13%) 4.1935\n",
      "['', 'i', 'that', 'i', 'to', 'the', 'that', 'the', 'that', 'the', 'that', '', 'the', 'that', '<EOS>']\n",
      "\n",
      "10m 26s (- 67m 54s) (10000 13%) 4.5515\n",
      "['', 'i', 'you', 'a', 'i', 'a', 'a', 'i', 'a', 'i', '<EOS>']\n",
      "\n",
      "10m 30s (- 67m 52s) (10050 13%) 3.9240\n",
      "['', 'i', 'is', 'that', 'i', 'of', 'the', 'is', '', 'the', '', '', '<EOS>']\n",
      "\n",
      "10m 33s (- 67m 50s) (10100 13%) 4.0450\n",
      "['', 'i', 'you', 'know', 'that', 'the', 'the', 'a', 'the', 'a', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "10m 36s (- 67m 47s) (10150 13%) 4.0901\n",
      "['', 'i', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'a', 'that', '', '<EOS>']\n",
      "\n",
      "10m 40s (- 67m 49s) (10200 13%) 4.7506\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'of', 'i', 'to', 'of', 'i', '', 'the', '', '<EOS>']\n",
      "\n",
      "10m 43s (- 67m 42s) (10250 13%) 3.6718\n",
      "['', 'that', 'is', 'a', 'i', 'i', 'is', 'a', 'the', '<EOS>']\n",
      "\n",
      "10m 46s (- 67m 40s) (10300 13%) 4.1989\n",
      "['', 'i', 'would', 'that', 'of', 'i', 'a', 'of', 'a', 'and', 'of', 'the', '<EOS>']\n",
      "\n",
      "10m 49s (- 67m 38s) (10350 13%) 4.1254\n",
      "['', 'i', 'that', 'is', 'that', 'i', 'of', 'a', 'that', 'of', 'the', 'a', 'you', 'of', 'the', 'a', '<EOS>']\n",
      "\n",
      "10m 53s (- 67m 36s) (10400 13%) 3.9993\n",
      "['', 'i', 'that', 's', 'to', 'the', 'the', 'to', 'the', 'the', '<EOS>']\n",
      "\n",
      "10m 56s (- 67m 32s) (10450 13%) 4.3115\n",
      "['', 'that', 'is', 'interesting', 'that', 'you', 'of', 'that', 'that', '<EOS>']\n",
      "\n",
      "10m 59s (- 67m 30s) (10500 14%) 4.5998\n",
      "['', 'i', 'would', 'be', 'a', 'of', 'to', 'of', 'that', 'the', 'to', 'the', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "11m 2s (- 67m 29s) (10550 14%) 4.2693\n",
      "['', 'i', 'i', 'a', 'of', 'a', 'i', 'i', 'a', 'of', 'the', 'a', 'i', '', '<EOS>']\n",
      "\n",
      "11m 6s (- 67m 29s) (10600 14%) 4.5891\n",
      "['', 'i', 'is', 'a', 'a', 'a', 'i', 'a', 'i', 'a', 'a', 'i', 'a', 'i', 'a', '', '<EOS>']\n",
      "\n",
      "11m 9s (- 67m 25s) (10650 14%) 4.4450\n",
      "['', 'i', 'i', 'that', 'the', 'the', 'the', 't', 'a', 'the', 'i', '<EOS>']\n",
      "\n",
      "11m 12s (- 67m 22s) (10700 14%) 4.4335\n",
      "['', 'i', 'is', 'i', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "11m 15s (- 67m 17s) (10750 14%) 3.5439\n",
      "['', 'i', 'is', 'that', 'a', 'that', 'a', 'that', 'a', '<EOS>']\n",
      "\n",
      "11m 18s (- 67m 15s) (10800 14%) 3.9973\n",
      "['', 'i', 'is', 'i', 'is', 'a', 'i', 'to', 'i', 'to', 'i', 'to', 'that', 'to', '<EOS>']\n",
      "\n",
      "11m 21s (- 67m 12s) (10850 14%) 4.2747\n",
      "['', 'i', 'is', 'i', 'i', 'i', 'that', 'to', 'i', 'the', '<EOS>']\n",
      "\n",
      "11m 24s (- 67m 7s) (10900 14%) 3.9933\n",
      "['', 'that', 'is', 'a', 'the', 'a', 'i', 'that', 'the', '', 'the', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "11m 28s (- 67m 4s) (10950 14%) 4.1340\n",
      "['', 'that', 'is', 'a', 'i', 'the', 'the', 'a', 'i', 'the', '<EOS>']\n",
      "\n",
      "11m 31s (- 67m 1s) (11000 14%) 4.1105\n",
      "['', 'yes', 'i', 'have', 'to', 'be', 'a', 'lot', 'of', 'the', 'and', 'i', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "11m 34s (- 67m 0s) (11050 14%) 4.0823\n",
      "['', 'i', 'i', 'i', 'the', 'i', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "11m 37s (- 66m 57s) (11100 14%) 4.5023\n",
      "['', 'yeah', 'i', 's', 'i', 'a', 'you', '', '<EOS>']\n",
      "\n",
      "11m 41s (- 66m 55s) (11150 14%) 3.9598\n",
      "['', 'i', 'i', 'the', 'i', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "11m 44s (- 66m 54s) (11200 14%) 4.4400\n",
      "['', 'i', 'i', 'too', 'i', 'that', 'i', 'they', 'the', 'that', 'the', 'to', 'the', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11m 48s (- 66m 52s) (11250 15%) 3.9096\n",
      "['', 'that', 'is', 'a', 'it', 'i', 'the', 'it', 'the', 'a', 'the', '<EOS>']\n",
      "\n",
      "11m 51s (- 66m 48s) (11300 15%) 4.2237\n",
      "['', 'that', 'is', 'interesting', 'that', 'would', 'be', 'a', 'to', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "11m 54s (- 66m 47s) (11350 15%) 4.7596\n",
      "['', 'i', 'i', 'that', 'to', 'the', 'to', 'the', 'the', 'to', 'the', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "11m 57s (- 66m 45s) (11400 15%) 4.6950\n",
      "['', 'i', 'would', 'be', 'in', 'the', '', 'of', 'the', '<EOS>']\n",
      "\n",
      "12m 1s (- 66m 42s) (11450 15%) 4.1074\n",
      "['', 'i', 'i', 't', 'that', 'i', 'to', '<EOS>']\n",
      "\n",
      "12m 4s (- 66m 43s) (11500 15%) 4.0995\n",
      "['', 'that', 'is', 'true', 'i', 'i', 'to', 'have', '', 'to', '<EOS>']\n",
      "\n",
      "12m 8s (- 66m 40s) (11550 15%) 3.9885\n",
      "['', 'i', 'is', 'sure', 'i', 'would', 'have', 'to', 'the', 'a', 'the', 'as', 'as', 'as', 'well', 'as', 'well', 'as', 'well', '', '<EOS>']\n",
      "\n",
      "12m 11s (- 66m 37s) (11600 15%) 4.3676\n",
      "['', 'yeah', 'i', 'have', 'a', 't', 'a', 'that', 'you', 'to', 'have', 'a', '<EOS>']\n",
      "\n",
      "12m 14s (- 66m 31s) (11650 15%) 3.9548\n",
      "['', 'i', 'i', 'that', 'the', 'i', 'a', 'that', 'the', 'to', '<EOS>']\n",
      "\n",
      "12m 17s (- 66m 28s) (11700 15%) 4.0072\n",
      "['', 'that', 'is', 't', 'a', 'that', 'i', 'the', 'a', 'that', 'of', 'the', '<EOS>']\n",
      "\n",
      "12m 20s (- 66m 24s) (11750 15%) 4.3250\n",
      "['', 'yes', 'that', 's', 'a', 'good', 'idea', 'i', 'was', 'a', 'good', 'idea', 'that', 's', 'a', 'good', 'day', '', '<EOS>']\n",
      "\n",
      "12m 23s (- 66m 20s) (11800 15%) 3.8994\n",
      "['', 'i', 'i', 'i', 'you', 'you', 'to', 'the', 'of', '<EOS>']\n",
      "\n",
      "12m 26s (- 66m 17s) (11850 15%) 4.3199\n",
      "['', 'yes', 'i', 'agree', 'i', 'is', 'but', 'i', 'do', 'you', 'like', '', '<EOS>']\n",
      "\n",
      "12m 29s (- 66m 14s) (11900 15%) 4.0121\n",
      "['', 'yes', 'i', 'agree', 'i', 'like', 'a', 'i', 'a', 'of', 'the', 'a', '<EOS>']\n",
      "\n",
      "12m 32s (- 66m 12s) (11950 15%) 4.1133\n",
      "['', 'yeah', 'that', 'is', 'the', 'a', 'that', '<EOS>']\n",
      "\n",
      "12m 35s (- 66m 8s) (12000 16%) 3.6452\n",
      "['', 'i', 'i', 'you', 'you', 'a', 'to', 'the', 'a', 'that', '<EOS>']\n",
      "\n",
      "12m 38s (- 66m 4s) (12050 16%) 4.2403\n",
      "['', 'i', 'i', 'that', 'i', 'i', 'that', 'i', 'to', 'the', '<EOS>']\n",
      "\n",
      "12m 41s (- 66m 0s) (12100 16%) 4.1756\n",
      "['', 'i', 'is', 'that', 'that', 'a', 'that', 'i', '', '<EOS>']\n",
      "\n",
      "12m 45s (- 65m 57s) (12150 16%) 4.2971\n",
      "['', 'that', 's', 'a', 'that', 'that', 'i', 'you', 'to', 'the', 'a', 'that', 'that', '<EOS>']\n",
      "\n",
      "12m 48s (- 65m 53s) (12200 16%) 4.0524\n",
      "['', 'i', 'would', 'have', 'to', 'you', 'a', 'that', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "12m 51s (- 65m 50s) (12250 16%) 4.5261\n",
      "['', 'i', 'is', 'a', 'i', 'to', 'the', 'a', '<EOS>']\n",
      "\n",
      "12m 54s (- 65m 47s) (12300 16%) 4.1716\n",
      "['', 'i', 'that', 'a', 'i', 'a', 'to', 'to', 'the', 'a', '', 'to', '<EOS>']\n",
      "\n",
      "12m 57s (- 65m 44s) (12350 16%) 3.9892\n",
      "['', 'i', 'agree', 'it', 't', 'you', 'to', 'have', 'a', 'the', 'to', 'the', 'a', 'the', 'the', '<EOS>']\n",
      "\n",
      "13m 0s (- 65m 39s) (12400 16%) 4.3035\n",
      "['', 'i', 'i', 'you', 'that', 'the', 'you', 'the', 'of', '', 'of', 'the', '<EOS>']\n",
      "\n",
      "13m 3s (- 65m 37s) (12450 16%) 4.0912\n",
      "['', 'it', 'is', 'very', 'interesting', '', '<EOS>']\n",
      "\n",
      "13m 6s (- 65m 33s) (12500 16%) 3.9132\n",
      "['', 'i', 'i', 'that', 'a', 'that', 'to', 'that', 'to', 'that', '', 'to', 'that', '<EOS>']\n",
      "\n",
      "13m 9s (- 65m 30s) (12550 16%) 4.4104\n",
      "['', 'i', 'i', 't', 'a', 'the', 'i', 'i', 'a', 'the', 'i', '', 'the', '<EOS>']\n",
      "\n",
      "13m 13s (- 65m 27s) (12600 16%) 4.1356\n",
      "['', 'yeah', 'it', 's', 'a', 'the', 'is', '', 'the', '<EOS>']\n",
      "\n",
      "13m 15s (- 65m 22s) (12650 16%) 4.1415\n",
      "['', 'i', 'i', 'i', 'i', 'you', 'a', 'the', '', 'to', 'the', '<EOS>']\n",
      "\n",
      "13m 18s (- 65m 18s) (12700 16%) 3.7269\n",
      "['', 'yeah', 'i', 'know', 'that', 'the', 'the', 'the', 'of', 'the', 'of', 'that', 'the', '', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "13m 21s (- 65m 15s) (12750 17%) 4.4307\n",
      "['', 'i', 'i', 'that', 'that', 'the', 'of', 'to', 'the', 'to', '', 'the', '<EOS>']\n",
      "\n",
      "13m 25s (- 65m 12s) (12800 17%) 3.9436\n",
      "['', 'i', 'i', 'you', 'of', 'to', 'of', '', '<EOS>']\n",
      "\n",
      "13m 28s (- 65m 8s) (12850 17%) 4.2181\n",
      "['', 'i', 'i', 'have', 'to', 'you', '', 'the', '', '', '<EOS>']\n",
      "\n",
      "13m 31s (- 65m 4s) (12900 17%) 3.9492\n",
      "['', 'i', 'would', 'have', 'not', 'have', 'but', 'i', 'you', 'it', '', '<EOS>']\n",
      "\n",
      "13m 33s (- 65m 0s) (12950 17%) 4.1449\n",
      "['', 'i', 'i', 'is', 'i', 'of', 'the', 'is', 'that', 'the', 'of', 'the', '<EOS>']\n",
      "\n",
      "13m 37s (- 64m 58s) (13000 17%) 4.1616\n",
      "['', 'i', 'i', 'i', 'is', 'to', 'i', 'a', '<EOS>']\n",
      "\n",
      "13m 40s (- 64m 56s) (13050 17%) 3.9515\n",
      "['', 'i', 'i', 'i', 'i', 'a', 'you', 'you', 'the', 'i', 'a', '<EOS>']\n",
      "\n",
      "13m 43s (- 64m 53s) (13100 17%) 3.6420\n",
      "['', 'i', 'i', 'you', 'you', 'of', 'of', 'to', 'of', 'i', 'all', 'of', 'the', 'of', 'all', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "13m 47s (- 64m 52s) (13150 17%) 4.5230\n",
      "['', 'i', 'is', 'i', 'i', 'i', 'to', 'that', '<EOS>']\n",
      "\n",
      "13m 50s (- 64m 49s) (13200 17%) 4.0269\n",
      "['', 'i', 'i', 'i', 'i', 'the', 'i', 'the', 'of', 'the', '<EOS>']\n",
      "\n",
      "13m 54s (- 64m 48s) (13250 17%) 4.3462\n",
      "['', 'i', 'i', 'i', 'not', 'i', 'not', 'i', 'it', 'to', '<EOS>']\n",
      "\n",
      "13m 57s (- 64m 43s) (13300 17%) 4.0384\n",
      "['', 'i', 'have', 'not', 'a', 'i', 'to', 'the', 'the', 'have', 'to', 'of', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "14m 0s (- 64m 41s) (13350 17%) 4.1354\n",
      "['', 'i', 'is', 'a', 'the', 'the', 'the', '', 'i', '<EOS>']\n",
      "\n",
      "14m 3s (- 64m 37s) (13400 17%) 3.8615\n",
      "['', 'i', 'i', 'a', 'a', 'that', 'to', 'the', 'a', 'that', 'to', 'the', '<EOS>']\n",
      "\n",
      "14m 6s (- 64m 33s) (13450 17%) 4.1413\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'to', 'the', 'i', 'i', '', 'to', 'that', '<EOS>']\n",
      "\n",
      "14m 9s (- 64m 30s) (13500 18%) 4.2249\n",
      "['', 'i', 'i', 'have', 'to', 'a', 'you', '', '', '<EOS>']\n",
      "\n",
      "14m 13s (- 64m 28s) (13550 18%) 4.4889\n",
      "['', 'i', 'i', 'i', 'i', 'that', 'i', '', '<EOS>']\n",
      "\n",
      "14m 16s (- 64m 25s) (13600 18%) 3.8197\n",
      "['', 'i', 'have', 'a', 'great', 'of', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "14m 18s (- 64m 20s) (13650 18%) 3.8250\n",
      "['', 'i', 'is', 'that', 'is', 'you', 'a', 'you', 'you', '', '<EOS>']\n",
      "\n",
      "14m 22s (- 64m 17s) (13700 18%) 3.8245\n",
      "['', 'i', 'i', 'i', 'i', 'that', 'i', 'that', 'i', '', 'the', '<EOS>']\n",
      "\n",
      "14m 25s (- 64m 14s) (13750 18%) 4.3803\n",
      "['', 'i', 'agree', 'i', 'i', 'to', 'the', 'the', 'you', '', '', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "14m 28s (- 64m 11s) (13800 18%) 4.1289\n",
      "['', 'yeah', 'i', 'think', 'it', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "14m 31s (- 64m 8s) (13850 18%) 4.4409\n",
      "['', 'i', 'i', 'a', 'a', 'to', 'that', 'a', 'i', 'to', 'a', 'the', 'a', '', '<EOS>']\n",
      "\n",
      "14m 35s (- 64m 6s) (13900 18%) 4.4674\n",
      "['', 'i', 'would', 'be', 'a', 'a', 'i', 'to', 'that', '', 'the', '<EOS>']\n",
      "\n",
      "14m 38s (- 64m 4s) (13950 18%) 4.3299\n",
      "['', 'i', 'would', 'have', 'to', 'the', 'you', '', 'to', '', '<EOS>']\n",
      "\n",
      "14m 41s (- 64m 1s) (14000 18%) 4.1615\n",
      "['', 'i', 'i', 'i', 'that', 'that', 'i', 'that', 'the', 'that', 'of', 'that', 'the', 'that', '', 'i', 'to', 'that', 'the', '', '<EOS>']\n",
      "\n",
      "14m 44s (- 63m 58s) (14050 18%) 4.4756\n",
      "['', 'i', 'i', 'i', 'the', 'of', 'the', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "14m 48s (- 63m 55s) (14100 18%) 4.0991\n",
      "['', 'i', 'i', 'the', 'the', 'a', 'the', 'you', 'the', 'you', 'the', 'to', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "14m 51s (- 63m 51s) (14150 18%) 4.2734\n",
      "['', 'i', 'i', 'i', 'the', 'the', 'the', '', '', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "14m 54s (- 63m 48s) (14200 18%) 3.8996\n",
      "['', 'yeah', 'i', 'm', 'not', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '', 'i', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "14m 57s (- 63m 45s) (14250 19%) 4.4327\n",
      "['', 'i', 'i', 'a', 'of', 'to', 'of', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "15m 0s (- 63m 43s) (14300 19%) 4.3484\n",
      "['', 'i', 'i', 'a', 'a', 'of', 'the', 'a', 'the', 'the', 'of', 'a', 'the', 'the', '<EOS>']\n",
      "\n",
      "15m 4s (- 63m 42s) (14350 19%) 4.3391\n",
      "['', 'i', 'you', 'i', 'to', 'to', 'to', 'to', 'it', 'to', 'the', 'to', 'the', 'it', 'to', 'the', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "15m 7s (- 63m 38s) (14400 19%) 4.0424\n",
      "['', 'i', 'is', 'i', 'the', 'the', 'to', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "15m 10s (- 63m 35s) (14450 19%) 4.1953\n",
      "['', 'i', 'm', 'not', 'sure', 'to', 'the', 'you', 'to', 'the', 'to', 'the', 'the', 'to', '<EOS>']\n",
      "\n",
      "15m 14s (- 63m 34s) (14500 19%) 4.6243\n",
      "['', 'i', 'i', 'i', 'i', 'to', 'that', 'the', 'i', '<EOS>']\n",
      "\n",
      "15m 17s (- 63m 31s) (14550 19%) 4.5137\n",
      "['', 'yes', 'the', 'same', 'of', 'the', 'time', '<EOS>']\n",
      "\n",
      "15m 20s (- 63m 26s) (14600 19%) 4.0281\n",
      "['', 'i', 'i', 'i', 'i', 'a', 'i', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "15m 23s (- 63m 22s) (14650 19%) 4.3429\n",
      "['', 'i', 's', 'not', 'to', 'but', 'i', 'to', 'to', 'but', 'the', 'to', 'but', 'i', 'the', 'to', '', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15m 26s (- 63m 18s) (14700 19%) 4.1302\n",
      "['', 'i', 'have', 'that', 'i', 'a', 'you', 'i', 'to', '<EOS>']\n",
      "\n",
      "15m 29s (- 63m 15s) (14750 19%) 3.9756\n",
      "['', 'i', 'like', 'to', 'you', 'i', '<EOS>']\n",
      "\n",
      "15m 31s (- 63m 10s) (14800 19%) 3.7081\n",
      "['', 'yeah', 'i', 'am', 'a', 'fan', 'of', 'i', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "15m 34s (- 63m 6s) (14850 19%) 4.0722\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'was', 'you', 'the', 'you', 'i', '', '<EOS>']\n",
      "\n",
      "15m 38s (- 63m 4s) (14900 19%) 4.2772\n",
      "['', 'i', 'i', 'is', 'i', 'is', 'that', 'i', 'the', 'the', 'of', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "15m 41s (- 63m 1s) (14950 19%) 4.6104\n",
      "['', 'i', 'i', 'i', 'i', 'of', 'it', 'of', 'it', 'to', 'of', 'i', 'i', 'the', 'i', '', 'the', 'i', '', '<EOS>']\n",
      "\n",
      "15m 44s (- 62m 58s) (15000 20%) 4.3715\n",
      "['', 'yeah', 'i', 'is', 'is', 'that', 'to', 'of', 'the', 'you', 'that', 'i', 'to', 'the', 'the', '<EOS>']\n",
      "\n",
      "15m 47s (- 62m 54s) (15050 20%) 4.1365\n",
      "['', 'i', 'i', 'you', 'the', 'the', 'you', '', '<EOS>']\n",
      "\n",
      "15m 51s (- 62m 52s) (15100 20%) 4.3506\n",
      "['', 'yeah', 'i', 'agree', 'i', 'am', 'not', 'a', 'fan', 'of', 'the', 'but', 'do', 'you', 'know', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "15m 54s (- 62m 51s) (15150 20%) 4.6811\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'the', 'the', '<EOS>']\n",
      "\n",
      "15m 57s (- 62m 48s) (15200 20%) 4.1057\n",
      "['', 'i', 'you', 'to', 'the', 'to', 'of', 'the', 'the', 'to', 'the', '', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "16m 0s (- 62m 44s) (15250 20%) 4.1397\n",
      "['', 'i', 'is', 'a', 'a', 'you', '', 'a', '', '<EOS>']\n",
      "\n",
      "16m 4s (- 62m 43s) (15300 20%) 4.1511\n",
      "['', 'i', 'agree', 'i', 'have', 'a', 'you', 'the', 'the', 'a', 'the', 'the', 'the', 'a', 'the', 'the', '', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "16m 7s (- 62m 39s) (15350 20%) 4.4485\n",
      "['', 'i', 'i', 'a', 'i', 'a', 'the', 'i', 'a', 'the', 'i', 'the', 'a', 'the', 'i', 'the', 'a', 'the', 'i', 'the', 'a', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "16m 11s (- 62m 39s) (15400 20%) 4.4356\n",
      "['', 'i', 's', 'a', 'the', 'the', 'a', 'the', 'that', 'the', 'the', 'a', 'the', 'that', 'the', 'the', '<EOS>']\n",
      "\n",
      "16m 14s (- 62m 35s) (15450 20%) 3.9494\n",
      "['', 'i', 'you', 'i', 'i', 'the', 'a', 'the', 'you', 'the', 'a', 'the', 'a', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "16m 17s (- 62m 32s) (15500 20%) 4.4059\n",
      "['', 'i', 'i', 'have', 'a', 'the', 'a', 'you', 'a', 'to', 'the', 'a', 'a', 'the', 'a', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "16m 20s (- 62m 29s) (15550 20%) 4.4877\n",
      "['', 'it', 'is', 'a', 'you', 'a', 'it', 'to', 'the', 'a', '', '<EOS>']\n",
      "\n",
      "16m 24s (- 62m 27s) (15600 20%) 4.5803\n",
      "['', 'i', 'did', 'not', 'know', 'that', 'the', 'sun', 'is', 'the', 'only', 'one', 'of', 'the', 'time', 'i', 'did', 'not', 'know', 'that', 'the', 'sun', '', '<EOS>']\n",
      "\n",
      "16m 27s (- 62m 23s) (15650 20%) 4.1077\n",
      "['', 'i', 'do', 'not', 'i', 'a', 'the', 'you', 'to', '', '<EOS>']\n",
      "\n",
      "16m 30s (- 62m 19s) (15700 20%) 3.9167\n",
      "['', 'i', 'i', 't', 'it', 'to', 'of', 'it', 'i', 'the', 'to', 'of', 'the', '', '<EOS>']\n",
      "\n",
      "16m 33s (- 62m 16s) (15750 21%) 4.5165\n",
      "['', 'i', 'i', 'i', 'a', 'a', 'i', 'a', 'i', 'to', '<EOS>']\n",
      "\n",
      "16m 36s (- 62m 13s) (15800 21%) 4.1593\n",
      "['', 'i', 'is', 'it', 'a', 'it', 'you', '', '<EOS>']\n",
      "\n",
      "16m 39s (- 62m 9s) (15850 21%) 4.3043\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'of', 'i', 'the', 'i', '', 'the', '', '<EOS>']\n",
      "\n",
      "16m 42s (- 62m 6s) (15900 21%) 4.3515\n",
      "['', 'i', 'do', 'i', 'is', 'a', 'to', 'i', 'a', 'to', '<EOS>']\n",
      "\n",
      "16m 45s (- 62m 2s) (15950 21%) 4.0198\n",
      "['', 'yes', 'i', 'is', 'the', 'i', 'in', 'the', '', '<EOS>']\n",
      "\n",
      "16m 48s (- 61m 59s) (16000 21%) 4.1635\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'the', 'i', '', 'i', '<EOS>']\n",
      "\n",
      "16m 51s (- 61m 56s) (16050 21%) 4.2230\n",
      "['', 'i', 'do', 'i', 'do', 'i', 'do', 'you', '', '<EOS>']\n",
      "\n",
      "16m 54s (- 61m 52s) (16100 21%) 3.9713\n",
      "['', 'i', 'i', 'do', 'i', 'know', 'the', 'the', 'you', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "16m 58s (- 61m 49s) (16150 21%) 4.0310\n",
      "['', 'i', 'i', 'is', 'i', 'the', 'that', 'i', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "17m 1s (- 61m 46s) (16200 21%) 4.5705\n",
      "['', 'i', 'i', 'is', 'the', 'i', 'the', 'i', 'the', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "17m 4s (- 61m 42s) (16250 21%) 4.2977\n",
      "['', 'i', 'i', 'is', 'that', 'there', 'is', 'a', 'lot', 'of', 'of', 'the', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "17m 7s (- 61m 39s) (16300 21%) 4.3991\n",
      "['', 'i', 'i', 'i', 'is', 'i', 'a', 'i', 'is', 'you', '', 'and', 'i', '', '<EOS>']\n",
      "\n",
      "17m 10s (- 61m 37s) (16350 21%) 4.5249\n",
      "['', 'i', 'i', 'the', 'you', 'the', 'to', 'the', 'the', 'to', 'the', 'the', 'a', '', '<EOS>']\n",
      "\n",
      "17m 14s (- 61m 35s) (16400 21%) 4.5216\n",
      "['', 'i', 'i', 'is', 'i', 'the', 'the', 'a', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "17m 17s (- 61m 31s) (16450 21%) 4.0605\n",
      "['', 'same', 'here', 'i', 'm', 'sure', 'i', 'do', 'know', 'that', 'the', '<EOS>']\n",
      "\n",
      "17m 20s (- 61m 29s) (16500 22%) 4.1235\n",
      "['', 'i', 'i', 't', 'i', 'the', 'a', 'the', 'that', 'the', '<EOS>']\n",
      "\n",
      "17m 23s (- 61m 26s) (16550 22%) 4.3140\n",
      "['', 'yeah', 'it', 'is', 'a', 'of', 'the', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "17m 26s (- 61m 22s) (16600 22%) 3.9998\n",
      "['', 'i', 'i', 'it', 'it', 'of', 'the', 'i', 'the', 'to', 'the', '<EOS>']\n",
      "\n",
      "17m 29s (- 61m 19s) (16650 22%) 4.0546\n",
      "['', 'i', 'is', 'i', 'i', 'the', 'i', 'the', 'the', '<EOS>']\n",
      "\n",
      "17m 32s (- 61m 15s) (16700 22%) 3.8973\n",
      "['', 'i', 'is', 'i', 'to', 'i', 'to', 'to', 'i', 'to', 'to', 'i', 'the', 'to', 'it', '', '<EOS>']\n",
      "\n",
      "17m 36s (- 61m 14s) (16750 22%) 4.2197\n",
      "['', 'i', 'm', 'not', 'sure', 'i', 'i', 'the', 'that', 'i', 'to', 'of', 'the', '<EOS>']\n",
      "\n",
      "17m 39s (- 61m 11s) (16800 22%) 4.3142\n",
      "['', 'i', 'i', 'i', 'to', 'i', 'it', 'of', 'to', '', 'i', '<EOS>']\n",
      "\n",
      "17m 43s (- 61m 9s) (16850 22%) 4.2790\n",
      "['', 'i', 'i', 'i', 'the', 'i', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "17m 47s (- 61m 8s) (16900 22%) 4.3289\n",
      "['', 'i', 'i', 'i', 'the', 'the', 'i', 'the', 'the', '<EOS>']\n",
      "\n",
      "17m 50s (- 61m 4s) (16950 22%) 4.0518\n",
      "['', 'i', 'i', 'i', 'the', 'you', 'the', 'the', 'the', 'to', '<EOS>']\n",
      "\n",
      "17m 53s (- 61m 1s) (17000 22%) 3.9907\n",
      "['', 'yeah', 'i', 'is', 'you', 'the', 'a', 'the', 'the', 'the', 'the', 'to', 'in', 'the', '<EOS>']\n",
      "\n",
      "17m 56s (- 60m 57s) (17050 22%) 4.5121\n",
      "['', 'yes', 'is', 'a', 'a', 'you', 'a', 'and', 'you', 'a', '<EOS>']\n",
      "\n",
      "17m 59s (- 60m 54s) (17100 22%) 3.9767\n",
      "['', 'i', 'i', 'i', 'i', 'the', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "18m 2s (- 60m 50s) (17150 22%) 4.2688\n",
      "['', 'yeah', 'i', 'is', 'i', 'of', 'a', 'the', 'the', 'it', 'the', '', '<EOS>']\n",
      "\n",
      "18m 5s (- 60m 47s) (17200 22%) 4.1691\n",
      "['', 'i', 'did', 'you', 'know', 'a', 'a', 'you', '<EOS>']\n",
      "\n",
      "18m 8s (- 60m 44s) (17250 23%) 4.0551\n",
      "['', 'i', 'i', 'is', 'have', 'to', 'the', 'the', 'to', 'of', 'the', '<EOS>']\n",
      "\n",
      "18m 12s (- 60m 42s) (17300 23%) 3.9905\n",
      "['', 'i', 'i', 'have', 't', 'a', 'the', 'and', 'it', 'to', 'of', 'and', 'the', 'i', '<EOS>']\n",
      "\n",
      "18m 15s (- 60m 39s) (17350 23%) 4.1288\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'the', 'it', 'the', 'i', 'the', 'i', 'the', 'i', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "18m 19s (- 60m 38s) (17400 23%) 4.3089\n",
      "['', 'i', 'i', 'i', 'that', 'a', 'i', 'the', 'that', 'the', '', 'the', '', 'the', '', 'i', 'the', '<EOS>']\n",
      "\n",
      "18m 22s (- 60m 37s) (17450 23%) 4.8301\n",
      "['', 'i', 'i', 'a', 'i', 'and', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "18m 25s (- 60m 33s) (17500 23%) 4.0355\n",
      "['', 'yeah', 'is', 'i', 'i', 'i', 'i', 'a', 'that', 'and', '<EOS>']\n",
      "\n",
      "18m 29s (- 60m 31s) (17550 23%) 4.2140\n",
      "['', 'i', 'is', 'i', 'i', 'i', 'to', 'that', 'i', 'to', 'that', 'to', 'that', 'i', 'to', '<EOS>']\n",
      "\n",
      "18m 32s (- 60m 28s) (17600 23%) 4.3284\n",
      "['', 'yes', 'i', 'like', 'the', 'of', 'the', '', '', '<EOS>']\n",
      "\n",
      "18m 35s (- 60m 24s) (17650 23%) 4.0341\n",
      "['', 'i', 'i', 'it', 'it', 'the', 'of', 'like', 'the', 'it', 'to', 'the', 'in', '', 'the', '<EOS>']\n",
      "\n",
      "18m 38s (- 60m 22s) (17700 23%) 4.6311\n",
      "['', 'i', 'is', 'i', 'i', 'you', 'a', 'the', '', 'is', '<EOS>']\n",
      "\n",
      "18m 41s (- 60m 18s) (17750 23%) 4.1852\n",
      "['', 'i', 'i', 'i', 'i', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "18m 45s (- 60m 17s) (17800 23%) 4.1835\n",
      "['', 'i', 'is', 'you', 'of', 'the', 'of', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "18m 48s (- 60m 13s) (17850 23%) 3.9303\n",
      "['', 'yeah', 'i', 'do', 'like', 'to', 'i', 'you', 'they', 'of', 'the', 'the', '<EOS>']\n",
      "\n",
      "18m 51s (- 60m 10s) (17900 23%) 4.5283\n",
      "['', 'i', 'have', 'not', 'have', 'to', 'but', 'i', 'it', 'that', 'it', 'of', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "18m 54s (- 60m 7s) (17950 23%) 4.1164\n",
      "['', 'i', 'i', 'i', 'you', 'i', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "18m 58s (- 60m 4s) (18000 24%) 4.1027\n",
      "['', 'i', 'i', 'i', 'of', 'the', 'a', '', 'and', 'the', '', '<EOS>']\n",
      "\n",
      "19m 1s (- 60m 1s) (18050 24%) 4.1940\n",
      "['', 'i', 'i', 'a', 'you', 'the', 'a', '', 'you', 'the', '', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19m 5s (- 60m 0s) (18100 24%) 4.2683\n",
      "['', 'i', 'i', 'i', 'and', 'the', 'of', 'the', '<EOS>']\n",
      "\n",
      "19m 8s (- 59m 56s) (18150 24%) 4.1540\n",
      "['', 'i', 'i', 'i', 'i', 'of', 'of', 'i', 'the', 'a', 'the', '<EOS>']\n",
      "\n",
      "19m 11s (- 59m 53s) (18200 24%) 3.8632\n",
      "['', 'i', 'i', 'a', 'i', 'a', 'i', 'the', 'a', 'a', 'the', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "19m 14s (- 59m 51s) (18250 24%) 4.6530\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'of', 'i', 'a', 'it', '<EOS>']\n",
      "\n",
      "19m 18s (- 59m 48s) (18300 24%) 4.5294\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', '', '<EOS>']\n",
      "\n",
      "19m 21s (- 59m 45s) (18350 24%) 4.0883\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'the', 'i', 'the', 'i', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "19m 24s (- 59m 42s) (18400 24%) 4.1209\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'of', 'a', 'the', 'i', '', '<EOS>']\n",
      "\n",
      "19m 27s (- 59m 39s) (18450 24%) 4.1917\n",
      "['', 'i', 'i', 'i', 'i', 'of', 'you', 'of', 'the', 'i', '', 'the', '<EOS>']\n",
      "\n",
      "19m 31s (- 59m 36s) (18500 24%) 4.3305\n",
      "['', 'i', 'i', 'i', 'i', 'the', 'i', 'the', 'the', '', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "19m 34s (- 59m 34s) (18550 24%) 4.4732\n",
      "['', 'i', 'is', 'a', 'i', 'that', 'i', 'the', 'i', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "19m 37s (- 59m 31s) (18600 24%) 3.9689\n",
      "['', 'i', 'i', 'i', 'i', 'a', 'but', 'i', 'i', 'i', '<EOS>']\n",
      "\n",
      "19m 40s (- 59m 27s) (18650 24%) 3.9939\n",
      "['', 'i', 'i', 'i', 'i', 'of', 'i', 'i', 'i', 'of', 'the', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "19m 44s (- 59m 25s) (18700 24%) 4.2548\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'that', 'of', 'the', 'i', 'i', 'the', 'of', 'the', '<EOS>']\n",
      "\n",
      "19m 47s (- 59m 21s) (18750 25%) 4.2189\n",
      "['', 'i', 'i', 'i', 'a', 'of', 'the', 'it', 'the', 'i', '<EOS>']\n",
      "\n",
      "19m 50s (- 59m 19s) (18800 25%) 4.1642\n",
      "['', 'i', 'i', 'the', 'the', 'of', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "19m 54s (- 59m 17s) (18850 25%) 4.1476\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'it', '<EOS>']\n",
      "\n",
      "19m 57s (- 59m 14s) (18900 25%) 4.0219\n",
      "['', 'i', 'i', 'i', 'i', 'you', 'a', 'the', '', 'you', 'a', '', 'the', '<EOS>']\n",
      "\n",
      "20m 0s (- 59m 11s) (18950 25%) 4.0278\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'you', '', '<EOS>']\n",
      "\n",
      "20m 4s (- 59m 8s) (19000 25%) 4.3110\n",
      "['', 'i', 'is', 'is', 'a', 'but', 'i', 'i', 'it', 'to', 'but', 'i', 'is', '<EOS>']\n",
      "\n",
      "20m 7s (- 59m 5s) (19050 25%) 4.3127\n",
      "['', 'i', 'i', 'a', 'it', 'i', 'you', 'to', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "20m 10s (- 59m 2s) (19100 25%) 4.1623\n",
      "['', 'i', 'i', 'it', 'that', 'i', 'it', 'that', 'i', 'it', 'that', 'the', '<EOS>']\n",
      "\n",
      "20m 13s (- 58m 59s) (19150 25%) 4.3661\n",
      "['', 'yeah', 'i', 'am', 'not', 'sure', 'i', 'i', 'it', '<EOS>']\n",
      "\n",
      "20m 17s (- 58m 57s) (19200 25%) 4.1700\n",
      "['', 'i', 'i', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '<EOS>']\n",
      "\n",
      "20m 20s (- 58m 54s) (19250 25%) 4.3687\n",
      "['', 'i', 'i', 'i', 'it', 'i', 'it', 'i', 'it', 'is', 'to', '<EOS>']\n",
      "\n",
      "20m 23s (- 58m 50s) (19300 25%) 3.8776\n",
      "['', 'i', 'i', 'a', 'the', 'the', 'the', 'the', 'to', 'the', 'the', 'the', 'to', 'the', 'the', '', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "20m 26s (- 58m 47s) (19350 25%) 4.2684\n",
      "['', 'i', 'am', 'not', 'sure', 'i', 'know', 'of', 'the', 'the', 'the', 'the', 'to', '<EOS>']\n",
      "\n",
      "20m 30s (- 58m 45s) (19400 25%) 4.3461\n",
      "['', 'i', 'is', 'a', 'i', 'that', 'a', 'that', 'of', 'the', 'a', 'the', 'that', '', 'the', '<EOS>']\n",
      "\n",
      "20m 33s (- 58m 42s) (19450 25%) 4.4828\n",
      "['', 'i', 'i', 'is', 'to', 'of', 'a', 'of', 'to', 'the', 'the', 'to', '<EOS>']\n",
      "\n",
      "20m 36s (- 58m 40s) (19500 26%) 4.3600\n",
      "['', 'yes', 'it', 's', 'a', 'lot', 'of', 'of', 'a', 'i', 'i', 'to', '<EOS>']\n",
      "\n",
      "20m 40s (- 58m 38s) (19550 26%) 3.9461\n",
      "['', 'yes', 'i', 'i', 'i', 'i', 'you', 'the', 'to', '<EOS>']\n",
      "\n",
      "20m 43s (- 58m 35s) (19600 26%) 4.2274\n",
      "['', 'i', 'is', 'a', 'a', 'a', 'a', 'a', 'the', 'a', '', 'a', '<EOS>']\n",
      "\n",
      "20m 46s (- 58m 32s) (19650 26%) 3.8766\n",
      "['', 'yeah', 'i', 'is', 'i', 'i', 'a', 'i', 'i', 'the', '', 'i', '<EOS>']\n",
      "\n",
      "20m 50s (- 58m 30s) (19700 26%) 4.3071\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "20m 53s (- 58m 26s) (19750 26%) 3.9014\n",
      "['', 'i', 'i', 'not', 'like', 'the', 'the', 'a', 'the', 'i', '', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "20m 57s (- 58m 25s) (19800 26%) 4.1924\n",
      "['', 'yes', 'i', 'i', 't', 'i', 'i', 'i', 'the', 'i', '<EOS>']\n",
      "\n",
      "21m 0s (- 58m 23s) (19850 26%) 4.2231\n",
      "['', 'i', 'i', 'i', 'you', 'of', 'the', 'the', 'to', 'the', 'i', '<EOS>']\n",
      "\n",
      "21m 4s (- 58m 20s) (19900 26%) 4.0387\n",
      "['', 'yeah', 'i', 'i', 'i', 'a', 'to', 'i', 'i', 'to', '<EOS>']\n",
      "\n",
      "21m 7s (- 58m 17s) (19950 26%) 4.1197\n",
      "['', 'yeah', 'i', 'i', 'a', 'i', 'it', '<EOS>']\n",
      "\n",
      "21m 10s (- 58m 14s) (20000 26%) 3.8332\n",
      "['', 'i', 'i', 'that', 'you', 'a', 'i', 'the', 'i', '', '<EOS>']\n",
      "\n",
      "21m 13s (- 58m 11s) (20050 26%) 4.1846\n",
      "['', 'i', 'you', 'know', 'a', 'you', 'i', 'to', '<EOS>']\n",
      "\n",
      "21m 17s (- 58m 8s) (20100 26%) 3.6091\n",
      "['', 'yes', 'i', 'i', 'that', 'a', 'i', '', '<EOS>']\n",
      "\n",
      "21m 20s (- 58m 5s) (20150 26%) 3.8282\n",
      "['', 'i', 'i', 'that', 'that', 'the', 'to', 'i', '<EOS>']\n",
      "\n",
      "21m 23s (- 58m 2s) (20200 26%) 3.6639\n",
      "['', 'i', 'i', 'that', 'you', 'a', 'that', 'a', 'i', '', '<EOS>']\n",
      "\n",
      "21m 26s (- 57m 59s) (20250 27%) 4.0139\n",
      "['', 'yes', 'i', 'do', 'i', 'that', 'a', 'i', 'a', 'that', 'a', 'to', 'i', 'the', 'a', 'that', 'a', 'the', '', 'a', 'to', '', '<EOS>']\n",
      "\n",
      "21m 30s (- 57m 56s) (20300 27%) 4.4564\n",
      "['', 'yes', 'i', 'did', 'not', 'know', 'you', 'the', 'the', 'the', 'the', 'a', 'the', '', '<EOS>']\n",
      "\n",
      "21m 33s (- 57m 53s) (20350 27%) 4.3998\n",
      "['', 'i', 'i', 'a', 'i', 'a', 'i', 'it', 'is', 'the', '', '<EOS>']\n",
      "\n",
      "21m 36s (- 57m 50s) (20400 27%) 4.1976\n",
      "['', 'i', 'i', 'a', 'the', 'a', 'the', 'the', 'the', 'the', '', 'the', 'the', '', '<EOS>']\n",
      "\n",
      "21m 39s (- 57m 47s) (20450 27%) 4.0746\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'a', 'i', 'to', '', 'i', '', '<EOS>']\n",
      "\n",
      "21m 42s (- 57m 43s) (20500 27%) 4.1392\n",
      "['', 'yeah', 'i', 'is', 'a', 'a', 'great', 'to', 'i', 'the', '', '<EOS>']\n",
      "\n",
      "21m 46s (- 57m 41s) (20550 27%) 4.1072\n",
      "['', 'i', 'you', 'know', 'the', 'a', 'the', 'to', '<EOS>']\n",
      "\n",
      "21m 49s (- 57m 38s) (20600 27%) 3.9002\n",
      "['', 'yeah', 'i', 'do', 'not', 'know', 'i', 'the', 'i', 'the', 'the', 'i', '', 'the', '<EOS>']\n",
      "\n",
      "21m 52s (- 57m 34s) (20650 27%) 4.0302\n",
      "['', 'i', 'do', 'not', 'know', 'i', 'the', 'the', 'you', 'the', 'the', '', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "21m 55s (- 57m 30s) (20700 27%) 4.1242\n",
      "['', 'i', 'i', 'know', 'the', 'you', 'the', 'the', 'i', 'the', '', 'the', '<EOS>']\n",
      "\n",
      "21m 58s (- 57m 27s) (20750 27%) 4.0031\n",
      "['', 'i', 'i', 'not', 'you', 'the', '', 'i', '<EOS>']\n",
      "\n",
      "22m 1s (- 57m 23s) (20800 27%) 4.1670\n",
      "['', 'yes', 'i', 'was', 'not', 't', 'to', 'the', 'i', 'the', '<EOS>']\n",
      "\n",
      "22m 4s (- 57m 19s) (20850 27%) 3.7671\n",
      "['', 'i', 'i', 'you', 'you', 'i', 'you', '', 'you', 'the', '', '', '<EOS>']\n",
      "\n",
      "22m 7s (- 57m 16s) (20900 27%) 3.6246\n",
      "['', 'i', 'i', 'i', 'i', 'to', 'i', '<EOS>']\n",
      "\n",
      "22m 10s (- 57m 13s) (20950 27%) 3.9617\n",
      "['', 'i', 'i', 'i', 'i', 'to', 'i', 'to', 'i', 'to', 'it', '', 'you', '<EOS>']\n",
      "\n",
      "22m 14s (- 57m 10s) (21000 28%) 4.3568\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'it', 'i', 'it', '', 'it', '', '<EOS>']\n",
      "\n",
      "22m 17s (- 57m 7s) (21050 28%) 4.2853\n",
      "['', 'i', 'have', 'to', 'have', 'to', 'the', 'i', 'the', 'to', 'have', 'to', 'the', 'a', 'the', '', 'the', '', '<EOS>']\n",
      "\n",
      "22m 20s (- 57m 5s) (21100 28%) 4.4375\n",
      "['', 'i', 'i', 'have', 'not', 'have', 'to', 'i', 'i', 'the', 'it', 'the', 'i', 'it', '', 'the', '', '<EOS>']\n",
      "\n",
      "22m 24s (- 57m 2s) (21150 28%) 4.0667\n",
      "['', 'i', 'you', 'i', 'i', 'to', 'it', 'i', '', '<EOS>']\n",
      "\n",
      "22m 27s (- 56m 58s) (21200 28%) 4.1499\n",
      "['', 'i', 'am', 'not', 'sure', 'i', 'it', 'i', '<EOS>']\n",
      "\n",
      "22m 30s (- 56m 55s) (21250 28%) 4.0979\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'to', 'the', '', 'i', '', '<EOS>']\n",
      "\n",
      "22m 33s (- 56m 52s) (21300 28%) 4.5979\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'it', 'to', 'i', 'it', '<EOS>']\n",
      "\n",
      "22m 37s (- 56m 50s) (21350 28%) 4.1714\n",
      "['', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'to', 'the', 'you', 'the', 'it', 'the', '<EOS>']\n",
      "\n",
      "22m 40s (- 56m 46s) (21400 28%) 4.2797\n",
      "['', 'i', 'i', 'is', 'is', 'i', 'to', '<EOS>']\n",
      "\n",
      "22m 42s (- 56m 42s) (21450 28%) 3.9918\n",
      "['', 'i', 'i', 'i', 'to', 'i', 'to', 'i', 'to', 'i', '<EOS>']\n",
      "\n",
      "22m 46s (- 56m 40s) (21500 28%) 4.4427\n",
      "['', 'i', 'is', 'i', 'to', 'to', 'i', 'to', 'to', '<EOS>']\n",
      "\n",
      "22m 49s (- 56m 37s) (21550 28%) 4.4945\n",
      "['', 'i', 'i', 'you', 'the', 'the', 'i', 'to', 'the', 'the', 'the', 'i', 'to', 'the', '', '<EOS>']\n",
      "\n",
      "22m 53s (- 56m 34s) (21600 28%) 4.8098\n",
      "['', 'i', 'have', 'the', 'i', 'i', '', 'the', '', '', '<EOS>']\n",
      "\n",
      "22m 56s (- 56m 31s) (21650 28%) 4.1347\n",
      "['', 'i', 'i', 'i', 'i', 'a', 'the', 'i', '', '', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22m 59s (- 56m 28s) (21700 28%) 4.1497\n",
      "['', 'i', 'you', 'have', 'the', 'a', 'you', 'the', 'the', 'a', 'the', 'to', 'the', '', '', 'the', '<EOS>']\n",
      "\n",
      "23m 3s (- 56m 27s) (21750 28%) 4.5874\n",
      "['', 'i', 'you', 'the', 'a', 'the', 'i', 'the', 'the', 'a', 'the', 'i', '', '', '', '<EOS>']\n",
      "\n",
      "23m 7s (- 56m 25s) (21800 29%) 4.4779\n",
      "['', 'i', 'i', 'a', 'i', 'the', 'i', 'to', 'the', 'a', 'to', 'the', 'to', '<EOS>']\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m attn_decoder1 \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# encoder1.load_state_dict(torch.load(PATH_ENC)) # можно и другую директорию, но вот это прямо внутри вашего гугл диска\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# attn_decoder1.load_state_dict(torch.load(PATH_DECO))\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_decoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m75000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m training_pair[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m target_tensor \u001b[38;5;241m=\u001b[39m training_pair[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     24\u001b[0m plot_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     64\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     65\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m target_length\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "encoder1.load_state_dict(torch.load(PATH_ENC)) # можно и другую директорию, но вот это прямо внутри вашего гугл диска\n",
    "attn_decoder1.load_state_dict(torch.load(PATH_DECO))\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_wUdnU1kf-s",
    "outputId": "b26cb3f8-a345-4bf7-93cb-dd6da9204a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> wow that is a lot of knowledge for the future of cinematography .\n",
      "= he was married to a white woman but the marriage was not legal until !\n",
      "< he s ! good the not of not not not not not not not not have ! the university of the from not have of not have ! have ! of have ! have of have of have of have of have the of not not not not not not not have of not have of not have ! of have of have of have ! of have of have ! without of the white not not not not not not have the moon of not of not have of not not not not not not not have of not not have ! of the moon not have not not not not not not not not have of not have ! of have of have of have ! of the white not have of not have not have of not have of not have of not have ! of have the moon of not have ! have ! of have of have of have of have the moon of not have of not not not not not not not not not not not not have of not have not have of not not not have ! of the moon but he have the of not not not not have of not not have of not not have of not not have of not have of not have of not not have ! of have of have of have of have the moon but he have of have of have of have of have of have s of have of have of the moon but he have ! without of have of have of have of have of have of have of have of have of have of have s the not not not have of not have ! have of have of have ! have of have of have of have ! of have of have ! have the moon of have of have of have of have of have of have of have of have of have of the white woman have the of not have of not have ! have of have of have of have the of not have of not have of not have not have ! have of have of have ! of have of have ! of have ! have of have ! have of have of have ! of have the moon of have of have of have of have of have of have of have of have of have the of not not not not not not not not have of not have ! have of have the of not have ! have the moon but he have ! of have ! without of the moon not not not not not have of not not not have of not not of not have ! of have the of not not have of not have not have ! of the moon but he have of have of have of have of have of have of have of have of have of have of have ! of have of have ! without of have of have of have ! of have of have of have of have ! of have the of not not have ! have of have the moon but he of have the of not not not not not have of not have of not have ! of the white not have of not have ! of the white not have of not have of not have ! of have ! have of have ! of have of have the of not have of not not have ! of have the moon not not not not not not have not not not not not not not not not not not have not not not not not not have of not have ! without of have of have the university but he have of have of have s the he but he have of have s the he but he of have of have s of have of have of have of have of have s of the moon but he have of have s the he have of have of have of have of have of have of have s of have of have of have of have of have of have of have s of have of have of have of have of have of have s of have of have of have of have of have of have of have of have of have of have of have of have s but he have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of have of without of without from but he of have of have of have of have of have of without of the university but he of have of have of have of have of have of have of have of have of have of but he have of have s the he have of have s of have of have of have of have of have of have of s google from but he of have of have of have of have of have of have of have of have of have of have of have of have ! of have of have of have of have of have of have of have of have of have of have s not have of have of have of have of have of have of have of have of have s not have of have of have of have of have of have of have of have of have s not have of have of have of have of have of have of have of have of have of have of have of have of have of have of good have of have of have of have of have of\n",
      "\n",
      "> well i believe that . that makes sense of course . its interesting since the s of the time the taller candidate has won .\n",
      "= interesting . i found funny the method of election they had in ancient greece it was by lottery\n",
      "< it ! have ! have ! have ! have ! have ! have ! i have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have ! have\n",
      "\n",
      "> so true ! i wouldn t mind seeing a horror movie about flame throwers in vehicles over in south africa where it s legal to prevent car jacking . ha !\n",
      "= that would be scary ! mcdonalds was the first restaurant with a drive thru !\n",
      "< ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "\n",
      "> what s interesting though is that when earth was formed a day was only about hours long now it s hours i wonder if the days will keep getting longer\n",
      "= i guess they will the work day will get up to hours . we need jobs reforesting the planet anyway .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< he i think we get will get get he get get get get get get get get get get get get get get get get get get he lol get get get he lol get get get get get get he get get get get get get get get get get get he up will get he get will get he get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get will get get get get he get get get get will get he get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get will get he get will get get get to get will get will get will get will get he get will get will get will get will get will get will get will get will get will he will get will he will get will get will get will get will get he lol will get will get will get he lol will get will get will get will get he will get will get will get will get will get he lol will get will get will get will get will get will get will get will get will <EOS>\n",
      "\n",
      "> it was a little before my time . i m more of the tonka generation .\n",
      "= i am the transformers generation from the s . i collected the toys but did not see the movies\n",
      "< what ! have ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "\n",
      "> that is an interesting way to go about it . i cant believe it all started over a late fee .\n",
      "= pretty crazy because now netflix has over million subscribers and growing\n",
      "< yeah ! ! ! ! have ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "\n",
      "> that is a lot of shoes . they must need it for all the extreme moves they perform . kind of like michael jackson s extreme lean that he has a patent on .\n",
      "= you re probably right !\n",
      "< what ! have ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! have ! have ! ! have ! have ! have ! ! have ! have ! have ! have ! have ! have ! have ! have ! have ! ! ! ! ! have ! have ! have ! have ! have ! ! have ! ! ! have ! have ! have ! ! ! ! ! have ! have ! ! ! have ! have ! ! ! have ! have ! have ! have ! ! ! have ! ! ! ! ! have ! have ! have ! have ! have ! have ! ! ! have ! have ! have ! have ! have ! have ! have ! have ! have ! ! ! have ! have ! ! have ! ! ! ! ! ! have ! ! have ! ! have ! ! ! have ! have ! ! ! have ! ! ! have ! ! have ! ! have ! have ! ! ! have ! have ! ! ! have ! ! have ! have ! ! ! have ! ! have ! ! ! ! have ! ! have ! have ! have ! have ! have ! have ! have ! have ! ! have ! ! ! ! ! have ! ! have ! have ! have ! ! ! ! have ! have ! have ! ! have ! have ! ! ! have ! have ! have ! ! have ! ! ! have ! have ! have ! have ! ! have ! have ! ! ! ! have ! have ! ! have ! have ! ! ! ! ! ! have ! ! have ! ! have ! ! have ! ! have ! ! ! have ! have ! ! have ! have ! have ! ! have ! ! have ! ! have ! have ! have ! have ! have ! have ! have ! ! have ! have ! ! have ! have ! ! ! ! have ! ! have ! have ! have ! have ! ! ! have ! have ! have ! have ! have ! have ! ! ! have ! ! ! have ! have ! have ! ! ! ! have ! have ! have ! have ! have ! ! have ! have ! ! ! have ! have ! ! have ! ! have ! have ! have ! ! have ! ! have ! ! have ! have ! have ! have ! have ! have ! have ! ! ! ! ! ! ! ! have ! have ! ! ! have ! ! ! have ! ! ! ! ! ! have ! have ! ! ! have ! ! ! have\n",
      "\n",
      "> yes but if they are professional i think the ballet company pays for the shoes . do you prefer functional shoes or decorative fashionable shoes or both ?\n",
      "= i m a mixture of both . seems like the big money i spend for nice shoes ends up being a waste though because then i don t wear the shoes . lol\n",
      "< yeah ! <EOS>\n",
      "\n",
      "> i like watching it in person . not so much on tv . it s more exciting to be there . i ve only been to one pro game with a friend who s really into the game i think it was the nationals many years ago . i don t recall who they played or what the score was but it was a fun afternoon .\n",
      "= yes i like to watch it from home with my friends lol and did you know that baseball managers wear uniforms because they are technically able to play for their teams if the need arises ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< yeah i ! have ! of ! wear ! wear ! wear wear ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! have ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! have ! of ! of ! of ! have ! of ! of ! of ! of ! of ! of ! of ! of ! have ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! wear ! of ! of ! of ! of ! wear ! of ! of ! of ! of ! of ! of ! of ! be ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! have ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! wear ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! wear ! of ! wear ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! be of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of ! of\n",
      "\n",
      "> there will still be helium i think . just not in the atmosphere . maybe they can create helium\n",
      "= i figure that a fusion plant could do that . probably require serious energy .\n",
      "< figure . i serious to serious to serious to serious serious energy . to serious to serious serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy . to serious energy to serious energy serious to serious to serious to serious to serious to serious to serious to serious court serious to require serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to serious to serious serious energy serious to serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious serious to serious to serious to serious serious to serious to serious to serious to serious energy serious to serious to serious to serious too to serious to serious to serious to serious to serious to serious to serious energy to serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy could to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to . serious to serious to serious serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious too serious to serious to serious to serious energy to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to too to . serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious serious to serious serious to serious to serious to serious to . serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to those to those to . serious serious to serious to serious to serious to serious to require serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy . to serious to serious to serious to serious to serious to require serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious serious serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious serious to serious to serious to serious to serious energy serious energy to serious to serious to serious to serious serious serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to . serious to serious to serious to serious to serious to serious to serious serious to serious to require serious to serious to serious to serious to serious to serious to serious to serious to serious to serious to serious . serious to serious to serious to serious to serious to serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious to serious to serious to serious to those to . to serious to serious to serious to serious too serious to serious to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious to serious to serious energy to serious to serious to serious to serious energy to too to serious to serious to serious to serious to serious to serious to serious energy to serious serious to serious serious to serious to serious to serious to serious serious to serious to serious to serious to serious to serious\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACG4Q06bkf-z"
   },
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-  Try with a different dataset\n",
    "\n",
    "   -  Another language pair\n",
    "   -  Human → Machine (e.g. IOT commands)\n",
    "   -  Chat → Response\n",
    "   -  Question → Answer\n",
    "\n",
    "-  Replace the embeddings with pre-trained word embeddings such as word2vec or\n",
    "   GloVe\n",
    "-  Try with more layers, more hidden units, and more sentences. Compare\n",
    "   the training time and results.\n",
    "-  If you use a translation file where pairs have two of the same phrase\n",
    "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
    "   this:\n",
    "\n",
    "   -  Train as an autoencoder\n",
    "   -  Save only the Encoder network\n",
    "   -  Train a new Decoder for translation from there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "cXeLJunHBsjD"
   },
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(encoder1, attn_decoder1, 'what are you doing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'don', 't', 'think', 'going', 'to', 'jazz', '?', '<EOS>']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq_bot.ipynb\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "061670ab7df04ec59ac89a79141d5773": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ad9f6135f4b4405b5facba733d28865": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0db735ce391a424c87a3f287340ab7fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f35c10168a74023b503c589e0a3e48e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a279709b86e47e6b44117900e352dde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f2392865fc8646509d45a5bfcdbdaad5",
       "IPY_MODEL_9d1ff43c43ee4dfd9d3d6db33f3fef7f",
       "IPY_MODEL_5bf2e4b9333d4909b661e69dc5e5bb5c"
      ],
      "layout": "IPY_MODEL_061670ab7df04ec59ac89a79141d5773"
     }
    },
    "1ecd56e5cd334e90ac1963e6255723b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2ea006d2693a46b7a6f73ad98c3ad71f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f35c10168a74023b503c589e0a3e48e",
      "placeholder": "​",
      "style": "IPY_MODEL_ed65250433ee4013acda7747b672123b",
      "value": "Downloading: 100%"
     }
    },
    "37b45c0d2e3f421f8f8dba60722b1762": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "489ea138854d48008e1904c9e46dd6c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5bf2e4b9333d4909b661e69dc5e5bb5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa1455eaad504e11b44f2a9ca79a6697",
      "placeholder": "​",
      "style": "IPY_MODEL_63a6cde631a947a6bb85d1cbb89d9f21",
      "value": " 446k/446k [00:00&lt;00:00, 683kB/s]"
     }
    },
    "5bf945ae3c7540f6ba472f20e2fdacb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7129a21f55af4ccb8bb7544ee34b6831",
       "IPY_MODEL_ffe6cf8b83c649ec8aa08d270316b730",
       "IPY_MODEL_c3382ae68dd84a4ba848cf70807658e0"
      ],
      "layout": "IPY_MODEL_0db735ce391a424c87a3f287340ab7fa"
     }
    },
    "5e76746f82b5427ab86238f4af8f0990": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a688e92efa5740e3ae9a4dc973a06df8",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fecd7f53f93644bfa8c70dc8946a7919",
      "value": 665
     }
    },
    "5f0035855dfe4cd597ee7030acf16d72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63a6cde631a947a6bb85d1cbb89d9f21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69de0ae6cbbe4eb7812e5884318d20b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a228332a0f44ba29f6e211f43c884a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e6527f3e6c24c69ab4d669c28207a84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7129a21f55af4ccb8bb7544ee34b6831": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69de0ae6cbbe4eb7812e5884318d20b6",
      "placeholder": "​",
      "style": "IPY_MODEL_6e6527f3e6c24c69ab4d669c28207a84",
      "value": "Downloading: 100%"
     }
    },
    "7740d64e27724da6ba665101cbe4fd74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fe87edbe4a945e3b48b4640aefc9aa0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9002278fd22143c69a6ab1dd360bcbcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d1ff43c43ee4dfd9d3d6db33f3fef7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ad9f6135f4b4405b5facba733d28865",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ecd56e5cd334e90ac1963e6255723b6",
      "value": 456318
     }
    },
    "a688e92efa5740e3ae9a4dc973a06df8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3382ae68dd84a4ba848cf70807658e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2f168b45b1745a3ac7fb3c21699faa9",
      "placeholder": "​",
      "style": "IPY_MODEL_37b45c0d2e3f421f8f8dba60722b1762",
      "value": " 0.99M/0.99M [00:00&lt;00:00, 2.39MB/s]"
     }
    },
    "df887e109e674daeb07ea2a6a9291202": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed65250433ee4013acda7747b672123b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "efc568a8b8644795b1ac2e3e6a533a3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df887e109e674daeb07ea2a6a9291202",
      "placeholder": "​",
      "style": "IPY_MODEL_7740d64e27724da6ba665101cbe4fd74",
      "value": " 665/665 [00:00&lt;00:00, 16.9kB/s]"
     }
    },
    "f2392865fc8646509d45a5bfcdbdaad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a228332a0f44ba29f6e211f43c884a7",
      "placeholder": "​",
      "style": "IPY_MODEL_9002278fd22143c69a6ab1dd360bcbcf",
      "value": "Downloading: 100%"
     }
    },
    "f2f168b45b1745a3ac7fb3c21699faa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f60dee956d9d444f98a69ea9a6c8f17d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2ea006d2693a46b7a6f73ad98c3ad71f",
       "IPY_MODEL_5e76746f82b5427ab86238f4af8f0990",
       "IPY_MODEL_efc568a8b8644795b1ac2e3e6a533a3c"
      ],
      "layout": "IPY_MODEL_7fe87edbe4a945e3b48b4640aefc9aa0"
     }
    },
    "fa1455eaad504e11b44f2a9ca79a6697": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fecd7f53f93644bfa8c70dc8946a7919": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ffe6cf8b83c649ec8aa08d270316b730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f0035855dfe4cd597ee7030acf16d72",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_489ea138854d48008e1904c9e46dd6c1",
      "value": 1042301
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
